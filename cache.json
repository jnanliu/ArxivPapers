{"2025-07-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2507.02858v1","updated":"2025-07-03T17:59:04Z","published":"2025-07-03T17:59:04Z","title":"Requirements Elicitation Follow-Up Question Generation","summary":"  Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time.\n","authors":["Yuchen Shen","Anmol Singhal","Travis Breaux"],"pdf_url":"https://arxiv.org/pdf/2507.02858v1.pdf","comment":"13 pages, 2 figures, accepted at the 33rd IEEE International\n  Requirements Engineering 2025"},{"id":"http://arxiv.org/abs/2507.02856v1","updated":"2025-07-03T17:59:02Z","published":"2025-07-03T17:59:02Z","title":"Answer Matching Outperforms Multiple Choice for Language Model\n  Evaluation","summary":"  Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.\n","authors":["Nikhil Chandak","Shashwat Goel","Ameya Prabhu","Moritz Hardt","Jonas Geiping"],"pdf_url":"https://arxiv.org/pdf/2507.02856v1.pdf","comment":"34 pages, Code is available at\n  https://github.com/nikhilchandak/answer-matching"},{"id":"http://arxiv.org/abs/2507.02851v1","updated":"2025-07-03T17:55:43Z","published":"2025-07-03T17:55:43Z","title":"MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs","summary":"  Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.\n","authors":["Purbesh Mitra","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2507.02851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02850v1","updated":"2025-07-03T17:55:40Z","published":"2025-07-03T17:55:40Z","title":"LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge\n  Injection to All Users","summary":"  We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection).\n","authors":["Almog Hilel","Idan Shenfeld","Leshem Choshen","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2507.02850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02846v1","updated":"2025-07-03T17:53:48Z","published":"2025-07-03T17:53:48Z","title":"Legal Requirements Translation from Law","summary":"  Software systems must comply with legal regulations, which is a\nresource-intensive task, particularly for small organizations and startups\nlacking dedicated legal expertise. Extracting metadata from regulations to\nelicit legal requirements for software is a critical step to ensure compliance.\nHowever, it is a cumbersome task due to the length and complex nature of legal\ntext. Although prior work has pursued automated methods for extracting\nstructural and semantic metadata from legal text, key limitations remain: they\ndo not consider the interplay and interrelationships among attributes\nassociated with these metadata types, and they rely on manual labeling or\nheuristic-driven machine learning, which does not generalize well to new\ndocuments. In this paper, we introduce an approach based on textual entailment\nand in-context learning for automatically generating a canonical representation\nof legal text, encodable and executable as Python code. Our representation is\ninstantiated from a manually designed Python class structure that serves as a\ndomain-specific metamodel, capturing both structural and semantic legal\nmetadata and their interrelationships. This design choice reduces the need for\nlarge, manually labeled datasets and enhances applicability to unseen\nlegislation. We evaluate our approach on 13 U.S. state data breach notification\nlaws, demonstrating that our generated representations pass approximately 89.4%\nof test cases and achieve a precision and recall of 82.2 and 88.7,\nrespectively.\n","authors":["Anmol Singhal","Travis Breaux"],"pdf_url":"https://arxiv.org/pdf/2507.02846v1.pdf","comment":"13 pages, 7 figures, Accepted at the 33rd IEEE International\n  Requirements Engineering 2025"},{"id":"http://arxiv.org/abs/2507.02844v1","updated":"2025-07-03T17:53:12Z","published":"2025-07-03T17:53:12Z","title":"Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context\n  Injection","summary":"  With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack.\n","authors":["Ziqi Miao","Yi Ding","Lijun Li","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2507.02844v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.11268v2","updated":"2025-07-03T17:51:44Z","published":"2025-02-16T21:02:36Z","title":"Improved Unbiased Watermark for Large Language Models","summary":"  As artificial intelligence surpasses human capabilities in text generation,\nthe necessity to authenticate the origins of AI-generated content has become\nparamount. Unbiased watermarks offer a powerful solution by embedding\nstatistical signals into language model-generated text without distorting the\nquality. In this paper, we introduce MCmark, a family of unbiased,\nMulti-Channel-based watermarks. MCmark works by partitioning the model's\nvocabulary into segments and promoting token probabilities within a selected\nsegment based on a watermark key. We demonstrate that MCmark not only preserves\nthe original distribution of the language model but also offers significant\nimprovements in detectability and robustness over existing unbiased watermarks.\nOur experiments with widely-used language models demonstrate an improvement in\ndetectability of over 10% using MCmark, compared to existing state-of-the-art\nunbiased watermarks. This advancement underscores MCmark's potential in\nenhancing the practical application of watermarking in AI-generated texts.\n","authors":["Ruibo Chen","Yihan Wu","Junfeng Guo","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2502.11268v2.pdf","comment":"ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2507.02841v1","updated":"2025-07-03T17:51:06Z","published":"2025-07-03T17:51:06Z","title":"StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to\n  Reason","summary":"  Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor improving the complex reasoning abilities of large language models (LLMs).\nHowever, current RLVR methods face two significant challenges: the near-miss\nreward problem, where a small mistake can invalidate an otherwise correct\nreasoning process, greatly hindering training efficiency; and exploration\nstagnation, where models tend to focus on solutions within their ``comfort\nzone,'' lacking the motivation to explore potentially more effective\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\nalgorithm that utilizes multi-level stepwise hints to help models explore the\nsolution space more effectively. StepHint generates valid reasoning chains from\nstronger models and partitions these chains into reasoning steps using our\nproposed adaptive partitioning method. The initial few steps are used as hints,\nand simultaneously, multiple-level hints (each comprising a different number of\nsteps) are provided to the model. This approach directs the model's exploration\ntoward a promising solution subspace while preserving its flexibility for\nindependent exploration. By providing hints, StepHint mitigates the near-miss\nreward problem, thereby improving training efficiency. Additionally, the\nexternal reasoning pathways help the model develop better reasoning abilities,\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\nsix mathematical benchmarks, while also demonstrating superior generalization\nand excelling over baselines on out-of-domain benchmarks.\n","authors":["Kaiyi Zhang","Ang Lv","Jinpeng Li","Yongbo Wang","Feng Wang","Haoyuan Hu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2507.02841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.18959v3","updated":"2025-07-03T17:48:36Z","published":"2025-06-23T17:27:19Z","title":"From Web Search towards Agentic Deep Research: Incentivizing Search with\n  Reasoning Agents","summary":"  Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research.\n","authors":["Weizhi Zhang","Yangning Li","Yuanchen Bei","Junyu Luo","Guancheng Wan","Liangwei Yang","Chenxuan Xie","Yuyao Yang","Wei-Chieh Huang","Chunyu Miao","Henry Peng Zou","Xiao Luo","Yusheng Zhao","Yankai Chen","Chunkit Chan","Peilin Zhou","Xinyang Zhang","Chenwei Zhang","Jingbo Shang","Ming Zhang","Yangqiu Song","Irwin King","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2506.18959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02834v1","updated":"2025-07-03T17:44:55Z","published":"2025-07-03T17:44:55Z","title":"ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided\n  Reinforcement Learning","summary":"  Recent advances in large language models have been driven by reinforcement\nlearning (RL)-style post-training, which improves reasoning by optimizing model\noutputs based on reward or preference signals. GRPO-style approaches implement\nthis by using self-generated samples labeled by an outcome-based verifier.\nHowever, these methods depend heavily on the model's initial ability to produce\npositive samples. They primarily refine what the model already knows\n(distribution sharpening) rather than enabling the model to solve problems\nwhere it initially fails. This limitation is especially problematic in\nearly-stage RL training and on challenging reasoning tasks, where positive\nsamples are unlikely to be generated. To unlock reasoning ability in such\nsettings, the model must explore new reasoning trajectories beyond its current\noutput distribution. Such exploration requires access to sufficiently good\npositive samples to guide the learning. While expert demonstrations seem like a\nnatural solution, we find that they are often ineffective in RL post-training.\nInstead, we identify two key properties of effective positive samples: they\nshould (1) be likely under the current policy, and (2) increase the model's\nlikelihood of predicting the correct answer. Based on these insights, we\npropose $\\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and\nmodular framework that generates such samples by conditioning on the\nground-truth answer. ExPO enables efficient exploration and guides the model to\nproduce reasoning trajectories more aligned with its policy than expert-written\nCoTs, while ensuring higher quality than its own (incorrect) samples.\nExperiments show that ExPO improves both learning efficiency and final\nperformance on reasoning benchmarks, surpassing expert-demonstration-based\nmethods in challenging settings such as MATH level-5, where the model initially\nstruggles the most.\n","authors":["Ruiyang Zhou","Shuozhe Li","Amy Zhang","Liu Leqi"],"pdf_url":"https://arxiv.org/pdf/2507.02834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02833v1","updated":"2025-07-03T17:44:33Z","published":"2025-07-03T17:44:33Z","title":"Generalizing Verifiable Instruction Following","summary":"  A crucial factor for successful human and AI interaction is the ability of\nlanguage models or chatbots to follow human instructions precisely. A common\nfeature of instructions are output constraints like ``only answer with yes or\nno\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to\ncraft a more useful answer. Even today's strongest models struggle with\nfulfilling such constraints. We find that most models strongly overfit on a\nsmall set of verifiable constraints from the benchmarks that test these\nabilities, a skill called precise instruction following, and are not able to\ngeneralize well to unseen output constraints. We introduce a new benchmark,\nIFBench, to evaluate precise instruction following generalization on 58 new,\ndiverse, and challenging verifiable out-of-domain constraints. In addition, we\nperform an extensive analysis of how and on what data models can be trained to\nimprove precise instruction following generalization. Specifically, we\ncarefully design constraint verification modules and show that reinforcement\nlearning with verifiable rewards (RLVR) significantly improves instruction\nfollowing. In addition to IFBench, we release 29 additional new hand-annotated\ntraining constraints and verification functions, RLVR training prompts, and\ncode.\n","authors":["Valentina Pyatkin","Saumya Malik","Victoria Graf","Hamish Ivison","Shengyi Huang","Pradeep Dasigi","Nathan Lambert","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2507.02833v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2507.02822v1","updated":"2025-07-03T17:33:58Z","published":"2025-07-03T17:33:58Z","title":"SynapseRoute: An Auto-Route Switching Framework on Dual-State Large\n  Language Model","summary":"  With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost.\n","authors":["Wencheng Zhang","Shiqin Qiao","Lingjie Luo","Yinfeng Li","Chuanyang Zheng","Qian Xu","Meng Li","Yong Gui","Yijun He","Jianing Qiu","Jindong Hong","Jiankai Sun"],"pdf_url":"https://arxiv.org/pdf/2507.02822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02804v1","updated":"2025-07-03T17:07:20Z","published":"2025-07-03T17:07:20Z","title":"Multimodal Mathematical Reasoning with Diverse Solving Perspective","summary":"  Recent progress in large-scale reinforcement learning (RL) has notably\nenhanced the reasoning capabilities of large language models (LLMs), especially\nin mathematical domains. However, current multimodal LLMs (MLLMs) for\nmathematical reasoning often rely on one-to-one image-text pairs and\nsingle-solution supervision, overlooking the diversity of valid reasoning\nperspectives and internal reflections. In this work, we introduce MathV-DP, a\nnovel dataset that captures multiple diverse solution trajectories for each\nimage-question pair, fostering richer reasoning supervision. We further propose\nQwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and\nenhanced via group relative policy optimization (GRPO), a rule-based RL\napproach that integrates correctness discrimination and diversity-aware reward\nfunctions. Our method emphasizes learning from varied reasoning perspectives\nand distinguishing between correct yet distinct solutions. Extensive\nexperiments on the MathVista's minitest and Math-V benchmarks demonstrate that\nQwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and\ngenerative diversity, highlighting the importance of incorporating diverse\nperspectives and reflective reasoning in multimodal mathematical reasoning.\n","authors":["Wenhao Shi","Zhiqiang Hu","Yi Bin","Yang Yang","See-Kiong Ng","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2507.02804v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2507.02799v1","updated":"2025-07-03T17:01:53Z","published":"2025-07-03T17:01:53Z","title":"Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language\n  Models","summary":"  Reasoning Language Models (RLMs) have gained traction for their ability to\nperform complex, multi-step reasoning tasks through mechanisms such as\nChain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these\ncapabilities promise improved reliability, their impact on robustness to social\nbiases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,\noriginally designed for Large Language Models (LLMs), to investigate the\nadversarial robustness of RLMs to bias elicitation. We systematically evaluate\nstate-of-the-art RLMs across diverse sociocultural dimensions, using an\nLLM-as-a-judge approach for automated safety scoring and leveraging jailbreak\ntechniques to assess the strength of built-in safety mechanisms. Our evaluation\naddresses three key questions: (i) how the introduction of reasoning\ncapabilities affects model fairness and robustness; (ii) whether models\nfine-tuned for reasoning exhibit greater safety than those relying on CoT\nprompting at inference time; and (iii) how the success rate of jailbreak\nattacks targeting bias elicitation varies with the reasoning mechanisms\nemployed. Our findings reveal a nuanced relationship between reasoning\ncapabilities and bias safety. Surprisingly, models with explicit reasoning,\nwhether via CoT prompting or fine-tuned reasoning traces, are generally more\nvulnerable to bias elicitation than base models without such mechanisms,\nsuggesting reasoning may unintentionally open new pathways for stereotype\nreinforcement. Reasoning-enabled models appear somewhat safer than those\nrelying on CoT prompting, which are particularly prone to contextual reframing\nattacks through storytelling prompts, fictional personas, or reward-shaped\ninstructions. These results challenge the assumption that reasoning inherently\nimproves robustness and underscore the need for more bias-aware approaches to\nreasoning design.\n","authors":["Riccardo Cantini","Nicola Gabriele","Alessio Orsino","Domenico Talia"],"pdf_url":"https://arxiv.org/pdf/2507.02799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02790v1","updated":"2025-07-03T16:54:32Z","published":"2025-07-03T16:54:32Z","title":"From Long Videos to Engaging Clips: A Human-Inspired Video Editing\n  Framework with Multimodal Narrative Understanding","summary":"  The rapid growth of online video content, especially on short video\nplatforms, has created a growing demand for efficient video editing techniques\nthat can condense long-form videos into concise and engaging clips. Existing\nautomatic editing methods predominantly rely on textual cues from ASR\ntranscripts and end-to-end segment selection, often neglecting the rich visual\ncontext and leading to incoherent outputs. In this paper, we propose a\nhuman-inspired automatic video editing framework (HIVE) that leverages\nmultimodal narrative understanding to address these limitations. Our approach\nincorporates character extraction, dialogue analysis, and narrative\nsummarization through multimodal large language models, enabling a holistic\nunderstanding of the video content. To further enhance coherence, we apply\nscene-level segmentation and decompose the editing process into three subtasks:\nhighlight detection, opening/ending selection, and pruning of irrelevant\ncontent. To facilitate research in this area, we introduce DramaAD, a novel\nbenchmark dataset comprising over 800 short drama episodes and 500\nprofessionally edited advertisement clips. Experimental results demonstrate\nthat our framework consistently outperforms existing baselines across both\ngeneral and advertisement-oriented editing tasks, significantly narrowing the\nquality gap between automatic and human-edited videos.\n","authors":["Xiangfeng Wang","Xiao Li","Yadong Wei","Xueyu Song","Yang Song","Xiaoqiang Xia","Fangrui Zeng","Zaiyi Chen","Liu Liu","Gu Xu","Tong Xu"],"pdf_url":"https://arxiv.org/pdf/2507.02790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.22049v2","updated":"2025-07-03T16:54:09Z","published":"2025-06-27T09:45:15Z","title":"GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling","summary":"  Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the shortcut to dominate over sub-layer outputs in the residual\nconnection and limiting the learning capacity of deeper layers. To mitigate\nthis issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple\ntechnique that can be used in combination with existing approaches. GPAS works\nby scaling down the intermediate activations while keeping their gradients\nunchanged. This leaves information in the activations intact, and avoids the\ngradient vanishing problem associated with gradient downscaling. Extensive\nexperiments across various model sizes from 71M to 1B show that GPAS achieves\nconsistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also\nshows promise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings. Our code is available at\nhttps://github.com/dandingsky/GPAS.\n","authors":["Tianhao Chen","Xin Xu","Zijing Liu","Pengxiang Li","Xinyuan Song","Ajay Kumar Jaiswal","Fan Zhang","Jishan Hu","Yang Wang","Hao Chen","Shizhe Diao","Shiwei Liu","Yu Li","Lu Yin","Can Yang"],"pdf_url":"https://arxiv.org/pdf/2506.22049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00612v3","updated":"2025-07-03T16:50:12Z","published":"2025-05-31T15:51:09Z","title":"Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge\n  Graph Guided Distractor Generation","summary":"  Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs.\n","authors":["Running Yang","Wenlong Deng","Minghui Chen","Yuyin Zhou","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2506.00612v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02778v1","updated":"2025-07-03T16:41:30Z","published":"2025-07-03T16:41:30Z","title":"Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs","summary":"  Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness.\n","authors":["Ken Tsui"],"pdf_url":"https://arxiv.org/pdf/2507.02778v1.pdf","comment":"31 pages, 18 figures"},{"id":"http://arxiv.org/abs/2507.02768v1","updated":"2025-07-03T16:28:25Z","published":"2025-07-03T16:28:25Z","title":"DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with\n  Self-Generated Cross-Modal Alignment","summary":"  We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs.\n","authors":["Ke-Han Lu","Zhehuai Chen","Szu-Wei Fu","Chao-Han Huck Yang","Sung-Feng Huang","Chih-Kai Yang","Chee-En Yu","Chun-Wei Chen","Wei-Chih Chen","Chien-yu Huang","Yi-Cheng Lin","Yu-Xiang Lin","Chi-An Fu","Chun-Yi Kuan","Wenze Ren","Xuanjun Chen","Wei-Ping Huang","En-Pei Hu","Tzu-Quan Lin","Yuan-Kuei Wu","Kuan-Po Huang","Hsiao-Ying Huang","Huang-Cheng Chou","Kai-Wei Chang","Cheng-Han Chiang","Boris Ginsburg","Yu-Chiang Frank Wang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2507.02768v1.pdf","comment":"Model and code available at:\n  https://github.com/kehanlu/DeSTA2.5-Audio"},{"id":"http://arxiv.org/abs/2412.05693v3","updated":"2025-07-03T16:06:35Z","published":"2024-12-07T16:41:54Z","title":"Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression","summary":"  Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.\n","authors":["Michael R. Metel","Boxing Chen","Mehdi Rezagholizadeh"],"pdf_url":"https://arxiv.org/pdf/2412.05693v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02744v1","updated":"2025-07-03T16:04:16Z","published":"2025-07-03T16:04:16Z","title":"Measurement of the Granularity of Vowel Production Space By Just\n  Producible Different (JPD) Limens","summary":"  A body of work over the past several decades has demonstrated that the\ncomplex and coordinated articulatory movements of human vowel production are\ngoverned (at least in part)by control mechanisms whose targets are regions of\nauditory space. Within the target region control at the sub-phonemic level has\nalso been demonstrated. But the degree of accuracy of that control is unknown.\nThe current work investigates this question by asking how far apart must two\nvowel stimuli lie in auditory space in order to yield reliably different\nimitations? This distance is termed 'Just Producible Difference' (JPD). The\ncurrent study uses a vowel mimicry paradigm to derive the first measurement of\nJPD among two sets of English speakers during front vowel production. JPD is\nestimated at between 14 and 51 mels in F1 X F2 space. This finding has\nimplications for episodic theories of speech production. It also clarifies the\npossible structures of human vowel systems, by setting a theoretical lower\nbound for how close two vowel phonemes may be in a speaker's formant space, and\nhence a psychophysical explanation of observed trends in number and patterns of\npossible vowel phonemes.\n","authors":["Peter Viechnicki"],"pdf_url":"https://arxiv.org/pdf/2507.02744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02737v1","updated":"2025-07-03T15:54:55Z","published":"2025-07-03T15:54:55Z","title":"Early Signs of Steganographic Capabilities in Frontier LLMs","summary":"  Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\nfrom misuse and misalignment. However, LLMs could evade monitoring through\nsteganography: Encoding hidden information within seemingly benign generations.\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\nbetter understand the risk they pose. We focus on two types of steganography:\npassing encoded messages and performing encoded reasoning. We find that current\nmodels are unable to encode short messages in their outputs without a monitor\nnoticing under standard affordances. They can succeed, however, if given\nadditional affordances such as using an unmonitored scratchpad and coordinating\non what encoding scheme to use. We additionally find early signs that models\ncan perform basic encoded reasoning in a simple state-tracking problem. This\nincludes some ability to reason with their own and pre-defined schemes,\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\nWhile these capabilities are likely insufficient to bypass well-designed\nmonitors at present, this could change in the future.\n","authors":["Artur Zolkowski","Kei Nishimura-Gasparian","Robert McCarthy","Roland S. Zimmermann","David Lindner"],"pdf_url":"https://arxiv.org/pdf/2507.02737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21506v2","updated":"2025-07-03T15:47:40Z","published":"2025-06-26T17:32:50Z","title":"Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge","summary":"  Agentic search such as Deep Research systems-where agents autonomously browse\nthe web, synthesize information, and return comprehensive citation-backed\nanswers-represents a major shift in how users interact with web-scale\ninformation. While promising greater efficiency and cognitive offloading, the\ngrowing complexity and open-endedness of agentic search have outpaced existing\nevaluation benchmarks and methodologies, which largely assume short search\nhorizons and static answers. In this paper, we introduce Mind2Web 2, a\nbenchmark of 130 realistic, high-quality, and long-horizon tasks that require\nreal-time web browsing and extensive information synthesis, constructed with\nover 1000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of ten frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, highlighting its great potential. Altogether, Mind2Web\n2 provides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.\n","authors":["Boyu Gou","Zanming Huang","Yuting Ning","Yu Gu","Michael Lin","Weijian Qi","Andrei Kopanev","Botao Yu","Bernal Jiménez Gutiérrez","Yiheng Shu","Chan Hee Song","Jiaman Wu","Shijie Chen","Hanane Nour Moussa","Tianshu Zhang","Jian Xie","Yifei Li","Tianci Xue","Zeyi Liao","Kai Zhang","Boyuan Zheng","Zhaowei Cai","Viktor Rozgic","Morteza Ziyadi","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2506.21506v2.pdf","comment":"Project Homepage: https://osu-nlp-group.github.io/Mind2Web-2/"},{"id":"http://arxiv.org/abs/2412.18530v2","updated":"2025-07-03T15:39:41Z","published":"2024-12-24T16:24:43Z","title":"On Characterizations for Language Generation: Interplay of\n  Hallucinations, Breadth, and Stability","summary":"  We study language generation in the limit - introduced by Kleinberg and\nMullainathan [KM24] - building on classical works of Gold [Gol67] and Angluin\n[Ang79]. [KM24]'s main result is an algorithm for generating from any countable\nlanguage collection in the limit. While their algorithm eventually generates\nunseen strings from the target language $K$, it sacrifices coverage or breadth,\ni.e., its ability to generate a rich set of strings. Recent work introduces\ndifferent notions of breadth and explores when generation with breadth is\npossible, leaving a full characterization of these notions open. Our first set\nof results settles this by characterizing generation for existing notions of\nbreadth and their natural extensions. Interestingly, our lower bounds are very\nflexible and hold for many performance metrics beyond breadth - for instance,\nshowing that, in general, it is impossible to train generators which achieve a\nhigher perplexity or lower hallucination rate for $K$ compared to other\nlanguages. Next, we study language generation with breadth and stable\ngenerators - algorithms that eventually stop changing after seeing an arbitrary\nbut finite number of strings - and prove unconditional lower bounds for such\ngenerators, strengthening the results of [KMV25] and demonstrating that\ngeneration with many existing notions of breadth becomes equally hard, when\nstability is required. This gives a separation for generation with approximate\nbreadth, between stable and unstable generators, highlighting the rich\ninterplay between breadth, stability, and consistency in language generation.\n","authors":["Alkis Kalavasis","Anay Mehrotra","Grigoris Velegkas"],"pdf_url":"https://arxiv.org/pdf/2412.18530v2.pdf","comment":"v2 improves exposition and simplifies proofs"},{"id":"http://arxiv.org/abs/2411.00863v2","updated":"2025-07-03T15:14:51Z","published":"2024-10-30T18:00:04Z","title":"Next-Token Prediction Task Assumes Optimal Data Ordering for LLM\n  Training in Proof Generation","summary":"  In the field of large language model (LLM)-based proof generation, despite\nextensive training on large datasets such as ArXiv, LLMs still exhibit only\nmodest performance on proving tasks of moderate difficulty. We believe that\nthis is partly due to the widespread presence of suboptimal ordering within the\ndata for each proof used in training. For example, published proofs often\nfollow a purely logical order, where each step logically proceeds from the\nprevious steps based on the deductive rules. This order is designed to\nfacilitate the verification of the proof's soundness, rather than to help\npeople and models learn the discovery process of the proof. In proof\ngeneration, we argue that the optimal order for one training data sample occurs\nwhen the relevant intermediate supervision for a particular proof step in the\nproof is always positioned to the left of that proof step. We call such order\nthe intuitively sequential order. We validate our claims using two tasks:\nintuitionistic propositional logic theorem-proving and digit multiplication.\nOur experiments verify the order effect and provide support for our\nexplanations. We demonstrate that training is most effective when the proof is\nin the intuitively sequential order. Moreover, the order effect and the\nperformance gap between models trained on different data orders can be\nsubstantial -- with an 11 percent improvement in proof success rate observed in\nthe propositional logic theorem-proving task, between models trained on the\noptimal order compared to the worst order. Lastly, we define a common type of\norder issue in advanced math proofs and find that 17.3 percent of theorems with\nnontrivial proofs in the first two chapters of a widely used graduate-level\nmathematics textbook suffer from this issue. A detailed list of those proofs is\nprovided in the appendix.\n","authors":["Chenyang An","Shima Imani","Feng Yao","Chengyu Dong","Ali Abbasi","Harsh Shrivastava","Samuel Buss","Jingbo Shang","Gayathri Mahalingam","Pramod Sharma","Maurice Diesendruck"],"pdf_url":"https://arxiv.org/pdf/2411.00863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02694v1","updated":"2025-07-03T15:04:38Z","published":"2025-07-03T15:04:38Z","title":"Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers","summary":"  Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.\n","authors":["Zhijian Xu","Yilun Zhao","Manasi Patwardhan","Lovekesh Vig","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2507.02694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02679v1","updated":"2025-07-03T14:42:03Z","published":"2025-07-03T14:42:03Z","title":"Exploring Gender Bias Beyond Occupational Titles","summary":"  In this work, we investigate the correlation between gender and contextual\nbiases, focusing on elements such as action verbs, object nouns, and\nparticularly on occupations. We introduce a novel dataset, GenderLexicon, and a\nframework that can estimate contextual bias and its related gender bias. Our\nmodel can interpret the bias with a score and thus improve the explainability\nof gender bias. Also, our findings confirm the existence of gender biases\nbeyond occupational stereotypes. To validate our approach and demonstrate its\neffectiveness, we conduct evaluations on five diverse datasets, including a\nJapanese dataset.\n","authors":["Ahmed Sabir","Rajesh Sharama"],"pdf_url":"https://arxiv.org/pdf/2507.02679v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.13886v2","updated":"2025-07-03T14:30:25Z","published":"2025-05-20T03:47:44Z","title":"Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General\n  Reasoning","summary":"  Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic.\n","authors":["Jingqi Tong","Jixin Tang","Hangcheng Li","Yurong Mou","Ming Zhang","Jun Zhao","Yanbo Wen","Fan Song","Jiahao Zhan","Yuyang Lu","Chaoran Tao","Zhiyuan Guo","Jizhou Yu","Tianhao Cheng","Changhao Jiang","Zhen Wang","Tao Liang","Zhihui Fei","Mingyang Wan","Guojun Ma","Weifeng Ge","Guanhua Chen","Tao Gui","Xipeng Qiu","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2505.13886v2.pdf","comment":"63 pages, 23 figures, submitted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2507.02666v1","updated":"2025-07-03T14:29:43Z","published":"2025-07-03T14:29:43Z","title":"ASDA: Audio Spectrogram Differential Attention Mechanism for\n  Self-Supervised Representation Learning","summary":"  In recent advancements in audio self-supervised representation learning, the\nstandard Transformer architecture has emerged as the predominant approach, yet\nits attention mechanism often allocates a portion of attention weights to\nirrelevant information, potentially impairing the model's discriminative\nability. To address this, we introduce a differential attention mechanism,\nwhich effectively mitigates ineffective attention allocation through the\nintegration of dual-softmax operations and appropriately tuned differential\ncoefficients. Experimental results demonstrate that our ASDA model achieves\nstate-of-the-art (SOTA) performance across multiple benchmarks, including audio\nclassification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting\n(98.3% accuracy on SPC-2), and environmental sound classification (96.1%\naccuracy on ESC-50). These results highlight ASDA's effectiveness in audio\ntasks, paving the way for broader applications.\n","authors":["Junyu Wang","Tianrui Wang","Meng Ge","Longbiao Wang","Jianwu Dang"],"pdf_url":"https://arxiv.org/pdf/2507.02666v1.pdf","comment":"Accepted at Interspeech2025"},{"id":"http://arxiv.org/abs/2507.02659v1","updated":"2025-07-03T14:20:41Z","published":"2025-07-03T14:20:41Z","title":"OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding","summary":"  Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.\n","authors":["Ramchalam Kinattinkara Ramakrishnan","Zhaocong Yuan","Shaojie Zhuo","Chen Feng","Yicheng Lin","Chenzheng Su","Xiaopeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.02659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02652v1","updated":"2025-07-03T14:18:08Z","published":"2025-07-03T14:18:08Z","title":"Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search","summary":"  Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA.\n","authors":["Jiajie Jin","Xiaoxi Li","Guanting Dong","Yuyao Zhang","Yutao Zhu","Yang Zhao","Hongjin Qian","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2507.02652v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2507.02618v1","updated":"2025-07-03T13:45:02Z","published":"2025-07-03T13:45:02Z","title":"Strategic Intelligence in Large Language Models: Evidence from\n  evolutionary Game Theory","summary":"  Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty.\n","authors":["Kenneth Payne","Baptiste Alloui-Cros"],"pdf_url":"https://arxiv.org/pdf/2507.02618v1.pdf","comment":"29 pages, 27 tables, 4 figures"},{"id":"http://arxiv.org/abs/2506.08713v2","updated":"2025-07-03T13:39:37Z","published":"2025-06-10T11:56:06Z","title":"Explainable Compliance Detection with Multi-Hop Natural Language\n  Inference on Assurance Case Structure","summary":"  Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process.\n","authors":["Fariz Ikhwantri","Dusica Marijan"],"pdf_url":"https://arxiv.org/pdf/2506.08713v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07618v2","updated":"2025-07-03T13:18:23Z","published":"2024-11-12T07:54:13Z","title":"Direct Preference Optimization Using Sparse Feature-Level Constraints","summary":"  The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.\n","authors":["Qingyu Yin","Chak Tou Leong","Hongbo Zhang","Minjun Zhu","Hanqi Yan","Qiang Zhang","Yulan He","Wenjie Li","Jun Wang","Yue Zhang","Linyi Yang"],"pdf_url":"https://arxiv.org/pdf/2411.07618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01334v2","updated":"2025-07-03T13:15:11Z","published":"2025-07-02T03:51:16Z","title":"Symbolic or Numerical? Understanding Physics Problem Solving in\n  Reasoning LLMs","summary":"  Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.\n","authors":["Nifu Dan","Yujun Cai","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2507.01334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02595v1","updated":"2025-07-03T13:09:18Z","published":"2025-07-03T13:09:18Z","title":"MPF: Aligning and Debiasing Language Models post Deployment via Multi\n  Perspective Fusion","summary":"  Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning.\n","authors":["Xin Guan","PeiHsin Lin","Zekun Wu","Ze Wang","Ruibo Zhang","Emre Kazim","Adriano Koshiyama"],"pdf_url":"https://arxiv.org/pdf/2507.02595v1.pdf","comment":"Accepted at ICML 2025 AIW Workshop"},{"id":"http://arxiv.org/abs/2410.12532v3","updated":"2025-07-03T13:02:30Z","published":"2024-10-16T13:10:27Z","title":"MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based\n  Agent Collaboration","summary":"  In healthcare intelligence, the ability to fuse heterogeneous, multi-intent\ninformation from diverse clinical sources is fundamental to building reliable\ndecision-making systems. Large Language Model (LLM)-driven information\ninteraction systems currently showing potential promise in the healthcare\ndomain. Nevertheless, they often suffer from information redundancy and\ncoupling when dealing with complex medical intents, leading to severe\nhallucinations and performance bottlenecks. To this end, we propose MedAide, an\nLLM-based medical multi-agent collaboration framework designed to enable\nintent-aware information fusion and coordinated reasoning across specialized\nhealthcare domains. Specifically, we introduce a regularization-guided module\nthat combines syntactic constraints with retrieval augmented generation to\ndecompose complex queries into structured representations, facilitating\nfine-grained clinical information fusion and intent resolution. Additionally, a\ndynamic intent prototype matching module is proposed to utilize dynamic\nprototype representation with a semantic similarity matching mechanism to\nachieve adaptive recognition and updating of the agent's intent in multi-round\nhealthcare dialogues. Ultimately, we design a rotation agent collaboration\nmechanism that introduces dynamic role rotation and decision-level information\nfusion across specialized medical agents. Extensive experiments are conducted\non four medical benchmarks with composite intents. Experimental results from\nautomated metrics and expert doctor evaluations show that MedAide outperforms\ncurrent LLMs and improves their medical proficiency and strategic reasoning.\n","authors":["Dingkang Yang","Jinjie Wei","Mingcheng Li","Jiyao Liu","Lihao Liu","Ming Hu","Junjun He","Yakun Ju","Wei Zhou","Yang Liu","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12532v3.pdf","comment":"LLM-based Multi-Agent Collaboration for Medical Applications"},{"id":"http://arxiv.org/abs/2507.02593v1","updated":"2025-07-03T12:59:28Z","published":"2025-07-03T12:59:28Z","title":"Revisiting Active Learning under (Human) Label Variation","summary":"  Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.\n","authors":["Cornelia Gruber","Helen Alber","Bernd Bischl","Göran Kauermann","Barbara Plank","Matthias Aßenmacher"],"pdf_url":"https://arxiv.org/pdf/2507.02593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02592v1","updated":"2025-07-03T12:59:07Z","published":"2025-07-03T12:59:07Z","title":"WebSailor: Navigating Super-human Reasoning for Web Agent","summary":"  Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.\n","authors":["Kuan Li","Zhongwang Zhang","Huifeng Yin","Liwen Zhang","Litu Ou","Jialong Wu","Wenbiao Yin","Baixuan Li","Zhengwei Tao","Xinyu Wang","Weizhou Shen","Junkai Zhang","Dingchu Zhang","Xixi Wu","Yong Jiang","Ming Yan","Pengjun Xie","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.02592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12479v2","updated":"2025-07-03T12:11:34Z","published":"2025-06-14T12:43:07Z","title":"AI Flow: Perspectives, Scenarios, and Approaches","summary":"  Pioneered by the foundational information theory by Claude Shannon and the\nvisionary framework of machine intelligence by Alan Turing, the convergent\nevolution of information and communication technologies (IT/CT) has created an\nunbroken wave of connectivity and computation. This synergy has sparked a\ntechnological revolution, now reaching its peak with large artificial\nintelligence (AI) models that are reshaping industries and redefining\nhuman-machine collaboration. However, the realization of ubiquitous\nintelligence faces considerable challenges due to substantial resource\nconsumption in large models and high communication bandwidth demands. To\naddress these challenges, AI Flow has been introduced as a multidisciplinary\nframework that integrates cutting-edge IT and CT advancements, with a\nparticular emphasis on the following three key points. First, device-edge-cloud\nframework serves as the foundation, which integrates end devices, edge servers,\nand cloud clusters to optimize scalability and efficiency for low-latency model\ninference. Second, we introduce the concept of familial models, which refers to\na series of different-sized models with aligned hidden features, enabling\neffective collaboration and the flexibility to adapt to varying resource\nconstraints and dynamic scenarios. Third, connectivity- and interaction-based\nintelligence emergence is a novel paradigm of AI Flow. By leveraging\ncommunication networks to enhance connectivity, the collaboration among AI\nmodels across heterogeneous nodes achieves emergent intelligence that surpasses\nthe capability of any single model. The innovations of AI Flow provide enhanced\nintelligence, timely responsiveness, and ubiquitous accessibility to AI\nservices, paving the way for the tighter fusion of AI techniques and\ncommunication systems.\n","authors":["Hongjun An","Wenhan Hu","Sida Huang","Siqi Huang","Ruanjun Li","Yuanzhi Liang","Jiawei Shao","Yiliang Song","Zihan Wang","Cheng Yuan","Chi Zhang","Hongyuan Zhang","Wenhao Zhuang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2506.12479v2.pdf","comment":"Authors are with Institute of Artificial Intelligence (TeleAI), China\n  Telecom, China. Author names are listed alphabetically by surname. This work\n  was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:\n  shaojw2@chinatelecom.cn) under the leadership of Prof. Xuelong Li. The\n  corresponding author is Prof. Xuelong Li (e-mail: xuelong li@ieee.org), the\n  CTO and Chief Scientist of China Telecom"},{"id":"http://arxiv.org/abs/2505.15075v2","updated":"2025-07-03T10:35:35Z","published":"2025-05-21T03:43:37Z","title":"Traveling Across Languages: Benchmarking Cross-Lingual Consistency in\n  Multimodal LLMs","summary":"  The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.\n","authors":["Hao Wang","Pinzhi Huang","Jihan Yang","Saining Xie","Daisuke Kawahara"],"pdf_url":"https://arxiv.org/pdf/2505.15075v2.pdf","comment":"https://github.com/nlp-waseda/traveling-across-languages"},{"id":"http://arxiv.org/abs/2507.01551v2","updated":"2025-07-03T10:33:08Z","published":"2025-07-02T10:05:14Z","title":"Self-Guided Process Reward Optimization with Redefined Step-wise\n  Advantage for Process Reinforcement Learning","summary":"  Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.\n","authors":["Wu Fei","Hao Kong","Shuxian Liang","Yang Lin","Yibo Yang","Jing Tang","Lei Chen","Xiansheng Hua"],"pdf_url":"https://arxiv.org/pdf/2507.01551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02506v1","updated":"2025-07-03T10:13:42Z","published":"2025-07-03T10:13:42Z","title":"IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on\n  Indian Bail Orders","summary":"  Legal NLP remains underdeveloped in regions like India due to the scarcity of\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\npipeline and verified for consistency. This resource supports a wide range of\nlegal NLP tasks such as outcome prediction, summarization, and fairness\nanalysis, and is the first publicly available dataset focused specifically on\nIndian bail jurisprudence.\n","authors":["Sneha Deshmukh","Prathmesh Kamble"],"pdf_url":"https://arxiv.org/pdf/2507.02506v1.pdf","comment":"9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and\n  GitHub. Submitted to arXiv for open access"},{"id":"http://arxiv.org/abs/2506.23661v2","updated":"2025-07-03T09:46:05Z","published":"2025-06-30T09:37:19Z","title":"Robustness of Misinformation Classification Systems to Adversarial\n  Examples Through BeamAttack","summary":"  We extend BeamAttack, an adversarial attack algorithm designed to evaluate\nthe robustness of text classification systems through word-level modifications\nguided by beam search. Our extensions include support for word deletions and\nthe option to skip substitutions, enabling the discovery of minimal\nmodifications that alter model predictions. We also integrate LIME to better\nprioritize word replacements. Evaluated across multiple datasets and victim\nmodels (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA\nframework, our approach achieves over a 99\\% attack success rate while\npreserving the semantic and lexical similarity of the original texts. Through\nboth quantitative and qualitative analysis, we highlight BeamAttack's\neffectiveness and its limitations. Our implementation is available at\nhttps://github.com/LucK1Y/BeamAttack\n","authors":["Arnisa Fazla","Lucas Krauter","David Guzman Piedrahita","Andrianos Michail"],"pdf_url":"https://arxiv.org/pdf/2506.23661v2.pdf","comment":"12 pages main text, 27 pages total including references and\n  appendices. 13 figures, 10 tables. Accepted for publication in the LNCS\n  proceedings of CLEF 2025 (Best-of-Labs track)"},{"id":"http://arxiv.org/abs/2408.01119v3","updated":"2025-07-03T09:32:20Z","published":"2024-08-02T09:00:03Z","title":"Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer","summary":"  Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks.\n","authors":["Robert Belanec","Simon Ostermann","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2408.01119v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01548v2","updated":"2025-07-03T08:45:46Z","published":"2025-07-02T10:00:12Z","title":"Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for\n  Elderly Migrants","summary":"  This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.\n","authors":["Wen Zhan","Ziqun Hua","Peiyue Lin","Yunfei Chen"],"pdf_url":"https://arxiv.org/pdf/2507.01548v2.pdf","comment":"A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review"},{"id":"http://arxiv.org/abs/2507.02428v1","updated":"2025-07-03T08:34:15Z","published":"2025-07-03T08:34:15Z","title":"A Cookbook for Community-driven Data Collection of Impaired Speech in\n  LowResource Languages","summary":"  This study presents an approach for collecting speech samples to build\nAutomatic Speech Recognition (ASR) models for impaired speech, particularly,\nlow-resource languages. It aims to democratize ASR technology and data\ncollection by developing a \"cookbook\" of best practices and training for\ncommunity-driven data collection and ASR model building. As a proof-of-concept,\nthis study curated the first open-source dataset of impaired speech in Akan: a\nwidely spoken indigenous language in Ghana. The study involved participants\nfrom diverse backgrounds with speech impairments. The resulting dataset, along\nwith the cookbook and open-source tools, are publicly available to enable\nresearchers and practitioners to create inclusive ASR technologies tailored to\nthe unique needs of speech impaired individuals. In addition, this study\npresents the initial results of fine-tuning open-source ASR models to better\nrecognize impaired speech in Akan.\n","authors":["Sumaya Ahmed Salihs","Isaac Wiafe","Jamal-Deen Abdulai","Elikem Doe Atsakpo","Gifty Ayoka","Richard Cave","Akon Obu Ekpezu","Catherine Holloway","Katrin Tomanek","Fiifi Baffoe Payin Winful"],"pdf_url":"https://arxiv.org/pdf/2507.02428v1.pdf","comment":"This version has been reviewed and accepted for presentation at the\n  InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5\n  pages and 3 tables"},{"id":"http://arxiv.org/abs/2406.07016v5","updated":"2025-07-03T08:26:13Z","published":"2024-06-11T07:16:34Z","title":"Delving into LLM-assisted writing in biomedical publications through\n  excess vocabulary","summary":"  Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic.\n","authors":["Dmitry Kobak","Rita González-Márquez","Emőke-Ágnes Horvát","Jan Lause"],"pdf_url":"https://arxiv.org/pdf/2406.07016v5.pdf","comment":"v5: Reverting to v3"},{"id":"http://arxiv.org/abs/2507.02407v1","updated":"2025-07-03T08:01:26Z","published":"2025-07-03T08:01:26Z","title":"Benchmarking Akan ASR Models Across Domain-Specific Datasets: A\n  Comparative Evaluation of Performance, Scalability, and Adaptability","summary":"  Most existing automatic speech recognition (ASR) research evaluate models\nusing in-domain datasets. However, they seldom evaluate how they generalize\nacross diverse speech contexts. This study addresses this gap by benchmarking\nseven Akan ASR models built on transformer architectures, such as Whisper and\nWav2Vec2, using four Akan speech corpora to determine their performance. These\ndatasets encompass various domains, including culturally relevant image\ndescriptions, informal conversations, biblical scripture readings, and\nspontaneous financial dialogues. A comparison of the word error rate and\ncharacter error rate highlighted domain dependency, with models performing\noptimally only within their training domains while showing marked accuracy\ndegradation in mismatched scenarios. This study also identified distinct error\nbehaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned\nWhisper Akan models led to more fluent but potentially misleading transcription\nerrors, Wav2Vec2 produced more obvious yet less interpretable outputs when\nencountering unfamiliar inputs. This trade-off between readability and\ntransparency in ASR errors should be considered when selecting architectures\nfor low-resource language (LRL) applications. These findings highlight the need\nfor targeted domain adaptation techniques, adaptive routing strategies, and\nmultilingual training frameworks for Akan and other LRLs.\n","authors":["Mark Atta Mensah","Isaac Wiafe","Akon Ekpezu","Justice Kwame Appati","Jamal-Deen Abdulai","Akosua Nyarkoa Wiafe-Akenten","Frank Ernest Yeboah","Gifty Odame"],"pdf_url":"https://arxiv.org/pdf/2507.02407v1.pdf","comment":"This version has been reviewed and accepted for presentation at the\n  Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025\n  in Munich, Germany. 17 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2506.14634v3","updated":"2025-07-03T07:58:18Z","published":"2025-06-17T15:28:53Z","title":"AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation","summary":"  The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.\n","authors":["Leah von der Heyde","Anna-Carolina Haensch","Bernd Weiß","Jessica Daikeler"],"pdf_url":"https://arxiv.org/pdf/2506.14634v3.pdf","comment":"to appear in Survey Research Methods"},{"id":"http://arxiv.org/abs/2507.02380v1","updated":"2025-07-03T07:22:06Z","published":"2025-07-03T07:22:06Z","title":"JoyTTS: LLM-based Spoken Chatbot With Voice Cloning","summary":"  JoyTTS is an end-to-end spoken chatbot that combines large language models\n(LLM) with text-to-speech (TTS) technology, featuring voice cloning\ncapabilities. This project is built upon the open-source MiniCPM-o and\nCosyVoice2 models and trained on 2000 hours of conversational data. We have\nalso provided the complete training code to facilitate further development and\noptimization by the community. On the testing machine seed-tts-zh, it achieves\na SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.\nThe code and models, along with training and inference scripts, are available\nat https://github.com/jdh-algo/JoyTTS.git.\n","authors":["Fangru Zhou","Jun Zhao","Guoxin Wang"],"pdf_url":"https://arxiv.org/pdf/2507.02380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02378v1","updated":"2025-07-03T07:19:56Z","published":"2025-07-03T07:19:56Z","title":"Efficient Code LLM Training via Distribution-Consistent and\n  Diversity-Aware Data Selection","summary":"  Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs.\n","authors":["Weijie Lyu","Sheng-Jun Huang","Xuan Xia"],"pdf_url":"https://arxiv.org/pdf/2507.02378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02364v1","updated":"2025-07-03T06:52:44Z","published":"2025-07-03T06:52:44Z","title":"QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency\n  in Hybrid Quantum-Classical Transformers","summary":"  Parameterized quantum circuits (PQCs) have recently emerged as promising\ncomponents for enhancing the expressibility of neural architectures. In this\nwork, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the\nfeedforward network (FFN) modules of a compact BERT variant are replaced by\nPQC-based layers. This design is motivated by the dominant parameter\ncontribution of FFNs, which account for approximately two-thirds of the\nparameters within standard Transformer encoder blocks. While prior studies have\nprimarily integrated PQCs into self-attention modules, our work focuses on the\nFFN and systematically investigates the trade-offs between PQC depth,\nexpressibility, and trainability. Our final PQC architecture incorporates a\nresidual connection, both $R_Y$ and $R_Z$ rotations, and an alternating\nentanglement strategy to ensure stable training and high expressibility. Our\nexperiments, conducted on a classical simulator, on the SST-2 and DBpedia\nbenchmarks demonstrate two key findings. First, a carefully configured\nQFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its\nclassical counterpart in a full-data setting while reducing FFN-specific\nparameters by over 99%. Second, our model exhibits a consistent and competitive\nedge in few-shot learning scenarios, confirming its potential for superior data\nefficiency. These results, supported by an ablation study on a non-optimized\nPQC that failed to learn, confirm that PQCs can serve as powerful and\nparameter-efficient alternatives to classical FFNs when co-designed with\nfoundational deep learning principles.\n","authors":["Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2507.02364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08010v3","updated":"2025-07-03T06:51:31Z","published":"2023-11-14T09:09:58Z","title":"Improving the Robustness of Distantly-Supervised Named Entity\n  Recognition via Uncertainty-Aware Teacher Learning and Student-Student\n  Collaborative Learning","summary":"  Distantly-Supervised Named Entity Recognition (DS-NER) is widely used in\nreal-world scenarios. It can effectively alleviate the burden of annotation by\nmatching entities in existing knowledge bases with snippets in the text but\nsuffer from the label noise. Recent works attempt to adopt the teacher-student\nframework to gradually refine the training labels and improve the overall\nrobustness. However, these teacher-student methods achieve limited performance\nbecause the poor calibration of the teacher network produces incorrectly\npseudo-labeled samples, leading to error propagation. Therefore, we propose:\n(1) Uncertainty-Aware Teacher Learning that leverages the prediction\nuncertainty to reduce the number of incorrect pseudo labels in the\nself-training stage; (2) Student-Student Collaborative Learning that allows the\ntransfer of reliable labels between two student networks instead of\nindiscriminately relying on all pseudo labels from its teacher, and further\nenables a full exploration of mislabeled samples rather than simply filtering\nunreliable pseudo-labeled samples. We evaluate our proposed method on five\nDS-NER datasets, demonstrating that our method is superior to the\nstate-of-the-art DS-NER methods.\n","authors":["Shuzheng Si","Helan Hu","Haozhe Zhao","Shuang Zeng","Kaikai An","Zefan Cai","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2311.08010v3.pdf","comment":"ACL 2024 (Findings)"},{"id":"http://arxiv.org/abs/2507.02357v1","updated":"2025-07-03T06:43:05Z","published":"2025-07-03T06:43:05Z","title":"Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and\n  Confidence-Informed Ensembling for Multimodal Large Language Models","summary":"  This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.\n","authors":["Christian Jaumann","Annemarie Friedrich","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2507.02357v1.pdf","comment":"Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025"},{"id":"http://arxiv.org/abs/2505.21880v2","updated":"2025-07-03T06:38:34Z","published":"2025-05-28T01:54:28Z","title":"Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation","summary":"  This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications.\n","authors":["Yu-Lun Song","Chung-En Tsern","Che-Cheng Wu","Yu-Ming Chang","Syuan-Bo Huang","Wei-Chu Chen","Michael Chia-Liang Lin","Yu-Ta Lin"],"pdf_url":"https://arxiv.org/pdf/2505.21880v2.pdf","comment":"8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025"},{"id":"http://arxiv.org/abs/2507.01923v2","updated":"2025-07-03T06:29:26Z","published":"2025-07-02T17:32:35Z","title":"Decision-Oriented Text Evaluation","summary":"  Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.\n","authors":["Yu-Shiang Huang","Chuan-Ju Wang","Chung-Chi Chen"],"pdf_url":"https://arxiv.org/pdf/2507.01923v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11556v2","updated":"2025-07-03T06:24:49Z","published":"2024-12-16T08:42:00Z","title":"Token Prepending: A Training-Free Approach for Eliciting Better Sentence\n  Embeddings from LLMs","summary":"  Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost.\n","authors":["Yuchen Fu","Zifeng Cheng","Zhiwei Jiang","Zhonghui Wang","Yafeng Yin","Zhengliang Li","Qing Gu"],"pdf_url":"https://arxiv.org/pdf/2412.11556v2.pdf","comment":"Accept to ACL 2025 (Oral). Code are available on\n  https://github.com/fuyuchenIfyw/token_prepending.git"},{"id":"http://arxiv.org/abs/2503.00958v2","updated":"2025-07-03T06:07:21Z","published":"2025-03-02T16:47:31Z","title":"Layered Insights: Generalizable Analysis of Authorial Style by\n  Leveraging All Transformer Layers","summary":"  We propose a new approach for the authorship attribution task that leverages\nthe various linguistic representations learned at different layers of\npre-trained transformer-based models. We evaluate our approach on three\ndatasets, comparing it to a state-of-the-art baseline in in-domain and\nout-of-domain scenarios. We found that utilizing various transformer layers\nimproves the robustness of authorship attribution models when tested on\nout-of-domain data, resulting in new state-of-the-art results. Our analysis\ngives further insights into how our model's different layers get specialized in\nrepresenting certain stylistic features that benefit the model when tested out\nof the domain.\n","authors":["Milad Alshomary","Nikhil Reddy Varimalla","Vishal Anand","Smaranda Muresan","Kathleen McKeown"],"pdf_url":"https://arxiv.org/pdf/2503.00958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01352v2","updated":"2025-07-03T05:58:40Z","published":"2025-07-02T04:40:29Z","title":"Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy","summary":"  Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.\n","authors":["Chris Yuhao Liu","Liang Zeng","Yuzhen Xiao","Jujie He","Jiacai Liu","Chaojie Wang","Rui Yan","Wei Shen","Fuxiang Zhang","Jiacheng Xu","Yang Liu","Yahui Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.01352v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.17828v2","updated":"2025-07-03T05:12:51Z","published":"2025-06-21T21:49:02Z","title":"Aligning Frozen LLMs by Reinforcement Learning: An Iterative\n  Reweight-then-Optimize Approach","summary":"  Aligning large language models (LLMs) with human preferences usually requires\nfine-tuning methods such as RLHF and DPO. These methods directly optimize the\nmodel parameters, so they cannot be used in test-time to improve model\nperformance, nor are they applicable when the model weights are not accessible.\nIn contrast, test-time methods sidestep weight updates by leveraging reward\nfunctions to guide and improve output quality. However, they incur high\ninference costs, and their one-shot guidance is often based on imperfect reward\nor value functions, leading to suboptimal outputs. In this work, we present a\nmethod named Iterative Reweight-then-Optimize (IRO), a reinforcement learning\n(RL) framework that performs RL-style alignment of the (frozen) base model\nwithout touching its parameters. During training, each iteration (i) samples\ncandidates from the base model, (ii) resamples using current value functions,\nand (iii) trains a new lightweight value function that guides the next decoding\npass. At test time, the value functions are used to guide the base model\ngeneration via a search-based optimization process. Notably, users can apply\nIRO to align a model on their own dataset, similar to OpenAI's reinforcement\nfine-tuning (RFT), but without requiring access to the model weights.\n","authors":["Xinnan Zhang","Chenliang Li","Siliang Zeng","Jiaxiang Li","Zhongruo Wang","Kaixiang Lin","Songtao Lu","Alfredo Garcia","Mingyi Hong"],"pdf_url":"https://arxiv.org/pdf/2506.17828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22618v3","updated":"2025-07-03T04:51:05Z","published":"2025-05-28T17:39:15Z","title":"Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding","summary":"  Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.\n","authors":["Chengyue Wu","Hao Zhang","Shuchen Xue","Zhijian Liu","Shizhe Diao","Ligeng Zhu","Ping Luo","Song Han","Enze Xie"],"pdf_url":"https://arxiv.org/pdf/2505.22618v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10576v3","updated":"2025-07-03T04:28:09Z","published":"2024-06-15T09:31:03Z","title":"Bypass Back-propagation: Optimization-based Structural Pruning for Large\n  Language Models via Policy Gradient","summary":"  Recent Large-Language Models (LLMs) pruning methods typically operate at the\npost-training phase without the expensive weight finetuning, however, their\npruning criteria often rely on heuristically hand-crafted metrics, potentially\nleading to suboptimal performance. We instead propose a novel\noptimization-based structural pruning that learns the pruning masks in a\nprobabilistic space directly by optimizing the loss of the pruned model. To\npreserve efficiency, our method eliminates the back-propagation through the LLM\nper se during optimization, requiring only the forward pass of the LLM. We\nachieve this by learning an underlying Bernoulli distribution to sample binary\npruning masks, where we decouple the Bernoulli parameters from LLM loss,\nfacilitating efficient optimization via policy gradient estimator without\nback-propagation. Thus, our method can 1) support global and heterogeneous\npruning (i.e., automatically determine different redundancy for different\nlayers), and 2) optionally initialize with a metric-based method (for our\nBernoulli distributions). Extensive experiments conducted on LLaMA, LLaMA-2,\nLLaMA-3, Vicuna, and Mistral models using the C4 and WikiText2 datasets\ndemonstrate the promising performance of our method in efficiency and\neffectiveness. Code is available at\nhttps://github.com/ethanygao/backprop-free_LLM_pruning.\n","authors":["Yuan Gao","Zujing Liu","Weizhong Zhang","Bo Du","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2406.10576v3.pdf","comment":"ACL2025 Main Accepted"},{"id":"http://arxiv.org/abs/2501.03262v4","updated":"2025-07-03T04:17:04Z","published":"2025-01-04T02:08:06Z","title":"REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt\n  and Reward Models","summary":"  Large Language Models (LLMs) fine-tuned via Reinforcement Learning from Human\nFeedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR)\nsignificantly improve the alignment of human-AI values and further raise the\nupper bound of AI capabilities, particularly in reasoning-intensive,\nlong-context Chain-of-Thought (long-CoT) tasks. However, existing RLHF (or\nRLVR) frameworks commonly face challenges such as inference bottlenecks and\ncomplexity barriers, restricting their accessibility for newcomers. To bridge\nthis gap, we introduce \\textbf{OpenRLHF}, a user-friendly, scalable, and\neasy-to-learn open-source RLHF framework built upon Ray, vLLM, DeepSpeed, and\nHuggingFace Transformers, featuring a simplified design, clear code structure,\nand comprehensive documentation to facilitate entry for researchers and\npractitioners. Experimental results show that OpenRLHF achieves superior\ntraining efficiency with speedups ranging from 1.22x to 1.68x across different\nmodel sizes compared to state-of-the-art frameworks, while requiring\nsignificantly fewer lines of code for implementation. OpenRLHF is publicly\navailable at https://github.com/OpenRLHF/OpenRLHF, and has already been adopted\nby leading institutions to accelerate RLHF research and learning.\n","authors":["Jian Hu","Xibin Wu","Wei Shen","Jason Klein Liu","Zilin Zhu","Weixun Wang","Songlin Jiang","Haoran Wang","Hao Chen","Bin Chen","Weikai Fang"," Xianyu","Yu Cao","Haotian Xu"],"pdf_url":"https://arxiv.org/pdf/2501.03262v4.pdf","comment":"fix typo"},{"id":"http://arxiv.org/abs/2507.02302v1","updated":"2025-07-03T04:13:01Z","published":"2025-07-03T04:13:01Z","title":"DoMIX: An Efficient Framework for Exploiting Domain Knowledge in\n  Fine-Tuning","summary":"  Domain-Adaptive Pre-training (DAP) has recently gained attention for its\neffectiveness in fine-tuning pre-trained models. Building on this, continual\nDAP has been explored to develop pre-trained models capable of incrementally\nincorporating different domain datasets. However, existing continual DAP\nmethods face several limitations: (1) high computational cost and GPU memory\nusage during training; (2) sensitivity to incremental data order; and (3)\nproviding a single, generalized model for all end tasks, which contradicts the\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\naddresses these challenges by leveraging LoRA modules, a representative\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\nand parallel domain-adaptive pre-training that is robust to domain order and\neffectively utilizes accumulated knowledge to provide tailored pre-trained\nmodels for specific tasks. We also demonstrate that our method can be extended\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\nat https://github.com/dohoonkim-ai/DoMIX.\n","authors":["Dohoon Kim","Donghun Kang","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2507.02302v1.pdf","comment":"22 pages, 5 figures, ACL 2025 Main"},{"id":"http://arxiv.org/abs/2503.18681v3","updated":"2025-07-03T04:04:04Z","published":"2025-03-24T13:53:00Z","title":"Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of\n  Multi-Modal Large Language Models","summary":"  Sarcasm detection, as a crucial research direction in the field of Natural\nLanguage Processing (NLP), has attracted widespread attention. Traditional\nsarcasm detection tasks have typically focused on single-modal approaches\n(e.g., text), but due to the implicit and subtle nature of sarcasm, such\nmethods often fail to yield satisfactory results. In recent years, researchers\nhave shifted the focus of sarcasm detection to multi-modal approaches. However,\neffectively leveraging multi-modal information to accurately identify sarcastic\ncontent remains a challenge that warrants further exploration. Leveraging the\npowerful integrated processing capabilities of Multi-Modal Large Language\nModels (MLLMs) for various information sources, we propose an innovative\nmulti-modal Commander-GPT framework. Inspired by military strategy, we first\ndecompose the sarcasm detection task into six distinct sub-tasks. A central\ncommander (decision-maker) then assigns the best-suited large language model to\naddress each specific sub-task. Ultimately, the detection results from each\nmodel are aggregated to identify sarcasm. We conducted extensive experiments on\nMMSD and MMSD 2.0, utilizing four multi-modal large language models and six\nprompting strategies. Our experiments demonstrate that our approach achieves\nstate-of-the-art performance, with a 19.3% improvement in F1 score, without\nnecessitating fine-tuning or ground-truth rationales.\n","authors":["Yazhou Zhang","Chunwang Zou","Bo Wang","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2503.18681v3.pdf","comment":"Our original goal was to use Commander-GPT: Dividing and Routing for\n  Multimodal Sarcasm Detection (arXiv:2506.19420) to replace Commander-GPT:\n  Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large\n  Language Models (arXiv:2503.18681). Due to various reasons, both versions\n  were released, so we would like to withdraw the latter"},{"id":"http://arxiv.org/abs/2506.21191v2","updated":"2025-07-03T04:01:44Z","published":"2025-06-26T12:49:07Z","title":"Prompt-Guided Turn-Taking Prediction","summary":"  Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts.\n","authors":["Koji Inoue","Mikey Elmers","Yahui Fu","Zi Haur Pang","Divesh Lala","Keiko Ochi","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2506.21191v2.pdf","comment":"This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work"},{"id":"http://arxiv.org/abs/2311.14727v2","updated":"2025-07-03T03:54:21Z","published":"2023-11-20T13:08:21Z","title":"Optimal strategies to perform multilingual analysis of social content\n  for a novel dataset in the tourism domain","summary":"  The rising influence of social media platforms in various domains, including\ntourism, has highlighted the growing need for efficient and automated Natural\nLanguage Processing (NLP) strategies to take advantage of this valuable\nresource. However, the transformation of multilingual, unstructured, and\ninformal texts into structured knowledge still poses significant challenges,\nmost notably the never-ending requirement for manually annotated data to train\ndeep learning classifiers. In this work, we study different NLP techniques to\nestablish the best ones to obtain competitive performances while keeping the\nneed for training annotated data to a minimum. To do so, we built the first\npublicly available multilingual dataset (French, English, and Spanish) for the\ntourism domain, composed of tourism-related tweets. The dataset includes\nmultilayered, manually revised annotations for Named Entity Recognition (NER)\nfor Locations and Fine-grained Thematic Concepts Extraction mapped to the\nThesaurus of Tourism and Leisure Activities of the World Tourism Organization,\nas well as for Sentiment Analysis at the tweet level. Extensive experimentation\ncomparing various few-shot and fine-tuning techniques with modern language\nmodels demonstrate that modern few-shot techniques allow us to obtain\ncompetitive results for all three tasks with very little annotation data: 5\ntweets per label (15 in total) for Sentiment Analysis, 30 tweets for Named\nEntity Recognition of Locations and 1K tweets annotated with fine-grained\nthematic concepts, a highly fine-grained sequence labeling task based on an\ninventory of 315 classes. We believe that our results, grounded in a novel\ndataset, pave the way for applying NLP to new domain-specific applications,\nreducing the need for manual annotations and circumventing the complexities of\nrule-based, ad-hoc solutions.\n","authors":["Maxime Masson","Rodrigo Agerri","Christian Sallaberry","Marie-Noelle Bessagnet","Annig Le Parc Lacayrelle","Philippe Roose"],"pdf_url":"https://arxiv.org/pdf/2311.14727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02287v1","updated":"2025-07-03T03:51:33Z","published":"2025-07-03T03:51:33Z","title":"Seeing Through Green: Text-Based Classification and the Firm's Returns\n  from Green Patents","summary":"  This paper introduces Natural Language Processing for identifying ``true''\ngreen patents from official supporting documents. We start our training on\nabout 12.4 million patents that had been classified as green from previous\nliterature. Thus, we train a simple neural network to enlarge a baseline\ndictionary through vector representations of expressions related to\nenvironmental technologies. After testing, we find that ``true'' green patents\nrepresent about 20\\% of the total of patents classified as green from previous\nliterature. We show heterogeneity by technological classes, and then check that\n`true' green patents are about 1\\% less cited by following inventions. In the\nsecond part of the paper, we test the relationship between patenting and a\ndashboard of firm-level financial accounts in the European Union. After\ncontrolling for reverse causality, we show that holding at least one ``true''\ngreen patent raises sales, market shares, and productivity. If we restrict the\nanalysis to high-novelty ``true'' green patents, we find that they also yield\nhigher profits. Our findings underscore the importance of using text analyses\nto gauge finer-grained patent classifications that are useful for policymaking\nin different domains.\n","authors":["Lapo Santarlasci","Armando Rungi","Antonio Zinilli"],"pdf_url":"https://arxiv.org/pdf/2507.02287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00903v3","updated":"2025-07-03T03:31:52Z","published":"2024-10-01T17:46:21Z","title":"Causal Representation Learning with Generative Artificial Intelligence:\n  Application to Texts as Treatments","summary":"  In this paper, we demonstrate how to enhance the validity of causal inference\nwith unstructured high-dimensional treatments like texts, by leveraging the\npower of generative Artificial Intelligence (GenAI). Specifically, we propose\nto use a deep generative model such as large language models (LLMs) to\nefficiently generate treatments and use their internal representation for\nsubsequent causal effect estimation. We show that the knowledge of this true\ninternal representation helps disentangle the treatment features of interest,\nsuch as specific sentiments and certain topics, from other possibly unknown\nconfounding features. Unlike existing methods, the proposed GenAI-Powered\nInference (GPI) methodology eliminates the need to learn causal representation\nfrom the data, and hence produces more accurate and efficient estimates. We\nformally establish the conditions required for the nonparametric identification\nof the average treatment effect, propose an estimation strategy that avoids the\nviolation of the overlap assumption, and derive the asymptotic properties of\nthe proposed estimator through the application of double machine learning.\nFinally, using an instrumental variables approach, we extend the proposed\nmethodology to the settings in which the treatment feature is based on human\nperception. The proposed GPI methodology is also applicable to text reuse where\nan LLM is used to regenerate existing texts. We conduct simulation and\nempirical studies, using the generated text data from an open-source LLM, Llama\n3, to illustrate the advantages of our estimator over state-of-the-art causal\nrepresentation learning algorithms.\n","authors":["Kosuke Imai","Kentaro Nakamura"],"pdf_url":"https://arxiv.org/pdf/2410.00903v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12816v3","updated":"2025-07-03T03:25:07Z","published":"2025-04-17T10:21:15Z","title":"SMARTe: Slot-based Method for Accountable Relational Triple extraction","summary":"  Relational Triple Extraction (RTE) is a fundamental task in Natural Language\nProcessing (NLP). However, prior research has primarily focused on optimizing\nmodel performance, with limited efforts to understand the internal mechanisms\ndriving these models. Many existing methods rely on complex preprocessing to\ninduce specific interactions, often resulting in opaque systems that may not\nfully align with their theoretical foundations. To address these limitations,\nwe propose SMARTe: a Slot-based Method for Accountable Relational Triple\nextraction. SMARTe introduces intrinsic interpretability through a slot\nattention mechanism and frames the task as a set prediction problem. Slot\nattention consolidates relevant information into distinct slots, ensuring all\npredictions can be explicitly traced to learned slot representations and the\ntokens contributing to each predicted relational triple. While emphasizing\ninterpretability, SMARTe achieves performance comparable to state-of-the-art\nmodels. Evaluations on the NYT and WebNLG datasets demonstrate that adding\ninterpretability does not compromise performance. Furthermore, we conducted\nqualitative assessments to showcase the explanations provided by SMARTe, using\nattention heatmaps that map to their respective tokens. We conclude with a\ndiscussion of our findings and propose directions for future research. Our code\nis available at https://github.com/Chen-XueWen/SMARTe.\n","authors":["Xue Wen Tan","Stanley Kok"],"pdf_url":"https://arxiv.org/pdf/2504.12816v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02259v1","updated":"2025-07-03T03:11:50Z","published":"2025-07-03T03:11:50Z","title":"MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory\n  Agent","summary":"  Despite improvements by length extrapolation, efficient attention and memory\nmodules, handling infinitely long documents with linear complexity without\nperformance degradation during extrapolation remains the ultimate challenge in\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\nalgorithm to facilitate training via independent-context multi-conversation\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.\n","authors":["Hongli Yu","Tinghong Chen","Jiangtao Feng","Jiangjie Chen","Weinan Dai","Qiying Yu","Ya-Qin Zhang","Wei-Ying Ma","Jingjing Liu","Mingxuan Wang","Hao Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.02259v1.pdf","comment":"Project Page: https://memagent-sialab.github.io/"},{"id":"http://arxiv.org/abs/2502.06106v2","updated":"2025-07-03T03:07:00Z","published":"2025-02-10T02:35:53Z","title":"Circuit-tuning: A Mechanistic Approach for Identifying Parameter\n  Redundancy and Fine-tuning Neural Networks","summary":"  The study of mechanistic interpretability aims to reverse-engineer a model to\nexplain its behaviors. While recent studies have focused on the static\nmechanism of a certain behavior, the learning dynamics inside a model remain to\nbe explored. In this work, we develop an interpretable fine-tuning method for\nanalyzing the mechanism behind learning. We first introduce the concept of\nnode-level intrinsic dimensionality to describe the learning process of a model\nin a computational graph. Based on our theory, we propose circuit-tuning, a\ntwo-stage algorithm that iteratively builds the minimal subgraph for a specific\ntask and updates the key parameters in a heuristic way. Experimental results\nconfirm the existence of the intrinsic dimensionality at the node level and\ndemonstrate the effectiveness of our method for transparent and interpretable\nfine-tuning. We visualize and analyze the circuits before, during, and after\nfine-tuning, providing new insights into the self-organization mechanism of a\nneural network in the learning process.\n","authors":["Yueyan Li","Wenhao Gao","Caixia Yuan","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.00606v2","updated":"2025-07-03T02:30:05Z","published":"2025-07-01T09:39:04Z","title":"Mixture of Reasonings: Teach Large Language Models to Reason with\n  Adaptive Strategies","summary":"  Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning. Our experiments show that MoR significantly\nenhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT\nprompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates\nthe need for task-specific prompts, offering a generalizable solution for\nrobust reasoning across diverse tasks.\n","authors":["Tao Xiong","Xavier Hu","Wenyan Fan","Shengyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.00606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02221v1","updated":"2025-07-03T00:55:58Z","published":"2025-07-03T00:55:58Z","title":"GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic\n  Data Commons","summary":"  Motivation: The Genomic Data Commons (GDC) provides access to high quality,\nharmonized cancer genomics data through a unified curation and analysis\nplatform centered around patient cohorts. While GDC users can interactively\ncreate complex cohorts through the graphical Cohort Builder, users (especially\nnew ones) may struggle to find specific cohort descriptors across hundreds of\npossible fields and properties. However, users may be better able to describe\ntheir desired cohort in free-text natural language.\n  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for\ncurating cohorts from the GDC. GDC Cohort Copilot automatically generates the\nGDC cohort filter corresponding to a user-input natural language description of\ntheir desired cohort, before exporting the cohort back to the GDC for further\nanalysis. An interactive user interface allows users to further refine the\ngenerated cohort. We develop and evaluate multiple large language models (LLMs)\nfor GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC\nCohort LLM achieves better results than GPT-4o prompting in generating GDC\ncohorts.\n  Availability and implementation: The standalone docker image for GDC Cohort\nCopilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.\nSource code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC\nCohort LLM weights are available at https://huggingface.co/uc-ctds.\n","authors":["Steven Song","Anirudh Subramanyam","Zhenyu Zhang","Aarti Venkat","Robert L. Grossman"],"pdf_url":"https://arxiv.org/pdf/2507.02221v1.pdf","comment":"11 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2507.02212v1","updated":"2025-07-03T00:21:38Z","published":"2025-07-03T00:21:38Z","title":"SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in\n  Academic Papers","summary":"  Graphical Abstracts (GAs) play a crucial role in visually conveying the key\nfindings of scientific papers. While recent research has increasingly\nincorporated visual materials such as Figure 1 as de facto GAs, their potential\nto enhance scientific communication remains largely unexplored. Moreover,\ndesigning effective GAs requires advanced visualization skills, creating a\nbarrier to their widespread adoption. To tackle these challenges, we introduce\nSciGA-145k, a large-scale dataset comprising approximately 145,000 scientific\npapers and 1.14 million figures, explicitly designed for supporting GA\nselection and recommendation as well as facilitating research in automated GA\ngeneration. As a preliminary step toward GA design support, we define two\ntasks: 1) Intra-GA recommendation, which identifies figures within a given\npaper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,\nwhich retrieves GAs from other papers to inspire the creation of new GAs. We\nprovide reasonable baseline models for these tasks. Furthermore, we propose\nConfidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation\nmetric that offers a fine-grained analysis of model behavior. CAR addresses\nlimitations in traditional ranking-based metrics by considering cases where\nmultiple figures within a paper, beyond the explicitly labeled GA, may also\nserve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a\nfoundation for advancing visual scientific communication while contributing to\nthe development of AI for Science.\n","authors":["Takuro Kawada","Shunsuke Kitada","Sota Nemoto","Hitoshi Iyatomi"],"pdf_url":"https://arxiv.org/pdf/2507.02212v1.pdf","comment":"21 pages, 15 figures, 4 tables. Project Page:\n  https://iyatomilab.github.io/SciGA/"}]},"2025-07-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.16765v3","updated":"2025-07-02T23:43:36Z","published":"2024-11-25T03:13:08Z","title":"SHuBERT: Self-Supervised Sign Language Representation Learning via\n  Multi-Stream Cluster Prediction","summary":"  Sign language processing has traditionally relied on task-specific models,\nlimiting the potential for transfer learning across tasks. Pre-training methods\nfor sign language have typically focused on either supervised pre-training,\nwhich cannot take advantage of unlabeled data, or context-independent (frame or\nvideo segment) representations, which ignore the effects of relationships\nacross time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a\nself-supervised contextual representation model learned from approximately\n1,000 hours of American Sign Language video. SHuBERT adapts masked token\nprediction objectives to multi-stream visual sign language input, learning to\npredict multiple targets corresponding to clustered hand, face, and body pose\nstreams. SHuBERT achieves state-of-the-art performance across multiple tasks\nincluding sign language translation, isolated sign language recognition, and\nfingerspelling detection.\n","authors":["Shester Gueuwou","Xiaodan Du","Greg Shakhnarovich","Karen Livescu","Alexander H. Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16765v3.pdf","comment":"Fixed Figure 1. ACL 2025"},{"id":"http://arxiv.org/abs/2507.02200v1","updated":"2025-07-02T23:41:31Z","published":"2025-07-02T23:41:31Z","title":"ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text\n  Recognition with Chain-of-Thought Reasoning","summary":"  Event stream based scene text recognition is a newly arising research topic\nin recent years which performs better than the widely used RGB cameras in\nextremely challenging scenarios, especially the low illumination, fast motion.\nExisting works either adopt end-to-end encoder-decoder framework or large\nlanguage models for enhanced recognition, however, they are still limited by\nthe challenges of insufficient interpretability and weak contextual logical\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\nevent stream into tokens and utilize a Llama tokenizer to encode the given\ngeneration prompt. A Q-former is used to align the vision token to the\npre-trained large language model Vicuna-7B and output both the answer and\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\nalso propose a large-scale CoT dataset to train our framework via a three stage\nprocessing (i.e., generation, polish, and expert verification). This dataset\nprovides a solid data foundation for the development of subsequent\nreasoning-based large models. Extensive experiments on three event stream STR\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\neffectiveness and interpretability of our proposed framework. The source code\nand pre-trained models will be released on\nhttps://github.com/Event-AHU/ESTR-CoT.\n","authors":["Xiao Wang","Jingtao Jiang","Qiang Chen","Lan Chen","Lin Zhu","Yaowei Wang","Yonghong Tian","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2507.02200v1.pdf","comment":"A Strong Baseline for Reasoning based Event Stream Scene Text\n  Recognition"},{"id":"http://arxiv.org/abs/2507.02199v1","updated":"2025-07-02T23:35:21Z","published":"2025-07-02T23:35:21Z","title":"Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer","summary":"  Chain-of-thought (CoT) reasoning has enabled transformer-based language\nmodels to excel at complex mathematics and multi-step planning. However, in\nstandard decoder-only architectures, these reasoning steps are externalized in\nnatural language, improving interpretability at the cost of efficiency. To\ncapture reasoning that is not easily represented in words, many works have\nexplored recurrent architectures that aim to internalize reasoning in latent\nspace, potentially supporting latent CoT. In this paper, we investigate whether\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\nthat reuses layers at inference time without increasing parameter count. We\nexamine the model's internal behavior on arithmetic tasks using a suite of\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\nfinal and intermediate result tokens. Furthermore, we uncover significant\nprobing inconsistencies across recurrent blocks, where the interpretability of\nhidden states depends heavily on both the layer index and the decoding method.\nFinally, we empirically show that increasing recurrence depth yields only\nmarginal gains and falls well short of models that explicitly externalize\nreasoning steps. The code is available at\nhttps://github.com/wenquanlu/huginn-latent-cot.\n","authors":["Wenquan Lu","Yuechuan Yang","Kyle Lee","Yanshu Li","Enqi Liu"],"pdf_url":"https://arxiv.org/pdf/2507.02199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02176v1","updated":"2025-07-02T22:16:42Z","published":"2025-07-02T22:16:42Z","title":"Analyzing and Improving Speaker Similarity Assessment for Speech\n  Synthesis","summary":"  Modeling voice identity is challenging due to its multifaceted nature. In\ngenerative speech systems, identity is often assessed using automatic speaker\nverification (ASV) embeddings, designed for discrimination rather than\ncharacterizing identity. This paper investigates which aspects of a voice are\ncaptured in such representations. We find that widely used ASV embeddings focus\nmainly on static features like timbre and pitch range, while neglecting dynamic\nelements such as rhythm. We also identify confounding factors that compromise\nspeaker similarity measurements and suggest mitigation strategies. To address\nthese gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm\npatterns. This work contributes to the ongoing challenge of assessing speaker\nidentity consistency in the context of ever-better voice cloning systems. We\npublicly release our code.\n","authors":["Marc-André Carbonneau","Benjamin van Niekerk","Hugo Seuté","Jean-Philippe Letendre","Herman Kamper","Julian Zaïdi"],"pdf_url":"https://arxiv.org/pdf/2507.02176v1.pdf","comment":"Accepted at SSW13 - Interspeech 2025 Speech Synthesis Workshop"},{"id":"http://arxiv.org/abs/2306.13840v4","updated":"2025-07-02T21:53:51Z","published":"2023-06-24T02:25:56Z","title":"Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data","summary":"  Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.\n","authors":["Brando Miranda","Alycia Lee","Sudharsan Sundar","Allison Casasola","Rylan Schaeffer","Elyas Obbad","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2306.13840v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15830v3","updated":"2025-07-02T21:36:55Z","published":"2025-06-18T19:17:47Z","title":"Rethinking LLM Training through Information Geometry and Quantum Metrics","summary":"  Optimization in large language models (LLMs) unfolds over high-dimensional\nparameter spaces with non-Euclidean structure. Information geometry frames this\nlandscape using the Fisher information metric, enabling more principled\nlearning via natural gradient descent. Though often impractical, this geometric\nlens clarifies phenomena such as sharp minima, generalization, and observed\nscaling laws. We argue that curvature-aware approaches deepen our understanding\nof LLM training. Finally, we speculate on quantum analogies based on the\nFubini-Study metric and Quantum Fisher Information, hinting at efficient\noptimization in quantum-enhanced systems.\n","authors":["Riccardo Di Sipio"],"pdf_url":"https://arxiv.org/pdf/2506.15830v3.pdf","comment":"9 pages, 1 figure(s)"},{"id":"http://arxiv.org/abs/2501.08496v3","updated":"2025-07-02T21:19:42Z","published":"2025-01-14T23:59:23Z","title":"Quantifying the Importance of Data Alignment in Downstream Model\n  Performance","summary":"  Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization.\n","authors":["Krrish Chawla","Aryan Sahai","Mario DePavia","Sudharsan Sundar","Brando Miranda","Elyas Obbad","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2501.08496v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02145v1","updated":"2025-07-02T21:02:41Z","published":"2025-07-02T21:02:41Z","title":"Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for\n  Dialogue Summarization","summary":"  Dialogue summarization is a challenging task with significant practical value\nin customer service, meeting analysis, and conversational AI. Although large\nlanguage models (LLMs) have achieved substantial progress in summarization\ntasks, the performance of step-by-step reasoning architectures-specifically\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\nabstraction and conciseness. In this work, we present the first comprehensive\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\ndialogue summarization. Our study spans diverse languages, domains, and summary\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\nadvanced evaluation protocols that include both LLM-based automatic metrics and\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\nour findings show that explicit stepwise reasoning does not consistently\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\nto verbosity, factual inconsistencies, and less concise summaries compared to\ntheir non-reasoning counterparts. Through scenario-specific analyses and\ndetailed case studies, we further identify when and why explicit reasoning may\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\nwork provides new insights into the limitations of current reasoning LLMs and\nhighlights the need for targeted modeling and evaluation strategies for\nreal-world dialogue summarization.\n","authors":["Keyan Jin","Yapeng Wang","Leonel Santos","Tao Fang","Xu Yang","Sio Kei Im","Hugo Gonçalo Oliveira"],"pdf_url":"https://arxiv.org/pdf/2507.02145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02135v1","updated":"2025-07-02T20:47:40Z","published":"2025-07-02T20:47:40Z","title":"Dissecting the Impact of Mobile DVFS Governors on LLM Inference\n  Performance and Energy Efficiency","summary":"  Large Language Models (LLMs) are increasingly being integrated into various\napplications and services running on billions of mobile devices. However,\ndeploying LLMs on resource-limited mobile devices faces a significant challenge\ndue to their high demand for computation, memory, and ultimately energy. While\ncurrent LLM frameworks for mobile use three power-hungry components-CPU, GPU,\nand Memory-even when running primarily-GPU LLM models, optimized DVFS governors\nfor CPU, GPU, and memory featured in modern mobile devices operate\nindependently and are oblivious of each other. Motivated by the above\nobservation, in this work, we first measure the energy-efficiency of a SOTA LLM\nframework consisting of various LLM models on mobile phones which showed the\ntriplet mobile governors result in up to 40.4% longer prefilling and decoding\nlatency compared to optimal combinations of CPU, GPU, and memory frequencies\nwith the same energy consumption for sampled prefill and decode lengths.\nSecond, we conduct an in-depth measurement study to uncover how the intricate\ninterplay (or lack of) among the mobile governors cause the above inefficiency\nin LLM inference. Finally, based on these insights, we design FUSE - a unified\nenergy-aware governor for optimizing the energy efficiency of LLM inference on\nmobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the\ntime-to-first-token and time-per-output-token latencies by 7.0%-16.9% and\n25.4%-36.8% on average with the same energy-per-token for various mobile LLM\nmodels.\n","authors":["Zongpu Zhang","Pranab Dash","Y. Charlie Hu","Qiang Xu","Jian Li","Haibing Guan"],"pdf_url":"https://arxiv.org/pdf/2507.02135v1.pdf","comment":"equal contribution between Zhang and Dash"},{"id":"http://arxiv.org/abs/2410.13808v2","updated":"2025-07-02T20:37:50Z","published":"2024-10-17T17:42:10Z","title":"De-mark: Watermark Removal in Large Language Models","summary":"  Watermarking techniques offer a promising way to identify machine-generated\ncontent via embedding covert information into the contents generated from\nlanguage models (LMs). However, the robustness of the watermarking schemes has\nnot been well explored. In this paper, we present De-mark, an advanced\nframework designed to remove n-gram-based watermarks effectively. Our method\nutilizes a novel querying strategy, termed random selection probing, which aids\nin assessing the strength of the watermark and identifying the red-green list\nwithin the n-gram watermark. Experiments on popular LMs, such as Llama3 and\nChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark\nremoval and exploitation tasks.\n","authors":["Ruibo Chen","Yihan Wu","Junfeng Guo","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2410.13808v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2507.02092v1","updated":"2025-07-02T19:17:29Z","published":"2025-07-02T19:17:29Z","title":"Energy-Based Transformers are Scalable Learners and Thinkers","summary":"  Inference-time computation techniques, analogous to human System 2 Thinking,\nhave recently become popular for improving model performances. However, most\nexisting approaches suffer from several limitations: they are modality-specific\n(e.g., working only in text), problem-specific (e.g., verifiable domains like\nmath and coding), or require additional supervision/training on top of\nunsupervised pretraining (e.g., verifiers or verifiable rewards). In this\npaper, we ask the question \"Is it possible to generalize these System 2\nThinking approaches, and develop models that learn to think solely from\nunsupervised learning?\" Interestingly, we find the answer is yes, by learning\nto explicitly verify the compatibility between inputs and\ncandidate-predictions, and then re-framing prediction problems as optimization\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\n(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy\nvalue to every input and candidate-prediction pair, enabling predictions\nthrough gradient descent-based energy minimization until convergence. Across\nboth discrete (text) and continuous (visual) modalities, we find EBTs scale\nfaster than the dominant Transformer++ approach during training, achieving an\nup to 35% higher scaling rate with respect to data, batch size, parameters,\nFLOPs, and depth. During inference, EBTs improve performance with System 2\nThinking by 29% more than the Transformer++ on language tasks, and EBTs\noutperform Diffusion Transformers on image denoising while using fewer forward\npasses. Further, we find that EBTs achieve better results than existing models\non most downstream tasks given the same or worse pretraining performance,\nsuggesting that EBTs generalize better than existing approaches. Consequently,\nEBTs are a promising new paradigm for scaling both the learning and thinking\ncapabilities of models.\n","authors":["Alexi Gladstone","Ganesh Nanduru","Md Mofijul Islam","Peixuan Han","Hyeonjeong Ha","Aman Chadha","Yilun Du","Heng Ji","Jundong Li","Tariq Iqbal"],"pdf_url":"https://arxiv.org/pdf/2507.02092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02088v1","updated":"2025-07-02T19:04:56Z","published":"2025-07-02T19:04:56Z","title":"McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language\n  Models","summary":"  As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.\n","authors":["Tian Lan","Xiangdong Su","Xu Liu","Ruirui Wang","Ke Chang","Jiang Li","Guanglai Gao"],"pdf_url":"https://arxiv.org/pdf/2507.02088v1.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2507.02087v1","updated":"2025-07-02T19:02:18Z","published":"2025-07-02T19:02:18Z","title":"Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions","summary":"  The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes.\n","authors":["Eitan Anzenberg","Arunava Samajpati","Sivasankaran Chandrasekar","Varun Kacholia"],"pdf_url":"https://arxiv.org/pdf/2507.02087v1.pdf","comment":"10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.22405v2","updated":"2025-07-02T17:58:37Z","published":"2025-06-27T17:27:26Z","title":"Sequential Diagnosis with Language Models","summary":"  Artificial intelligence holds great promise for expanding access to expert\nmedical knowledge and reasoning. However, most evaluations of language models\nrely on static vignettes and multiple-choice questions that fail to reflect the\ncomplexity and nuance of evidence-based medicine in real-world settings. In\nclinical practice, physicians iteratively formulate and revise diagnostic\nhypotheses, adapting each subsequent question and test to what they've just\nlearned, and weigh the evolving evidence before committing to a final\ndiagnosis. To emulate this iterative process, we introduce the Sequential\nDiagnosis Benchmark, which transforms 304 diagnostically challenging New\nEngland Journal of Medicine clinicopathological conference (NEJM-CPC) cases\ninto stepwise diagnostic encounters. A physician or AI begins with a short case\nabstract and must iteratively request additional details from a gatekeeper\nmodel that reveals findings only when explicitly queried. Performance is\nassessed not just by diagnostic accuracy but also by the cost of physician\nvisits and tests performed. We also present the MAI Diagnostic Orchestrator\n(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,\nproposes likely differential diagnoses and strategically selects high-value,\ncost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%\ndiagnostic accuracy--four times higher than the 20% average of generalist\nphysicians. MAI-DxO also reduces diagnostic costs by 20% compared to\nphysicians, and 70% compared to off-the-shelf o3. When configured for maximum\naccuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO\ngeneralize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and\nLlama families. We highlight how AI systems, when guided to think iteratively\nand act judiciously, can advance diagnostic precision and cost-effectiveness in\nclinical care.\n","authors":["Harsha Nori","Mayank Daswani","Christopher Kelly","Scott Lundberg","Marco Tulio Ribeiro","Marc Wilson","Xiaoxuan Liu","Viknesh Sounderajah","Jonathan Carlson","Matthew P Lungren","Bay Gross","Peter Hames","Mustafa Suleyman","Dominic King","Eric Horvitz"],"pdf_url":"https://arxiv.org/pdf/2506.22405v2.pdf","comment":"23 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.01951v1","updated":"2025-07-02T17:58:01Z","published":"2025-07-02T17:58:01Z","title":"Test-Time Scaling with Reflective Generative Model","summary":"  We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.\n","authors":["Zixiao Wang","Yuxin Wang","Xiaorui Wang","Mengting Xing","Jie Gao","Jianjun Xu","Guangcan Liu","Chenhui Jin","Zhuo Wang","Shengzhuo Zhang","Hongtao Xie"],"pdf_url":"https://arxiv.org/pdf/2507.01951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01936v1","updated":"2025-07-02T17:46:56Z","published":"2025-07-02T17:46:56Z","title":"The Thin Line Between Comprehension and Persuasion in LLMs","summary":"  Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.\n","authors":["Adrian de Wynter","Tangming Yuan"],"pdf_url":"https://arxiv.org/pdf/2507.01936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01931v1","updated":"2025-07-02T17:44:54Z","published":"2025-07-02T17:44:54Z","title":"Adaptability of ASR Models on Low-Resource Language: A Comparative Study\n  of Whisper and Wav2Vec-BERT on Bangla","summary":"  In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.\n","authors":["Md Sazzadul Islam Ridoy","Sumi Akter","Md. Aminur Rahman"],"pdf_url":"https://arxiv.org/pdf/2507.01931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01921v1","updated":"2025-07-02T17:30:24Z","published":"2025-07-02T17:30:24Z","title":"NaturalThoughts: Selecting and Distilling Reasoning Traces for General\n  Reasoning Tasks","summary":"  Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.\n","authors":["Yang Li","Youssef Emad","Karthik Padthe","Jack Lanchantin","Weizhe Yuan","Thao Nguyen","Jason Weston","Shang-Wen Li","Dong Wang","Ilia Kulikov","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2507.01921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01915v1","updated":"2025-07-02T17:25:26Z","published":"2025-07-02T17:25:26Z","title":"Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment\n  of Large Language Models","summary":"  Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.\n","authors":["Chengao Li","Hanyu Zhang","Yunkun Xu","Hongyan Xue","Xiang Ao","Qing He"],"pdf_url":"https://arxiv.org/pdf/2507.01915v1.pdf","comment":"19 pages, 3 figures. Accepted by ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2507.01903v1","updated":"2025-07-02T17:19:20Z","published":"2025-07-02T17:19:20Z","title":"AI4Research: A Survey of Artificial Intelligence for Scientific Research","summary":"  Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.\n","authors":["Qiguang Chen","Mingda Yang","Libo Qin","Jinhao Liu","Zheng Yan","Jiannan Guan","Dengyun Peng","Yiyan Ji","Hanjing Li","Mengkang Hu","Yimeng Zhang","Yihao Liang","Yuhang Zhou","Jiaqi Wang","Zhi Chen","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2507.01903v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2507.01900v1","updated":"2025-07-02T17:15:05Z","published":"2025-07-02T17:15:05Z","title":"High-Layer Attention Pruning with Rescaling","summary":"  Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.\n","authors":["Songtao Liu","Peng Liu"],"pdf_url":"https://arxiv.org/pdf/2507.01900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03814v3","updated":"2025-07-02T17:14:11Z","published":"2025-04-04T14:41:41Z","title":"Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?","summary":"  Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift.\n","authors":["Grgur Kovač","Jérémy Perez","Rémy Portelas","Peter Ford Dominey","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2504.03814v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01887v1","updated":"2025-07-02T16:57:01Z","published":"2025-07-02T16:57:01Z","title":"MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher\n  Assistants","summary":"  Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.\n","authors":["Dongyi Ding","Tiannan Wang","Chenghao Zhu","Meiling Tao","Yuchen Eleanor Jiang","Wangchunshu Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.01887v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.11764v2","updated":"2025-07-02T16:43:58Z","published":"2025-05-17T00:11:58Z","title":"Towards Universal Semantics With Large Language Models","summary":"  The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond.\n","authors":["Raymond Baartmans","Matthew Raffel","Rahul Vikram","Aiden Deringer","Lizhong Chen"],"pdf_url":"https://arxiv.org/pdf/2505.11764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21848v2","updated":"2025-07-02T16:43:33Z","published":"2025-06-27T01:45:20Z","title":"LinguaSynth: Heterogeneous Linguistic Signals for News Classification","summary":"  Deep learning has significantly advanced NLP, but its reliance on large\nblack-box models introduces critical interpretability and computational\nefficiency concerns. This paper proposes LinguaSynth, a novel text\nclassification framework that strategically integrates five complementary\nlinguistic feature types: lexical, syntactic, entity-level, word-level\nsemantics, and document-level semantics within a transparent logistic\nregression model. Unlike transformer-based architectures, LinguaSynth maintains\ninterpretability and computational efficiency, achieving an accuracy of 84.89\npercent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by\n3.32 percent. Through rigorous feature interaction analysis, we show that\nsyntactic and entity-level signals provide essential disambiguation and\neffectively complement distributional semantics. LinguaSynth sets a new\nbenchmark for interpretable, resource-efficient NLP models and challenges the\nprevailing assumption that deep neural networks are necessary for\nhigh-performing text classification.\n","authors":["Duo Zhang","Junyi Mo"],"pdf_url":"https://arxiv.org/pdf/2506.21848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01872v1","updated":"2025-07-02T16:38:51Z","published":"2025-07-02T16:38:51Z","title":"DIY-MKG: An LLM-Based Polyglot Language Learning System","summary":"  Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.\n","authors":["Kenan Tang","Yanhong Li","Yao Qin"],"pdf_url":"https://arxiv.org/pdf/2507.01872v1.pdf","comment":"Submitted to EMNLP 2025 System Demonstration"},{"id":"http://arxiv.org/abs/2507.01853v1","updated":"2025-07-02T16:07:54Z","published":"2025-07-02T16:07:54Z","title":"Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages","summary":"  The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.\n","authors":["Samridhi Raj Sinha","Rajvee Sheth","Abhishek Upperwal","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2507.01853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01844v1","updated":"2025-07-02T15:58:51Z","published":"2025-07-02T15:58:51Z","title":"Low-Perplexity LLM-Generated Sequences and Where To Find Them","summary":"  As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.\n","authors":["Arthur Wuhrmann","Anastasiia Kucherenko","Andrei Kucharavy"],"pdf_url":"https://arxiv.org/pdf/2507.01844v1.pdf","comment":"Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables"},{"id":"http://arxiv.org/abs/2410.06716v2","updated":"2025-07-02T15:36:54Z","published":"2024-10-09T09:39:55Z","title":"Guaranteed Generation from Large Language Models","summary":"  As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities.\n","authors":["Minbeom Kim","Thibaut Thonet","Jos Rozen","Hwaran Lee","Kyomin Jung","Marc Dymetman"],"pdf_url":"https://arxiv.org/pdf/2410.06716v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2409.20434v3","updated":"2025-07-02T15:34:00Z","published":"2024-09-30T15:53:38Z","title":"QAEncoder: Towards Aligned Representation Learning in Question Answering\n  Systems","summary":"  Modern QA systems entail retrieval-augmented generation (RAG) for accurate\nand trustworthy responses. However, the inherent gap between user queries and\nrelevant documents hinders precise matching. We introduce QAEncoder, a\ntraining-free approach to bridge this gap. Specifically, QAEncoder estimates\nthe expectation of potential queries in the embedding space as a robust\nsurrogate for the document embedding, and attaches document fingerprints to\neffectively distinguish these embeddings. Extensive experiments across diverse\ndatasets, languages, and embedding models confirmed QAEncoder's alignment\ncapability, which offers a simple-yet-effective solution with zero additional\nindex storage, retrieval latency, training costs, or catastrophic forgetting\nand hallucination issues. The repository is publicly available at\nhttps://github.com/IAAR-Shanghai/QAEncoder.\n","authors":["Zhengren Wang","Qinhan Yu","Shida Wei","Zhiyu Li","Feiyu Xiong","Xiaoxing Wang","Simin Niu","Hao Liang","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.20434v3.pdf","comment":"ACL 2025 Oral"},{"id":"http://arxiv.org/abs/2507.01810v1","updated":"2025-07-02T15:27:41Z","published":"2025-07-02T15:27:41Z","title":"Evaluating Structured Output Robustness of Small Language Models for\n  Open Attribute-Value Extraction from Clinical Notes","summary":"  We present a comparative analysis of the parseability of structured outputs\ngenerated by small language models for open attribute-value extraction from\nclinical notes. We evaluate three widely used serialization formats: JSON,\nYAML, and XML, and find that JSON consistently yields the highest parseability.\nStructural robustness improves with targeted prompting and larger models, but\ndeclines for longer documents and certain note types. Our error analysis\nidentifies recurring format-specific failure patterns. These findings offer\npractical guidance for selecting serialization formats and designing prompts\nwhen deploying language models in privacy-sensitive clinical settings.\n","authors":["Nikita Neveditsin","Pawan Lingras","Vijay Mago"],"pdf_url":"https://arxiv.org/pdf/2507.01810v1.pdf","comment":"To appear in the ACL Anthology"},{"id":"http://arxiv.org/abs/2507.01806v1","updated":"2025-07-02T15:24:47Z","published":"2025-07-02T15:24:47Z","title":"LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework\n  for LLMs","summary":"  Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.\n","authors":["Reza Arabpour","Haitz Sáez de Ocáriz Borde","Anastasis Kratsios"],"pdf_url":"https://arxiv.org/pdf/2507.01806v1.pdf","comment":"5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models"},{"id":"http://arxiv.org/abs/2507.01802v1","updated":"2025-07-02T15:21:29Z","published":"2025-07-02T15:21:29Z","title":"The Anatomy of Evidence: An Investigation Into Explainable ICD Coding","summary":"  Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.\n","authors":["Katharina Beckh","Elisa Studeny","Sujan Sai Gannamaneni","Dario Antweiler","Stefan Rüping"],"pdf_url":"https://arxiv.org/pdf/2507.01802v1.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2507.01790v1","updated":"2025-07-02T15:15:14Z","published":"2025-07-02T15:15:14Z","title":"How Do Vision-Language Models Process Conflicting Information Across\n  Modalities?","summary":"  AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.\n","authors":["Tianze Hua","Tian Yun","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2507.01790v1.pdf","comment":"All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing"},{"id":"http://arxiv.org/abs/2507.01786v1","updated":"2025-07-02T15:12:43Z","published":"2025-07-02T15:12:43Z","title":"Probing Evaluation Awareness of Language Models","summary":"  Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.\n","authors":["Jord Nguyen","Khiem Hoang","Carlo Leonardo Attubato","Felix Hofstätter"],"pdf_url":"https://arxiv.org/pdf/2507.01786v1.pdf","comment":"Technical AI Governance Workshop, ICML (Poster)"},{"id":"http://arxiv.org/abs/2507.01785v1","updated":"2025-07-02T15:11:12Z","published":"2025-07-02T15:11:12Z","title":"MuRating: A High Quality Data Selecting Approach to Multilingual Large\n  Language Model Pretraining","summary":"  Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.\n","authors":["Zhixun Chen","Ping Guo","Wenhan Han","Yifan Zhang","Binbin Liu","Haobin Lin","Fengze Liu","Yan Zhao","Bingni Zhang","Taifeng Wang","Yin Zheng","Meng Fang"],"pdf_url":"https://arxiv.org/pdf/2507.01785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01764v1","updated":"2025-07-02T14:46:26Z","published":"2025-07-02T14:46:26Z","title":"Data interference: emojis, homoglyphs, and issues of data fidelity in\n  corpora and their results","summary":"  Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research.\n","authors":["Matteo Di Cristofaro"],"pdf_url":"https://arxiv.org/pdf/2507.01764v1.pdf","comment":"Author submitted manuscript"},{"id":"http://arxiv.org/abs/2507.01752v1","updated":"2025-07-02T14:29:30Z","published":"2025-07-02T14:29:30Z","title":"Tuning without Peeking: Provable Privacy and Generalization Bounds for\n  LLM Post-Training","summary":"  Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.\n","authors":["Ismail Labiad","Mathurin Videau","Matthieu Kowalski","Marc Schoenauer","Alessandro Leite","Julia Kempe","Olivier Teytaud"],"pdf_url":"https://arxiv.org/pdf/2507.01752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01735v1","updated":"2025-07-02T14:10:25Z","published":"2025-07-02T14:10:25Z","title":"ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and\n  Comprehension of Corner Cases in Autonomous Driving","summary":"  In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases.\n","authors":["Kai Chen","Ruiyuan Gao","Lanqing Hong","Hang Xu","Xu Jia","Holger Caesar","Dengxin Dai","Bingbing Liu","Dzmitry Tsishkou","Songcen Xu","Chunjing Xu","Qiang Xu","Huchuan Lu","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2507.01735v1.pdf","comment":"ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/"},{"id":"http://arxiv.org/abs/2507.01734v1","updated":"2025-07-02T14:07:54Z","published":"2025-07-02T14:07:54Z","title":"LLMs for Legal Subsumption in German Employment Contracts","summary":"  Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.\n","authors":["Oliver Wardas","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2507.01734v1.pdf","comment":"PrePrint - ICAIL25, Chicago"},{"id":"http://arxiv.org/abs/2410.23114v3","updated":"2025-07-02T14:02:12Z","published":"2024-10-30T15:25:06Z","title":"Unified Triplet-Level Hallucination Evaluation for Large Vision-Language\n  Models","summary":"  Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, we design a unified\nframework to measure the object and relation hallucination in LVLMs\nsimultaneously. The core idea of our framework is to evaluate hallucinations\nvia (object, relation, object) triplets extracted from LVLMs' responses, making\nit easily generalizable to different vision-language tasks. Based on our\nframework, we further introduce Tri-HE, a novel Triplet-level Hallucination\nEvaluation benchmark which can be used to study both object and relation\nhallucination at the same time. With comprehensive evaluations on Tri-HE, we\nobserve that the relation hallucination issue is even more serious than object\nhallucination among existing LVLMs, highlighting a previously neglected problem\ntowards reliable LVLMs. Moreover, based on our findings, we design a simple\ntraining-free approach that effectively mitigates hallucinations for LVLMs. Our\ndataset and code for the reproduction of our experiments are available publicly\nat https://github.com/wujunjie1998/Tri-HE.\n","authors":["Junjie Wu","Tsz Ting Chung","Kai Chen","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2410.23114v3.pdf","comment":"Accepted by TMLR 2025. Project Page:\n  https://kaichen1998.github.io/projects/tri-he/"},{"id":"http://arxiv.org/abs/2507.01715v1","updated":"2025-07-02T13:46:00Z","published":"2025-07-02T13:46:00Z","title":"Stereotype Detection as a Catalyst for Enhanced Bias Detection: A\n  Multi-Task Learning Approach","summary":"  Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems.\n","authors":["Aditya Tomar","Rudra Murthy","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2507.01715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01702v1","updated":"2025-07-02T13:32:30Z","published":"2025-07-02T13:32:30Z","title":"AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large\n  Language Models on Harmfulness","summary":"  The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.\n","authors":["Zixin Chen","Hongzhan Lin","Kaixin Li","Ziyang Luo","Zhen Ye","Guang Chen","Zhiyong Huang","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2507.01702v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2507.01679v1","updated":"2025-07-02T13:04:09Z","published":"2025-07-02T13:04:09Z","title":"Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling","summary":"  Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.\n","authors":["Zeyu Huang","Tianhao Cheng","Zihan Qiu","Zili Wang","Yinghui Xu","Edoardo M. Ponti","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2507.01679v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2506.06382v2","updated":"2025-07-02T12:24:10Z","published":"2025-06-04T23:28:39Z","title":"On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models","summary":"  We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints.\n","authors":["Michał P. Karpowicz"],"pdf_url":"https://arxiv.org/pdf/2506.06382v2.pdf","comment":"major review, transformer inference application, examples added,\n  corrections"},{"id":"http://arxiv.org/abs/2408.02544v2","updated":"2025-07-02T12:23:53Z","published":"2024-08-05T15:16:22Z","title":"Caution for the Environment: Multimodal Agents are Susceptible to\n  Environmental Distractions","summary":"  This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in a graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general scenario is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing a simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness of\nagents, our findings first indicate that these agents are prone to\nenvironmental distractions. Furthermore, we implement an adversarial\nenvironment injection and analyze the approach to improve faithfulness, calling\nfor a collective focus on this important topic.\n","authors":["Xinbei Ma","Yiting Wang","Yao Yao","Tongxin Yuan","Aston Zhang","Zhuosheng Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.02544v2.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2507.01645v1","updated":"2025-07-02T12:17:55Z","published":"2025-07-02T12:17:55Z","title":"Adapting Language Models to Indonesian Local Languages: An Empirical\n  Study of Language Transferability on Zero-Shot Settings","summary":"  In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language.\n","authors":["Rifki Afina Putri"],"pdf_url":"https://arxiv.org/pdf/2507.01645v1.pdf","comment":"AMLDS 2025"},{"id":"http://arxiv.org/abs/2507.01633v1","updated":"2025-07-02T12:05:22Z","published":"2025-07-02T12:05:22Z","title":"Confidence and Stability of Global and Pairwise Scores in NLP Evaluation","summary":"  With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.\n","authors":["Georgii Levtsov","Dmitry Ustalov"],"pdf_url":"https://arxiv.org/pdf/2507.01633v1.pdf","comment":"8 pages, accepted at ACL SRW 2025"},{"id":"http://arxiv.org/abs/2507.01627v1","updated":"2025-07-02T11:58:04Z","published":"2025-07-02T11:58:04Z","title":"Chart Question Answering from Real-World Analytical Narratives","summary":"  We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting.\n","authors":["Maeve Hutchinson","Radu Jianu","Aidan Slingsby","Jo Wood","Pranava Madhyastha"],"pdf_url":"https://arxiv.org/pdf/2507.01627v1.pdf","comment":"This paper has been accepted to the ACL Student Research Workshop\n  (SRW) 2025"},{"id":"http://arxiv.org/abs/2401.14818v6","updated":"2025-07-02T11:25:33Z","published":"2024-01-26T12:45:55Z","title":"Developing ChemDFM as a large language foundation model for chemistry","summary":"  Artificial intelligence (AI) has played an increasingly important role in\nchemical research. However, most models currently used in chemistry are\nspecialist models that require training and tuning for specific tasks. A more\ngeneric and efficient solution would be an AI model that could address many\ntasks and support free-form dialogue in the broad field of chemistry. In its\nutmost form, such a generalist AI chemist could be referred to as Chemical\nGeneral Intelligence. Large language models (LLMs) have recently logged\ntremendous success in the general domain of natural language processing,\nshowing emerging task generalization and free-form dialogue capabilities.\nHowever, domain knowledge of chemistry is largely missing when training\ngeneral-domain LLMs. The lack of such knowledge greatly hinders the performance\nof generalist LLMs in the field of chemistry. To this end, we develop ChemDFM,\na pioneering LLM for chemistry trained on 34B tokens from chemical literature\nand textbooks, and fine-tuned using 2.7M instructions. As a result, it can\nunderstand and reason with chemical knowledge in free-form dialogue.\nQuantitative evaluations show that ChemDFM significantly surpasses most\nrepresentative open-source LLMs. It outperforms GPT-4 on a great portion of\nchemical tasks, despite the substantial size difference. We have open-sourced\nthe inference codes, evaluation datasets, and model weights of ChemDFM on\nHuggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B).\n","authors":["Zihan Zhao","Da Ma","Lu Chen","Liangtai Sun","Zihao Li","Yi Xia","Bo Chen","Hongshen Xu","Zichen Zhu","Su Zhu","Shuai Fan","Guodong Shen","Kai Yu","Xin Chen"],"pdf_url":"https://arxiv.org/pdf/2401.14818v6.pdf","comment":"10 pages, 12 figures, 12 tables. Published on Cell Report Physical\n  Science, DOI: https://doi.org/10.1016/j.xcrp.2025.102523"},{"id":"http://arxiv.org/abs/2507.01599v1","updated":"2025-07-02T11:04:49Z","published":"2025-07-02T11:04:49Z","title":"Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems","summary":"  Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.\n","authors":["Zhaoyan Sun","Jiayi Wang","Xinyang Zhao","Jiachi Wang","Guoliang Li"],"pdf_url":"https://arxiv.org/pdf/2507.01599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01597v1","updated":"2025-07-02T11:02:37Z","published":"2025-07-02T11:02:37Z","title":"T3DM: Test-Time Training-Guided Distribution Shift Modelling for\n  Temporal Knowledge Graph Reasoning","summary":"  Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.\n","authors":["Yuehang Si","Zefan Zeng","Jincai Huang","Qing Cheng"],"pdf_url":"https://arxiv.org/pdf/2507.01597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01594v1","updated":"2025-07-02T11:00:33Z","published":"2025-07-02T11:00:33Z","title":"Emotionally Intelligent Task-oriented Dialogue Systems: Architecture,\n  Representation, and Optimisation","summary":"  Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.\n","authors":["Shutong Feng","Hsien-chin Lin","Nurul Lubis","Carel van Niekerk","Michael Heck","Benjamin Ruppik","Renato Vukovic","Milica Gašić"],"pdf_url":"https://arxiv.org/pdf/2507.01594v1.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.01543v1","updated":"2025-07-02T09:53:41Z","published":"2025-07-02T09:53:41Z","title":"Is External Information Useful for Stance Detection with LLMs?","summary":"  In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.\n","authors":["Quang Minh Nguyen","Taegyoon Kim"],"pdf_url":"https://arxiv.org/pdf/2507.01543v1.pdf","comment":"ACL Findings 2025"},{"id":"http://arxiv.org/abs/2507.01541v1","updated":"2025-07-02T09:51:41Z","published":"2025-07-02T09:51:41Z","title":"Efficient Out-of-Scope Detection in Dialogue Systems via\n  Uncertainty-Driven LLM Routing","summary":"  Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.\n","authors":["Álvaro Zaera","Diana Nicoleta Popa","Ivan Sekulic","Paolo Rosso"],"pdf_url":"https://arxiv.org/pdf/2507.01541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01504v1","updated":"2025-07-02T09:10:33Z","published":"2025-07-02T09:10:33Z","title":"Following the Clues: Experiments on Person Re-ID using Cross-Modal\n  Intelligence","summary":"  The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.\n","authors":["Robert Aufschläger","Youssef Shoeb","Azarm Nowzad","Michael Heigl","Fabian Bally","Martin Schramm"],"pdf_url":"https://arxiv.org/pdf/2507.01504v1.pdf","comment":"accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia"},{"id":"http://arxiv.org/abs/2507.01479v1","updated":"2025-07-02T08:43:06Z","published":"2025-07-02T08:43:06Z","title":"Evaluating the Effectiveness of Direct Preference Optimization for\n  Personalizing German Automatic Text Simplifications for Persons with\n  Intellectual Disabilities","summary":"  Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.\n","authors":["Yingqiang Gao","Kaede Johnson","David Froehlich","Luisa Carrer","Sarah Ebling"],"pdf_url":"https://arxiv.org/pdf/2507.01479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13514v2","updated":"2025-07-02T08:17:33Z","published":"2024-09-20T13:53:37Z","title":"Unifying Global and Near-Context Biasing in a Single Trie Pass","summary":"  Despite the success of end-to-end automatic speech recognition (ASR) models,\nchallenges persist in recognizing rare, out-of-vocabulary words - including\nnamed entities (NE) - and in adapting to new domains using only text data. This\nwork presents a practical approach to address these challenges through an\nunexplored combination of an NE bias list and a word-level n-gram language\nmodel (LM). This solution balances simplicity and effectiveness, improving\nentities' recognition while maintaining or even enhancing overall ASR\nperformance. We efficiently integrate this enriched biasing method into a\ntransducer-based ASR system, enabling context adaptation with almost no\ncomputational overhead. We present our results on three datasets spanning four\nlanguages and compare them to state-of-the-art biasing strategies. We\ndemonstrate that the proposed combination of keyword biasing and n-gram LM\nimproves entity recognition by up to 32% relative and reduces overall WER by up\nto a 12% relative.\n","authors":["Iuliia Thorbecke","Esaú Villatoro-Tello","Juan Zuluaga-Gomez","Shashi Kumar","Sergio Burdisso","Pradeep Rangappa","Andrés Carofilis","Srikanth Madikeri","Petr Motlicek","Karthik Pandia","Kadri Hacioğlu","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2409.13514v2.pdf","comment":"Accepted to TSD2025"},{"id":"http://arxiv.org/abs/2506.06955v3","updated":"2025-07-02T08:15:13Z","published":"2025-06-08T00:38:18Z","title":"BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for\n  Belief-Inconsistent Syllogistic Reasoning","summary":"  We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety.\n","authors":["Ha-Thanh Nguyen","Chaoran Liu","Qianying Liu","Hideyuki Tachibana","Su Myat Noe","Yusuke Miyao","Koichi Takeda","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2506.06955v3.pdf","comment":"This version includes typo corrections, added logit lens analysis for\n  open models, and an updated author list"},{"id":"http://arxiv.org/abs/2507.01449v1","updated":"2025-07-02T08:08:30Z","published":"2025-07-02T08:08:30Z","title":"LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next\n  Next Token Speculation","summary":"  Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.\n","authors":["Tianyu Liu","Qitan Lv","Hao Li","Xing Gao","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2507.01449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.22853v2","updated":"2025-07-02T07:55:09Z","published":"2025-06-28T11:28:04Z","title":"DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language\n  Models in Multi-Round, Multi-Party Dialogues","summary":"  Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.\n","authors":["Kyochul Jang","Donghyeon Lee","Kyusik Kim","Dongseok Heo","Taewhoo Lee","Woojeong Kim","Bongwon Suh"],"pdf_url":"https://arxiv.org/pdf/2506.22853v2.pdf","comment":"9 pages, ACL 2025 Vienna"},{"id":"http://arxiv.org/abs/2507.01437v1","updated":"2025-07-02T07:45:22Z","published":"2025-07-02T07:45:22Z","title":"Clinical NLP with Attention-Based Deep Learning for Multi-Disease\n  Prediction","summary":"  This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks.\n","authors":["Ting Xu","Xiaoxiao Deng","Xiandong Meng","Haifeng Yang","Yan Wu"],"pdf_url":"https://arxiv.org/pdf/2507.01437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12084v4","updated":"2025-07-02T07:38:09Z","published":"2025-02-17T17:57:50Z","title":"VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit\n  Matching Visual Cues","summary":"  Visually linking matching cues is a crucial ability in daily life, such as\nidentifying the same person in multiple photos based on their cues, even\nwithout knowing who they are. Despite the extensive knowledge that\nvision-language models (VLMs) possess, it remains largely unexplored whether\nthey are capable of performing this fundamental task. To address this, we\nintroduce \\textbf{VLM2-Bench}, a benchmark designed to assess whether VLMs can\nVisually Link Matching cues, with 9 subtasks and over 3,000 test cases.\nComprehensive evaluation across twelve VLMs, along with further analysis of\nvarious language-side and vision-side prompting methods, leads to a total of\neight key findings. We identify critical challenges in models' ability to link\nvisual cues, highlighting a significant performance gap. Based on these\ninsights, we advocate for (i) enhancing core visual capabilities to improve\nadaptability and reduce reliance on prior knowledge, (ii) establishing clearer\nprinciples for integrating language-based reasoning in vision-centric tasks to\nprevent unnecessary biases, and (iii) shifting vision-text training paradigms\ntoward fostering models' ability to independently structure and infer\nrelationships among visual cues.\n","authors":["Jianshu Zhang","Dongyu Yao","Renjie Pi","Paul Pu Liang","Yi R. Fung"],"pdf_url":"https://arxiv.org/pdf/2502.12084v4.pdf","comment":"Project Page: https://vlm2-bench.github.io/ Camera Ready version"},{"id":"http://arxiv.org/abs/2507.01431v1","updated":"2025-07-02T07:33:19Z","published":"2025-07-02T07:33:19Z","title":"Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless\n  Handwritten STEM Grading","summary":"  Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.\n","authors":["Yoonseok Yang","Minjune Kim","Marlon Rondinelli","Keren Shao"],"pdf_url":"https://arxiv.org/pdf/2507.01431v1.pdf","comment":"7 pages, 5 figues, 1 table"},{"id":"http://arxiv.org/abs/2404.16369v3","updated":"2025-07-02T07:27:32Z","published":"2024-04-25T07:15:23Z","title":"Don't Say No: Jailbreaking LLM by Suppressing Refusal","summary":"  Ensuring the safety alignment of Large Language Models (LLMs) is critical for\ngenerating responses consistent with human values. However, LLMs remain\nvulnerable to jailbreaking attacks, where carefully crafted prompts manipulate\nthem into producing toxic content. One category of such attacks reformulates\nthe task as an optimization problem, aiming to elicit affirmative responses\nfrom the LLM. However, these methods heavily rely on predefined objectionable\nbehaviors, limiting their effectiveness and adaptability to diverse harmful\nqueries. In this study, we first identify why the vanilla target loss is\nsuboptimal and then propose enhancements to the loss objective. We introduce\nDSN (Don't Say No) attack, which combines a cosine decay schedule method with\nrefusal suppression to achieve higher success rates. Extensive experiments\ndemonstrate that DSN outperforms baseline attacks and achieves state-of-the-art\nattack success rates (ASR). DSN also shows strong universality and\ntransferability to unseen datasets and black-box models.\n","authors":["Yukai Zhou","Jian Lou","Zhijie Huang","Zhan Qin","Yibei Yang","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2404.16369v3.pdf","comment":"Accepted by ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2507.00601v2","updated":"2025-07-02T06:39:55Z","published":"2025-07-01T09:34:49Z","title":"Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt\n  and Alignment-Based Approach","summary":"  This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.\n","authors":["Shuangquan Lyu","Yingnan Deng","Guiran Liu","Zhen Qi","Ruotong Wang"],"pdf_url":"https://arxiv.org/pdf/2507.00601v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03456v2","updated":"2025-07-02T06:31:16Z","published":"2025-01-07T00:56:26Z","title":"Text to Band Gap: Pre-trained Language Models as Encoders for\n  Semiconductor Band Gap Prediction","summary":"  We investigate the use of transformer-based language models, RoBERTa, T5, and\nLLaMA, for predicting the band gaps of semiconductor materials directly from\ntextual representations that encode key material features such as chemical\ncomposition, crystal system, space group, number of atoms per unit cell,\nvalence electron count, and other relevant electronic and structural\nproperties. Quantum chemistry simulations such as DFT provide accurate\npredictions but are computationally intensive, limiting their feasibility for\nlarge-scale materials screening. Shallow ML models offer faster alternatives\nbut typically require extensive data preprocessing to convert non-numerical\nmaterial features into structured numerical inputs, often at the cost of losing\ncritical descriptive information. In contrast, our approach leverages\npretrained language models to process textual data directly, eliminating the\nneed for manual feature engineering. We construct material descriptions in two\nformats: structured strings that combine key features in a consistent template,\nand natural language narratives generated using the ChatGPT API. For each\nmodel, we append a custom regression head and perform task-specific finetuning\non a curated dataset of inorganic compounds. Our results show that finetuned\nlanguage models, particularly the decoder-only LLaMA-3 architecture, can\noutperform conventional approaches in prediction accuracy and flexibility,\nachieving an MAE of 0.25 eV and R2 of 0.89, compared to the best shallow ML\nbaseline, which achieved an MAE of 0.32 eV and R2 of 0.84. Notably, LLaMA-3\nachieves competitive accuracy with minimal finetuning, suggesting its\narchitecture enables more transferable representations for scientific tasks.\nThis work demonstrates the effectiveness of finetuned language models for\nscientific property prediction and provides a scalable, language-native\nframework for materials informatics.\n","authors":["Ying-Ting Yeh","Janghoon Ock","Shagun Maheshwari","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2501.03456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19121v2","updated":"2025-07-02T06:15:45Z","published":"2025-05-25T12:25:44Z","title":"Delving into Multilingual Ethical Bias: The MSQAD with Statistical\n  Hypothesis Tests for Large Language Models","summary":"  Despite the recent strides in large language models, studies have underscored\nthe existence of social biases within these systems. In this paper, we delve\ninto the validation and comparison of the ethical biases of LLMs concerning\nglobally discussed and potentially sensitive topics, hypothesizing that these\nbiases may arise from language-specific distinctions. Introducing the\nMultilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news\narticles from Human Rights Watch covering 17 topics, and generated socially\nsensitive questions along with corresponding responses in multiple languages.\nWe scrutinized the biases of these responses across languages and topics,\nemploying two statistical hypothesis tests. The results showed that the null\nhypotheses were rejected in most cases, indicating biases arising from\ncross-language differences. It demonstrates that ethical biases in responses\nare widespread across various languages, and notably, these biases were\nprevalent even among different LLMs. By making the proposed MSQAD openly\navailable, we aim to facilitate future research endeavors focused on examining\ncross-language biases in LLMs and their variant models.\n","authors":["Seunguk Yu","Juhwan Choi","Youngbin Kim"],"pdf_url":"https://arxiv.org/pdf/2505.19121v2.pdf","comment":"ACL 2025 main conference"},{"id":"http://arxiv.org/abs/2507.00808v2","updated":"2025-07-02T05:42:47Z","published":"2025-07-01T14:40:51Z","title":"Multi-interaction TTS toward professional recording reproduction","summary":"  Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthesized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enables iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available:\nhttps://ntt-hilab-gensp.github.io/ssw13multiinteractiontts/\n","authors":["Hiroki Kanagawa","Kenichi Fujita","Aya Watanabe","Yusuke Ijima"],"pdf_url":"https://arxiv.org/pdf/2507.00808v2.pdf","comment":"7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)"},{"id":"http://arxiv.org/abs/2502.18443v3","updated":"2025-07-02T05:38:28Z","published":"2025-02-25T18:38:38Z","title":"olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language\n  Models","summary":"  PDF documents have the potential to provide trillions of novel, high-quality\ntokens for training language models. However, these documents come in a\ndiversity of types with differing formats and visual layouts that pose a\nchallenge when attempting to extract and faithfully represent the underlying\ncontent for language model use. Traditional open source tools often produce\nlower quality extractions compared to vision language models (VLMs), but\nreliance on the best VLMs can be prohibitively costly (e.g., over 6,240 USD per\nmillion PDF pages for GPT-4o) or infeasible if the PDFs cannot be sent to\nproprietary APIs. We present olmOCR, an open-source toolkit for processing PDFs\ninto clean, linearized plain text in natural reading order while preserving\nstructured content like sections, tables, lists, equations, and more. Our\ntoolkit runs a fine-tuned 7B vision language model (VLM) trained on\nolmOCR-mix-0225, a sample of 260,000 pages from over 100,000 crawled PDFs with\ndiverse properties, including graphics, handwritten text and poor quality\nscans. olmOCR is optimized for large-scale batch processing, able to scale\nflexibly to different hardware setups and can convert a million PDF pages for\nonly 176 USD. To aid comparison with existing systems, we also introduce\nolmOCR-Bench, a curated set of 1,400 PDFs capturing many content types that\nremain challenging even for the best tools and VLMs, including formulas,\ntables, tiny fonts, old scans, and more. We find olmOCR outperforms even top\nVLMs including GPT-4o, Gemini Flash 2 and Qwen-2.5-VL. We openly release all\ncomponents of olmOCR: our fine-tuned VLM model, training code and data, an\nefficient inference pipeline that supports vLLM and SGLang backends, and\nbenchmark olmOCR-Bench.\n","authors":["Jake Poznanski","Aman Rangapur","Jon Borchardt","Jason Dunkelberger","Regan Huff","Daniel Lin","Aman Rangapur","Christopher Wilhelm","Kyle Lo","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2502.18443v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04787v2","updated":"2025-07-02T05:35:17Z","published":"2024-12-06T05:41:11Z","title":"Direct Quantized Training of Language Models with Stochastic Rounding","summary":"  Although recent quantized Large Language Models (LLMs), such as BitNet, have\npaved the way for significant reduction in memory usage during deployment with\nbinary or ternary weights, training these models still demands substantial\nmemory footprints. This is partly because high-precision (i.e., unquantized)\nweights required for straight-through estimation must be maintained throughout\nthe whole training process. To address this, we explore directly updating the\nquantized low-precision weights without relying on straight-through estimation\nduring backpropagation, aiming to save memory usage during training.\nSpecifically, we employ a stochastic rounding technique to minimize the\ninformation loss caused by the use of low-bit weights throughout training.\nExperimental results on our LLaMA-structured models of various sizes indicate\nthat (1) training with only low-precision weights is feasible even when they\nare constrained to ternary values; (2) extending the bit width to 8 bits\nachieves performance on par with BitNet b1.58; (3) our models remain robust to\nprecision scaling and memory reduction, showing minimal performance degradation\nwhen moving from FP32 to lower-memory environments (BF16/FP8); and (4) our\nmodels also support inference using ternary weights, showcasing their\nflexibility in deployment.\n","authors":["Kaiyan Zhao","Tsuguchika Tabaru","Kenichi Kobayashi","Takumi Honda","Masafumi Yamazaki","Yoshimasa Tsuruoka"],"pdf_url":"https://arxiv.org/pdf/2412.04787v2.pdf","comment":"work in progress, extended experiments to 1B size models"},{"id":"http://arxiv.org/abs/2507.00487v2","updated":"2025-07-02T04:35:44Z","published":"2025-07-01T07:02:26Z","title":"MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large\n  Language Models","summary":"  Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool.\n","authors":["Jianghao Lin","Xinyuan Wang","Xinyi Dai","Menghui Zhu","Bo Chen","Ruiming Tang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.00487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15962v2","updated":"2025-07-02T04:16:51Z","published":"2025-05-21T19:26:03Z","title":"Pre-training Large Memory Language Models with Internal and External\n  Knowledge","summary":"  Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge.\n","authors":["Linxi Zhao","Sofian Zalouk","Christian K. Belardi","Justin Lovelace","Jin Peng Zhou","Kilian Q. Weinberger","Yoav Artzi","Jennifer J. Sun"],"pdf_url":"https://arxiv.org/pdf/2505.15962v2.pdf","comment":"Code, models, and data available at\n  https://github.com/kilian-group/LMLM"},{"id":"http://arxiv.org/abs/2503.00032v4","updated":"2025-07-02T03:55:41Z","published":"2025-02-25T00:59:27Z","title":"KatFishNet: Detecting LLM-Generated Korean Text through Linguistic\n  Feature Analysis","summary":"  The rapid advancement of large language models (LLMs) increases the\ndifficulty of distinguishing between human-written and LLM-generated text.\nDetecting LLM-generated text is crucial for upholding academic integrity,\npreventing plagiarism, protecting copyrights, and ensuring ethical research\npractices. Most prior studies on detecting LLM-generated text focus primarily\non English text. However, languages with distinct morphological and syntactic\ncharacteristics require specialized detection approaches. Their unique\nstructures and usage patterns can hinder the direct application of methods\nprimarily designed for English. Among such languages, we focus on Korean, which\nhas relatively flexible spacing rules, a rich morphological system, and less\nfrequent comma usage compared to English. We introduce KatFish, the first\nbenchmark dataset for detecting LLM-generated Korean text. The dataset consists\nof text written by humans and generated by four LLMs across three genres.\n  By examining spacing patterns, part-of-speech diversity, and comma usage, we\nilluminate the linguistic differences between human-written and LLM-generated\nKorean text. Building on these observations, we propose KatFishNet, a detection\nmethod specifically designed for the Korean language. KatFishNet achieves an\naverage of 19.78% higher AUROC compared to the best-performing existing\ndetection method. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.\n","authors":["Shinwoo Park","Shubin Kim","Do-Kyung Kim","Yo-Sub Han"],"pdf_url":"https://arxiv.org/pdf/2503.00032v4.pdf","comment":"Accepted to ACL 2025 main conference"}]}}