<!DOCTYPE html>
<html lang="en">

<head>
    <title>ArxivPapers</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-03T00:00:00Z">2025-07-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">76</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Requirements Elicitation Follow-Up Question Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>x<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures, accepted at the 33rd IEEE International
  Requirements Engineering 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Answer Matching Outperforms Multiple Choice for Language Model
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, Code is available at
  https://github.com/nikhilchandak/answer-matching</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MOTIF: Modular <span class="highlight-title">Think</span>ing via Reinforcement Fine-tuning in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>P<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>U<span class="highlight-author"></span>l<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> <span class="highlight-title">LLM</span> Hypnosis: Exploiting User Feedback for Unauthorized Knowledge
  Injection to All Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Legal Requirements Translation from Law 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>x<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, Accepted at the 33rd IEEE International
  Requirements Engineering 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Visual Contextual Attack: Jailbreaking M<span class="highlight-title">LLM</span>s with Image-Driven Context
  Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the visual modality primarily serves as a trigger
for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding
in realistic scenarios. In this work, we define a novel setting: visual-centric
jailbreak, where visual information serves as a necessary component in
constructing a complete and realistic jailbreak context. Building on this
setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates
contextual dialogue using four distinct visual-focused strategies, dynamically
generating auxiliary images when necessary to construct a visual-centric
jailbreak scenario. To maximize attack effectiveness, it incorporates automatic
toxicity obfuscation and semantic refinement to produce a final attack prompt
that reliably triggers harmful responses from the target black-box MLLMs.
Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success
Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming
the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The
code is available at https://github.com/Dtc7w3PQ/Visco-Attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to
  Reason 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ExPO: Unlocking Hard <span class="highlight-title">Reasoning</span> with Self-Explanation-Guided
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Generalizing Verifiable Instruction Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>p<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>z<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> SynapseRoute: An Auto-Route Switching Framework on Dual-State Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Multimodal Mathematical <span class="highlight-title">Reasoning</span> with Diverse Solving Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Is <span class="highlight-title">Reasoning</span> All You Need? Probing Bias in the Age of <span class="highlight-title">Reasoning</span> Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> From Long Videos to Engaging Clips: A Human-Inspired Video Editing
  Framework with Multimodal Narrative Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) that leverages
multimodal narrative understanding to address these limitations. Our approach
incorporates character extraction, dialogue analysis, and narrative
summarization through multimodal large language models, enabling a holistic
understanding of the video content. To further enhance coherence, we apply
scene-level segmentation and decompose the editing process into three subtasks:
highlight detection, opening/ending selection, and pruning of irrelevant
content. To facilitate research in this area, we introduce DramaAD, a novel
benchmark dataset comprising over 800 short drama episodes and 500
professionally edited advertisement clips. Experimental results demonstrate
that our framework consistently outperforms existing baselines across both
general and advertisement-oriented editing tasks, significantly narrowing the
quality gap between automatic and human-edited videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Self-Correction Bench: Revealing and Addressing the Self-Correction
  Blind Spot in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with
  Self-Generated Cross-Modal Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>z<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>-<span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>-<span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>E<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>z<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>Q<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>b<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model
(LALM) designed for robust auditory perception and instruction-following,
without requiring task-specific audio instruction-tuning. Recent LALMs
typically augment Large Language Models (LLMs) with auditory capabilities by
training on large-scale, manually curated or LLM-synthesized audio-instruction
datasets. However, these approaches have often suffered from the catastrophic
forgetting of the LLM's original language abilities. To address this, we
revisit the data construction pipeline and propose DeSTA, a self-generated
cross-modal alignment strategy in which the backbone LLM generates its own
training targets. This approach preserves the LLM's native language proficiency
while establishing effective audio-text alignment, thereby enabling zero-shot
generalization without task-specific tuning. Using DeSTA, we construct
DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training
samples derived from 7,000 hours of audio spanning 50 diverse datasets,
including speech, environmental sounds, and music. DeSTA2.5-Audio achieves
state-of-the-art or competitive performance across a wide range of
audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,
Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate
that our self-generated strategy outperforms widely adopted data construction
and training strategies in both auditory perception and instruction-following
capabilities. Our findings underscore the importance of carefully designed data
construction in LALM development and offer practical insights for building
robust, general-purpose LALMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Model and code available at:
  https://github.com/kehanlu/DeSTA2.5-Audio</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Measurement of the Granularity of Vowel Production Space By Just
  Producible Different (JPD) Limens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Early Signs of Steganographic Capabilities in Frontier <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>s<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>-<span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>c<span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Can <span class="highlight-title">LLM</span>s Identify Critical Limitations within Scientific Research? A
  Systematic Evaluation on AI Research Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Exploring Gender Bias Beyond Occupational Titles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ASDA: Audio Spectrogram Differential Attention Mechanism for
  Self-Supervised Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>w<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent advancements in audio self-supervised representation learning, the
standard Transformer architecture has emerged as the predominant approach, yet
its attention mechanism often allocates a portion of attention weights to
irrelevant information, potentially impairing the model's discriminative
ability. To address this, we introduce a differential attention mechanism,
which effectively mitigates ineffective attention allocation through the
integration of dual-softmax operations and appropriately tuned differential
coefficients. Experimental results demonstrate that our ASDA model achieves
state-of-the-art (SOTA) performance across multiple benchmarks, including audio
classification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting
(98.3% accuracy on SPC-2), and environmental sound classification (96.1%
accuracy on ESC-50). These results highlight ASDA's effectiveness in audio
tasks, paving the way for broader applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device
  Speculative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Decoupled Planning and Execution: A Hierarchical <span class="highlight-title">Reasoning</span> Framework for
  Deep Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Strategic Intelligence in <span class="highlight-title">Large Language Model</span>s: Evidence from
  evolutionary Game Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 27 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MPF: Aligning and Debiasing Language Models post Deployment via Multi
  Perspective Fusion <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>H<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>m<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025 AIW Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Revisiting Active Learning under (Human) Label Variation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>ö<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>ß<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> WebSailor: Navigating Super-human <span class="highlight-title">Reasoning</span> for Web Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on
  Indian Bail Orders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>b<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and
  GitHub. Submitted to arXiv for open access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A Cookbook for Community-driven Data Collection of Impaired Speech in
  LowResource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>-<span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>b<span class="highlight-author"></span>d<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>p<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>t<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>b<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>k<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>i<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>f<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been reviewed and accepted for presentation at the
  InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5
  pages and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Benchmarking Akan ASR Models Across Domain-Specific Datasets: A
  Comparative Evaluation of Performance, Scalability, and Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>k<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>p<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>-<span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>b<span class="highlight-author"></span>d<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>A<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>t<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been reviewed and accepted for presentation at the
  Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025
  in Munich, Germany. 17 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> JoyTTS: <span class="highlight-title">LLM</span>-based Spoken Chatbot With Voice Cloning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  JoyTTS is an end-to-end spoken chatbot that combines large language models
(LLM) with text-to-speech (TTS) technology, featuring voice cloning
capabilities. This project is built upon the open-source MiniCPM-o and
CosyVoice2 models and trained on 2000 hours of conversational data. We have
also provided the complete training code to facilitate further development and
optimization by the community. On the testing machine seed-tts-zh, it achieves
a SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.
The code and models, along with training and inference scripts, are available
at https://github.com/jdh-algo/JoyTTS.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Efficient Code <span class="highlight-title">LLM</span> Training via Distribution-Consistent and
  Diversity-Aware Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency
  in Hybrid Quantum-Classical Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and
  Confidence-Informed Ensembling for Multimodal <span class="highlight-title">Large Language Model</span>s <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DoMIX: An Efficient Framework for Exploiting Domain Knowledge in
  Fine-Tuning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>p<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures, ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Seeing Through Green: Text-Based Classification and the Firm's Returns
  from Green Patents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Natural Language Processing for identifying ``true''
green patents from official supporting documents. We start our training on
about 12.4 million patents that had been classified as green from previous
literature. Thus, we train a simple neural network to enlarge a baseline
dictionary through vector representations of expressions related to
environmental technologies. After testing, we find that ``true'' green patents
represent about 20\% of the total of patents classified as green from previous
literature. We show heterogeneity by technological classes, and then check that
`true' green patents are about 1\% less cited by following inventions. In the
second part of the paper, we test the relationship between patenting and a
dashboard of firm-level financial accounts in the European Union. After
controlling for reverse causality, we show that holding at least one ``true''
green patent raises sales, market shares, and productivity. If we restrict the
analysis to high-novelty ``true'' green patents, we find that they also yield
higher profits. Our findings underscore the importance of using text analyses
to gauge finer-grained patent classifications that are useful for policymaking
in different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MemAgent: Reshaping Long-Context <span class="highlight-title">LLM</span> with Multi-Conv RL-based Memory
  Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>-<span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://memagent-sialab.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic
  Data Commons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in
  Academic Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures, 4 tables. Project Page:
  https://iyatomilab.github.io/SciGA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Improved Unbiased Watermark for <span class="highlight-title">Large Language Model</span>s <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As artificial intelligence surpasses human capabilities in text generation,
the necessity to authenticate the origins of AI-generated content has become
paramount. Unbiased watermarks offer a powerful solution by embedding
statistical signals into language model-generated text without distorting the
quality. In this paper, we introduce MCmark, a family of unbiased,
Multi-Channel-based watermarks. MCmark works by partitioning the model's
vocabulary into segments and promoting token probabilities within a selected
segment based on a watermark key. We demonstrate that MCmark not only preserves
the original distribution of the language model but also offers significant
improvements in detectability and robustness over existing unbiased watermarks.
Our experiments with widely-used language models demonstrate an improvement in
detectability of over 10% using MCmark, compared to existing state-of-the-art
unbiased watermarks. This advancement underscores MCmark's potential in
enhancing the practical application of watermarking in AI-generated texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> From Web Search towards Agentic Deep Research: Incentivizing Search with
  <span class="highlight-title">Reasoning</span> Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18959v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18959v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>r<span class="highlight-author"></span>w<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>p<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval is a cornerstone of modern knowledge acquisition,
enabling billions of queries each day across diverse domains. However,
traditional keyword-based search engines are increasingly inadequate for
handling complex, multi-step information needs. Our position is that Large
Language Models (LLMs), endowed with reasoning and agentic capabilities, are
ushering in a new paradigm termed Agentic Deep Research. These systems
transcend conventional information search techniques by tightly integrating
autonomous reasoning, iterative retrieval, and information synthesis into a
dynamic feedback loop. We trace the evolution from static web search to
interactive, agent-based systems that plan, explore, and learn. We also
introduce a test-time scaling law to formalize the impact of computational
depth on reasoning and search. Supported by benchmark results and the rise of
open-source implementations, we demonstrate that Agentic Deep Research not only
significantly outperforms existing approaches, but is also poised to become the
dominant paradigm for future information seeking. All the related resources,
including industry products, research papers, benchmark datasets, and
open-source implementations, are collected for the community in
https://github.com/DavidZWZ/Awesome-Deep-Research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> GPAS: Accelerating Convergence of <span class="highlight-title">LLM</span> Pretraining via
  Gradient-Preserving Activation Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the shortcut to dominate over sub-layer outputs in the residual
connection and limiting the learning capacity of deeper layers. To mitigate
this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple
technique that can be used in combination with existing approaches. GPAS works
by scaling down the intermediate activations while keeping their gradients
unchanged. This leaves information in the activations intact, and avoids the
gradient vanishing problem associated with gradient downscaling. Extensive
experiments across various model sizes from 71M to 1B show that GPAS achieves
consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also
shows promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings. Our code is available at
https://github.com/dandingsky/GPAS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge
  Graph Guided Distractor Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00612v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00612v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical tasks such as diagnosis and treatment require strong decision-making
abilities, highlighting the importance of rigorous evaluation benchmarks to
assess the reliability of large language models (LLMs). In this work, we
introduce a knowledge-guided data augmentation framework that enhances the
difficulty of clinical multiple-choice question (MCQ) datasets by generating
distractors (i.e., incorrect choices that are similar to the correct one and
may confuse existing LLMs). Using our KG-based pipeline, the generated choices
are both clinically plausible and deliberately misleading. Our approach
involves multi-step, semantically informed walks on a medical knowledge graph
to identify distractor paths-associations that are medically relevant but
factually incorrect-which then guide the LLM in crafting more deceptive
distractors. We apply the designed knowledge graph guided distractor generation
(KGGDG) pipline, to six widely used medical QA benchmarks and show that it
consistently reduces the accuracy of state-of-the-art LLMs. These findings
establish KGGDG as a powerful tool for enabling more robust and diagnostic
evaluations of medical LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Batch-Max: Higher <span class="highlight-title">LLM</span> Throughput using Larger Batch Sizes and KV Cache
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05693v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05693v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several works have developed eviction policies to remove key-value (KV) pairs
from the KV cache for more efficient inference. The focus has been on
compressing the KV cache after the input prompt has been processed for faster
token generation. In settings with limited GPU memory, and when the input
context is longer than the generation length, we show that by also compressing
the KV cache during the input processing phase, larger batch sizes can be used
resulting in significantly higher throughput while still maintaining the
original model's accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>é<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>é<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>z<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agentic search such as Deep Research systems-where agents autonomously browse
the web, synthesize information, and return comprehensive citation-backed
answers-represents a major shift in how users interact with web-scale
information. While promising greater efficiency and cognitive offloading, the
growing complexity and open-endedness of agentic search have outpaced existing
evaluation benchmarks and methodologies, which largely assume short search
horizons and static answers. In this paper, we introduce Mind2Web 2, a
benchmark of 130 realistic, high-quality, and long-horizon tasks that require
real-time web browsing and extensive information synthesis, constructed with
over 1000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of ten frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, highlighting its great potential. Altogether, Mind2Web
2 provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Homepage: https://osu-nlp-group.github.io/Mind2Web-2/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> On Characterizations for Language Generation: Interplay of
  Hallucinations, Breadth, and Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study language generation in the limit - introduced by Kleinberg and
Mullainathan [KM24] - building on classical works of Gold [Gol67] and Angluin
[Ang79]. [KM24]'s main result is an algorithm for generating from any countable
language collection in the limit. While their algorithm eventually generates
unseen strings from the target language $K$, it sacrifices coverage or breadth,
i.e., its ability to generate a rich set of strings. Recent work introduces
different notions of breadth and explores when generation with breadth is
possible, leaving a full characterization of these notions open. Our first set
of results settles this by characterizing generation for existing notions of
breadth and their natural extensions. Interestingly, our lower bounds are very
flexible and hold for many performance metrics beyond breadth - for instance,
showing that, in general, it is impossible to train generators which achieve a
higher perplexity or lower hallucination rate for $K$ compared to other
languages. Next, we study language generation with breadth and stable
generators - algorithms that eventually stop changing after seeing an arbitrary
but finite number of strings - and prove unconditional lower bounds for such
generators, strengthening the results of [KMV25] and demonstrating that
generation with many existing notions of breadth becomes equally hard, when
stability is required. This gives a separation for generation with approximate
breadth, between stable and unstable generators, highlighting the rich
interplay between breadth, stability, and consistency in language generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2 improves exposition and simplifies proofs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Next-Token Prediction Task Assumes Optimal Data Ordering for <span class="highlight-title">LLM</span>
  Training in Proof Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>b<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of large language model (LLM)-based proof generation, despite
extensive training on large datasets such as ArXiv, LLMs still exhibit only
modest performance on proving tasks of moderate difficulty. We believe that
this is partly due to the widespread presence of suboptimal ordering within the
data for each proof used in training. For example, published proofs often
follow a purely logical order, where each step logically proceeds from the
previous steps based on the deductive rules. This order is designed to
facilitate the verification of the proof's soundness, rather than to help
people and models learn the discovery process of the proof. In proof
generation, we argue that the optimal order for one training data sample occurs
when the relevant intermediate supervision for a particular proof step in the
proof is always positioned to the left of that proof step. We call such order
the intuitively sequential order. We validate our claims using two tasks:
intuitionistic propositional logic theorem-proving and digit multiplication.
Our experiments verify the order effect and provide support for our
explanations. We demonstrate that training is most effective when the proof is
in the intuitively sequential order. Moreover, the order effect and the
performance gap between models trained on different data orders can be
substantial -- with an 11 percent improvement in proof success rate observed in
the propositional logic theorem-proving task, between models trained on the
optimal order compared to the worst order. Lastly, we define a common type of
order issue in advanced math proofs and find that 17.3 percent of theorems with
nontrivial proofs in the first two chapters of a widely used graduate-level
mathematics textbook suffer from this issue. A detailed list of those proofs is
provided in the appendix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General
  <span class="highlight-title">Reasoning</span> <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-language Chain-of-Thought (CoT) data resources are relatively scarce
compared to text-only counterparts, limiting the improvement of reasoning
capabilities in Vision Language Models (VLMs). However, high-quality
vision-language reasoning data is expensive and labor-intensive to annotate. To
address this issue, we leverage a promising resource: game code, which
naturally contains logical structures and state transition processes.
Therefore, we propose Code2Logic, a novel game-code-driven approach for
multimodal reasoning data synthesis. Our approach leverages Large Language
Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning
processes and results through code execution. Using the Code2Logic approach, we
developed the GameQA dataset to train and evaluate VLMs. GameQA is
cost-effective and scalable, offers controllable difficulty gradation and is
diverse with 30 games and 158 tasks. Surprisingly, despite training solely on
game data, VLMs demonstrated out of domain generalization, specifically
Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language
benchmarks. Our code, dataset and models are available at
https://github.com/tongjingqi/Code2Logic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63 pages, 23 figures, submitted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Explainable Compliance Detection with Multi-Hop Natural Language
  Inference on Assurance Case Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Direct Preference Optimization Using Sparse Feature-Level Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The alignment of large language models (LLMs) with human preferences remains
a key challenge. While post-training techniques like Reinforcement Learning
from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have
achieved notable success, they often introduce computational inefficiencies and
training instability. In this paper, we propose Feature-level constrained
Preference Optimization (FPO), a novel method designed to simplify the
alignment process while ensuring stability. FPO leverages pre-trained Sparse
Autoencoders (SAEs) and introduces feature-level constraints, allowing for
efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using
sparse features activated in a well-trained sparse autoencoder and the quality
of sequential KL divergence by using the feature-level offline reference.
Experimental results on benchmark datasets demonstrate that FPO achieves a
5.08% absolute improvement in win rate with much lower computational cost
compared to state-of-the-art baselines, making it a promising solution for
efficient and controllable LLM alignments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Symbolic or Numerical? Understanding Physics Problem Solving in
  <span class="highlight-title">Reasoning</span> <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> MedAide: Information Fusion and Anatomy of Medical Intents via <span class="highlight-title">LLM</span>-based
  Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12532v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12532v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In healthcare intelligence, the ability to fuse heterogeneous, multi-intent
information from diverse clinical sources is fundamental to building reliable
decision-making systems. Large Language Model (LLM)-driven information
interaction systems currently showing potential promise in the healthcare
domain. Nevertheless, they often suffer from information redundancy and
coupling when dealing with complex medical intents, leading to severe
hallucinations and performance bottlenecks. To this end, we propose MedAide, an
LLM-based medical multi-agent collaboration framework designed to enable
intent-aware information fusion and coordinated reasoning across specialized
healthcare domains. Specifically, we introduce a regularization-guided module
that combines syntactic constraints with retrieval augmented generation to
decompose complex queries into structured representations, facilitating
fine-grained clinical information fusion and intent resolution. Additionally, a
dynamic intent prototype matching module is proposed to utilize dynamic
prototype representation with a semantic similarity matching mechanism to
achieve adaptive recognition and updating of the agent's intent in multi-round
healthcare dialogues. Ultimately, we design a rotation agent collaboration
mechanism that introduces dynamic role rotation and decision-level information
fusion across specialized medical agents. Extensive experiments are conducted
on four medical benchmarks with composite intents. Experimental results from
automated metrics and expert doctor evaluations show that MedAide outperforms
current LLMs and improves their medical proficiency and strategic reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Multi-Agent Collaboration for Medical Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> AI Flow: Perspectives, Scenarios, and Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pioneered by the foundational information theory by Claude Shannon and the
visionary framework of machine intelligence by Alan Turing, the convergent
evolution of information and communication technologies (IT/CT) has created an
unbroken wave of connectivity and computation. This synergy has sparked a
technological revolution, now reaching its peak with large artificial
intelligence (AI) models that are reshaping industries and redefining
human-machine collaboration. However, the realization of ubiquitous
intelligence faces considerable challenges due to substantial resource
consumption in large models and high communication bandwidth demands. To
address these challenges, AI Flow has been introduced as a multidisciplinary
framework that integrates cutting-edge IT and CT advancements, with a
particular emphasis on the following three key points. First, device-edge-cloud
framework serves as the foundation, which integrates end devices, edge servers,
and cloud clusters to optimize scalability and efficiency for low-latency model
inference. Second, we introduce the concept of familial models, which refers to
a series of different-sized models with aligned hidden features, enabling
effective collaboration and the flexibility to adapt to varying resource
constraints and dynamic scenarios. Third, connectivity- and interaction-based
intelligence emergence is a novel paradigm of AI Flow. By leveraging
communication networks to enhance connectivity, the collaboration among AI
models across heterogeneous nodes achieves emergent intelligence that surpasses
the capability of any single model. The innovations of AI Flow provide enhanced
intelligence, timely responsiveness, and ubiquitous accessibility to AI
services, paving the way for the tighter fusion of AI techniques and
communication systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authors are with Institute of Artificial Intelligence (TeleAI), China
  Telecom, China. Author names are listed alphabetically by surname. This work
  was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:
  shaojw2@chinatelecom.cn) under the leadership of Prof. Xuelong Li. The
  corresponding author is Prof. Xuelong Li (e-mail: xuelong li@ieee.org), the
  CTO and Chief Scientist of China Telecom</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Traveling Across Languages: Benchmarking Cross-Lingual Consistency in
  Multimodal <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of multimodal large language models (MLLMs) has
significantly enhanced their real-world applications. However, achieving
consistent performance across languages, especially when integrating cultural
knowledge, remains a significant challenge. To better assess this issue, we
introduce two new benchmarks: KnowRecall and VisRecall, which evaluate
cross-lingual consistency in MLLMs. KnowRecall is a visual question answering
benchmark designed to measure factual knowledge consistency in 15 languages,
focusing on cultural and historical questions about global landmarks. VisRecall
assesses visual memory consistency by asking models to describe landmark
appearances in 9 languages without access to images. Experimental results
reveal that state-of-the-art MLLMs, including proprietary ones, still struggle
to achieve cross-lingual consistency. This underscores the need for more robust
approaches that produce truly multilingual and culturally aware models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/nlp-waseda/traveling-across-languages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Self-Guided Process Reward Optimization with Redefined Step-wise
  Advantage for Process Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Robustness of Misinformation Classification Systems to Adversarial
  Examples Through BeamAttack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.23661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.23661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>z<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We extend BeamAttack, an adversarial attack algorithm designed to evaluate
the robustness of text classification systems through word-level modifications
guided by beam search. Our extensions include support for word deletions and
the option to skip substitutions, enabling the discovery of minimal
modifications that alter model predictions. We also integrate LIME to better
prioritize word replacements. Evaluated across multiple datasets and victim
models (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA
framework, our approach achieves over a 99\% attack success rate while
preserving the semantic and lexical similarity of the original texts. Through
both quantitative and qualitative analysis, we highlight BeamAttack's
effectiveness and its limitations. Our implementation is available at
https://github.com/LucK1Y/BeamAttack
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages main text, 27 pages total including references and
  appendices. 13 figures, 10 tables. Accepted for publication in the LNCS
  proceedings of CLEF 2025 (Best-of-Labs track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Task Prompt Vectors: Effective Initialization through Multi-Task
  Soft-Prompt Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01119v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01119v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>r<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning is an efficient solution for training large language models
(LLMs). However, current soft-prompt-based methods often sacrifice multi-task
modularity, requiring the training process to be fully or partially repeated
for each newly added task. While recent work on task vectors applied arithmetic
operations on full model weights to achieve the desired multi-task performance,
a similar approach for soft-prompts is still missing. To this end, we introduce
Task Prompt Vectors, created by element-wise difference between weights of
tuned soft-prompts and their random initialization. Experimental results on 12
NLU datasets show that task prompt vectors can be used in low-resource settings
to effectively initialize prompt tuning on similar tasks. In addition, we show
that task prompt vectors are independent of the random initialization of prompt
tuning on 2 different language model architectures. This allows prompt
arithmetics with the pre-trained vectors from different tasks. In this way, we
provide a competitive alternative to state-of-the-art baselines by arithmetic
addition of task prompt vectors from multiple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for
  Elderly Migrants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A version of this manuscript has been submitted to the [IASDR 2025
  Conference](https://iasdr2025.org/) and is currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Delving into <span class="highlight-title">LLM</span>-assisted writing in biomedical publications through
  excess vocabulary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07016v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07016v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>á<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>-<span class="highlight-author"></span>M<span class="highlight-author"></span>á<span class="highlight-author"></span>r<span class="highlight-author"></span>q<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>m<span class="highlight-author"></span>ő<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>Á<span class="highlight-author"></span>g<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>v<span class="highlight-author"></span>á<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) like ChatGPT can generate and revise text with
human-level performance. These models come with clear limitations: they can
produce inaccurate information, reinforce existing biases, and be easily
misused. Yet, many scientists use them for their scholarly writing. But how
wide-spread is such LLM usage in the academic literature? To answer this
question for the field of biomedical research, we present an unbiased,
large-scale approach: we study vocabulary changes in over 15 million biomedical
abstracts from 2010--2024 indexed by PubMed, and show how the appearance of
LLMs led to an abrupt increase in the frequency of certain style words. This
excess word analysis suggests that at least 13.5% of 2024 abstracts were
processed with LLMs. This lower bound differed across disciplines, countries,
and journals, reaching 40% for some subcorpora. We show that LLMs have had an
unprecedented impact on scientific writing in biomedical research, surpassing
the effect of major world events such as the Covid pandemic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v5: Reverting to v3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> AIn't Nothing But a Survey? Using <span class="highlight-title">Large Language Model</span>s for Coding
  German Open-Ended Survey Responses on Survey Motivation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14634v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14634v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>v<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>ß<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development and wider accessibility of LLMs have spurred
discussions about how they can be used in survey research, including
classifying open-ended survey responses. Due to their linguistic capacities, it
is possible that LLMs are an efficient alternative to time-consuming manual
coding and the pre-training of supervised machine learning models. As most
existing research on this topic has focused on English-language responses
relating to non-complex topics or on single LLMs, it is unclear whether its
findings generalize and how the quality of these classifications compares to
established methods. In this study, we investigate to what extent different
LLMs can be used to code open-ended survey responses in other contexts, using
German data on reasons for survey participation as an example. We compare
several state-of-the-art LLMs and several prompting approaches, and evaluate
the LLMs' performance by using human expert codings. Overall performance
differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory
levels of predictive performance. Performance differences between prompting
approaches are conditional on the LLM used. Finally, LLMs' unequal
classification performance across different categories of reasons for survey
participation results in different categorical distributions when not using
fine-tuning. We discuss the implications of these findings, both for
methodological research on coding open-ended responses and for their
substantive analysis, and for practitioners processing or substantively
analyzing such data. Finally, we highlight the many trade-offs researchers need
to consider when choosing automated methods for open-ended response
classification in the age of LLMs. In doing so, our study contributes to the
growing body of research about the conditions under which LLMs can be
efficiently, accurately, and reliably leveraged in survey research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in Survey Research Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Improving the Robustness of Distantly-Supervised Named Entity
  Recognition via Uncertainty-Aware Teacher Learning and Student-Student
  Collaborative Learning <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08010v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08010v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distantly-Supervised Named Entity Recognition (DS-NER) is widely used in
real-world scenarios. It can effectively alleviate the burden of annotation by
matching entities in existing knowledge bases with snippets in the text but
suffer from the label noise. Recent works attempt to adopt the teacher-student
framework to gradually refine the training labels and improve the overall
robustness. However, these teacher-student methods achieve limited performance
because the poor calibration of the teacher network produces incorrectly
pseudo-labeled samples, leading to error propagation. Therefore, we propose:
(1) Uncertainty-Aware Teacher Learning that leverages the prediction
uncertainty to reduce the number of incorrect pseudo labels in the
self-training stage; (2) Student-Student Collaborative Learning that allows the
transfer of reliable labels between two student networks instead of
indiscriminately relying on all pseudo labels from its teacher, and further
enables a full exploration of mislabeled samples rather than simply filtering
unreliable pseudo-labeled samples. We evaluate our proposed method on five
DS-NER datasets, demonstrating that our method is superior to the
state-of-the-art DS-NER methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Incorporating <span class="highlight-title">LLM</span>s for Large-Scale Urban Complex Mobility Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>E<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>-<span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents an innovative approach to urban mobility simulation by
integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).
Unlike traditional rule-based ABM, the proposed framework leverages LLM to
enhance agent diversity and realism by generating synthetic population
profiles, allocating routine and occasional locations, and simulating
personalized routes. Using real-world data, the simulation models individual
behaviors and large-scale mobility patterns in Taipei City. Key insights, such
as route heat maps and mode-specific indicators, provide urban planners with
actionable information for policy-making. Future work focuses on establishing
robust validation frameworks to ensure accuracy and reliability in urban
planning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM
  (Computational Urban Planning and Urban Management) Conference held by
  University College London (UCL) in 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Decision-Oriented Text Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Token Prepending: A Training-Free Approach for Eliciting Better Sentence
  Embeddings from <span class="highlight-title">LLM</span>s <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting sentence embeddings from large language models (LLMs) is a
promising direction, as LLMs have demonstrated stronger semantic understanding
capabilities. Previous studies typically focus on prompt engineering to elicit
sentence embeddings from LLMs by prompting the model to encode sentence
information into the embedding of the last token. However, LLMs are mostly
decoder-only models with causal attention and the earlier tokens in the
sentence cannot attend to the latter tokens, resulting in biased encoding of
sentence information and cascading effects on the final decoded token. To this
end, we propose a novel Token Prepending (TP) technique that prepends each
layer's decoded sentence embedding to the beginning of the sentence in the next
layer's input, allowing earlier tokens to attend to the complete sentence
information under the causal attention mechanism. The proposed TP technique is
a plug-and-play and training-free technique, which means it can be seamlessly
integrated with various prompt-based sentence embedding methods and
autoregressive LLMs. Extensive experiments on various Semantic Textual
Similarity (STS) tasks and downstream classification tasks demonstrate that our
proposed TP technique can significantly improve the performance of existing
prompt-based sentence embedding methods across different LLMs, while incurring
negligible additional inference cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2025 (Oral). Code are available on
  https://github.com/fuyuchenIfyw/token_prepending.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Layered Insights: Generalizable Analysis of Authorial Style by
  Leveraging All Transformer Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>d<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>c<span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for the authorship attribution task that leverages
the various linguistic representations learned at different layers of
pre-trained transformer-based models. We evaluate our approach on three
datasets, comparing it to a state-of-the-art baseline in in-domain and
out-of-domain scenarios. We found that utilizing various transformer layers
improves the robustness of authorship attribution models when tested on
out-of-domain data, resulting in new state-of-the-art results. Our analysis
gives further insights into how our model's different layers get specialized in
representing certain stylistic features that benefit the model when tested out
of the domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Aligning Frozen <span class="highlight-title">LLM</span>s by Reinforcement Learning: An Iterative
  Reweight-then-Optimize Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>f<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Fast-d<span class="highlight-title">LLM</span>: Training-free Acceleration of Diffusion <span class="highlight-title">LLM</span> by Enabling KV
  Cache and Parallel Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22618v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22618v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based large language models (Diffusion LLMs) have shown promise for
non-autoregressive text generation with parallel decoding capabilities.
However, the practical inference speed of open-sourced Diffusion LLMs often
lags behind autoregressive models due to the lack of Key-Value (KV) Cache and
quality degradation when decoding multiple tokens simultaneously. To bridge
this gap, we introduce a novel block-wise approximate KV Cache mechanism
tailored for bidirectional diffusion models, enabling cache reuse with
negligible performance drop. Additionally, we identify the root cause of
generation quality degradation in parallel decoding as the disruption of token
dependencies under the conditional independence assumption. To address this, we
propose a confidence-aware parallel decoding strategy that selectively decodes
tokens exceeding a confidence threshold, mitigating dependency violations and
maintaining generation quality. Experimental results on LLaDA and Dream models
across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$
throughput} improvement with minimal accuracy loss, closing the performance gap
with autoregressive models and paving the way for practical deployment of
Diffusion LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Bypass Back-propagation: Optimization-based Structural Pruning for Large
  Language Models via Policy Gradient <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10576v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10576v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Large-Language Models (LLMs) pruning methods typically operate at the
post-training phase without the expensive weight finetuning, however, their
pruning criteria often rely on heuristically hand-crafted metrics, potentially
leading to suboptimal performance. We instead propose a novel
optimization-based structural pruning that learns the pruning masks in a
probabilistic space directly by optimizing the loss of the pruned model. To
preserve efficiency, our method eliminates the back-propagation through the LLM
per se during optimization, requiring only the forward pass of the LLM. We
achieve this by learning an underlying Bernoulli distribution to sample binary
pruning masks, where we decouple the Bernoulli parameters from LLM loss,
facilitating efficient optimization via policy gradient estimator without
back-propagation. Thus, our method can 1) support global and heterogeneous
pruning (i.e., automatically determine different redundancy for different
layers), and 2) optionally initialize with a metric-based method (for our
Bernoulli distributions). Extensive experiments conducted on LLaMA, LLaMA-2,
LLaMA-3, Vicuna, and Mistral models using the C4 and WikiText2 datasets
demonstrate the promising performance of our method in efficiency and
effectiveness. Code is available at
https://github.com/ethanygao/backprop-free_LLM_pruning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL2025 Main Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt
  and Reward Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03262v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03262v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) fine-tuned via Reinforcement Learning from Human
Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR)
significantly improve the alignment of human-AI values and further raise the
upper bound of AI capabilities, particularly in reasoning-intensive,
long-context Chain-of-Thought (long-CoT) tasks. However, existing RLHF (or
RLVR) frameworks commonly face challenges such as inference bottlenecks and
complexity barriers, restricting their accessibility for newcomers. To bridge
this gap, we introduce \textbf{OpenRLHF}, a user-friendly, scalable, and
easy-to-learn open-source RLHF framework built upon Ray, vLLM, DeepSpeed, and
HuggingFace Transformers, featuring a simplified design, clear code structure,
and comprehensive documentation to facilitate entry for researchers and
practitioners. Experimental results show that OpenRLHF achieves superior
training efficiency with speedups ranging from 1.22x to 1.68x across different
model sizes compared to state-of-the-art frameworks, while requiring
significantly fewer lines of code for implementation. OpenRLHF is publicly
available at https://github.com/OpenRLHF/OpenRLHF, and has already been adopted
by leading institutions to accelerate RLHF research and learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fix typo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of
  Multi-Modal <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18681v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18681v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sarcasm detection, as a crucial research direction in the field of Natural
Language Processing (NLP), has attracted widespread attention. Traditional
sarcasm detection tasks have typically focused on single-modal approaches
(e.g., text), but due to the implicit and subtle nature of sarcasm, such
methods often fail to yield satisfactory results. In recent years, researchers
have shifted the focus of sarcasm detection to multi-modal approaches. However,
effectively leveraging multi-modal information to accurately identify sarcastic
content remains a challenge that warrants further exploration. Leveraging the
powerful integrated processing capabilities of Multi-Modal Large Language
Models (MLLMs) for various information sources, we propose an innovative
multi-modal Commander-GPT framework. Inspired by military strategy, we first
decompose the sarcasm detection task into six distinct sub-tasks. A central
commander (decision-maker) then assigns the best-suited large language model to
address each specific sub-task. Ultimately, the detection results from each
model are aggregated to identify sarcasm. We conducted extensive experiments on
MMSD and MMSD 2.0, utilizing four multi-modal large language models and six
prompting strategies. Our experiments demonstrate that our approach achieves
state-of-the-art performance, with a 19.3% improvement in F1 score, without
necessitating fine-tuning or ground-truth rationales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our original goal was to use Commander-GPT: Dividing and Routing for
  Multimodal Sarcasm Detection (arXiv:2506.19420) to replace Commander-GPT:
  Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large
  Language Models (arXiv:2503.18681). Due to various reasons, both versions
  were released, so we would like to withdraw the latter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Prompt-Guided Turn-Taking Prediction <span class="chip">SIGDIAL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at SIGdial Meeting on
  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's
  version of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Optimal strategies to perform multilingual analysis of social content
  for a novel dataset in the tourism domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>N<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>p<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rising influence of social media platforms in various domains, including
tourism, has highlighted the growing need for efficient and automated Natural
Language Processing (NLP) strategies to take advantage of this valuable
resource. However, the transformation of multilingual, unstructured, and
informal texts into structured knowledge still poses significant challenges,
most notably the never-ending requirement for manually annotated data to train
deep learning classifiers. In this work, we study different NLP techniques to
establish the best ones to obtain competitive performances while keeping the
need for training annotated data to a minimum. To do so, we built the first
publicly available multilingual dataset (French, English, and Spanish) for the
tourism domain, composed of tourism-related tweets. The dataset includes
multilayered, manually revised annotations for Named Entity Recognition (NER)
for Locations and Fine-grained Thematic Concepts Extraction mapped to the
Thesaurus of Tourism and Leisure Activities of the World Tourism Organization,
as well as for Sentiment Analysis at the tweet level. Extensive experimentation
comparing various few-shot and fine-tuning techniques with modern language
models demonstrate that modern few-shot techniques allow us to obtain
competitive results for all three tasks with very little annotation data: 5
tweets per label (15 in total) for Sentiment Analysis, 30 tweets for Named
Entity Recognition of Locations and 1K tweets annotated with fine-grained
thematic concepts, a highly fine-grained sequence labeling task based on an
inventory of 315 classes. We believe that our results, grounded in a novel
dataset, pave the way for applying NLP to new domain-specific applications,
reducing the need for manual annotations and circumventing the complexities of
rule-based, ad-hoc solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Causal Representation Learning with Generative Artificial Intelligence:
  Application to Texts as Treatments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00903v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00903v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we demonstrate how to enhance the validity of causal inference
with unstructured high-dimensional treatments like texts, by leveraging the
power of generative Artificial Intelligence (GenAI). Specifically, we propose
to use a deep generative model such as large language models (LLMs) to
efficiently generate treatments and use their internal representation for
subsequent causal effect estimation. We show that the knowledge of this true
internal representation helps disentangle the treatment features of interest,
such as specific sentiments and certain topics, from other possibly unknown
confounding features. Unlike existing methods, the proposed GenAI-Powered
Inference (GPI) methodology eliminates the need to learn causal representation
from the data, and hence produces more accurate and efficient estimates. We
formally establish the conditions required for the nonparametric identification
of the average treatment effect, propose an estimation strategy that avoids the
violation of the overlap assumption, and derive the asymptotic properties of
the proposed estimator through the application of double machine learning.
Finally, using an instrumental variables approach, we extend the proposed
methodology to the settings in which the treatment feature is based on human
perception. The proposed GPI methodology is also applicable to text reuse where
an LLM is used to regenerate existing texts. We conduct simulation and
empirical studies, using the generated text data from an open-source LLM, Llama
3, to illustrate the advantages of our estimator over state-of-the-art causal
representation learning algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> SMARTe: Slot-based Method for Accountable Relational Triple extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.12816v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.12816v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relational Triple Extraction (RTE) is a fundamental task in Natural Language
Processing (NLP). However, prior research has primarily focused on optimizing
model performance, with limited efforts to understand the internal mechanisms
driving these models. Many existing methods rely on complex preprocessing to
induce specific interactions, often resulting in opaque systems that may not
fully align with their theoretical foundations. To address these limitations,
we propose SMARTe: a Slot-based Method for Accountable Relational Triple
extraction. SMARTe introduces intrinsic interpretability through a slot
attention mechanism and frames the task as a set prediction problem. Slot
attention consolidates relevant information into distinct slots, ensuring all
predictions can be explicitly traced to learned slot representations and the
tokens contributing to each predicted relational triple. While emphasizing
interpretability, SMARTe achieves performance comparable to state-of-the-art
models. Evaluations on the NYT and WebNLG datasets demonstrate that adding
interpretability does not compromise performance. Furthermore, we conducted
qualitative assessments to showcase the explanations provided by SMARTe, using
attention heatmaps that map to their respective tokens. We conclude with a
discussion of our findings and propose directions for future research. Our code
is available at https://github.com/Chen-XueWen/SMARTe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Circuit-tuning: A Mechanistic Approach for Identifying Parameter
  Redundancy and Fine-tuning Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of mechanistic interpretability aims to reverse-engineer a model to
explain its behaviors. While recent studies have focused on the static
mechanism of a certain behavior, the learning dynamics inside a model remain to
be explored. In this work, we develop an interpretable fine-tuning method for
analyzing the mechanism behind learning. We first introduce the concept of
node-level intrinsic dimensionality to describe the learning process of a model
in a computational graph. Based on our theory, we propose circuit-tuning, a
two-stage algorithm that iteratively builds the minimal subgraph for a specific
task and updates the key parameters in a heuristic way. Experimental results
confirm the existence of the intrinsic dimensionality at the node level and
demonstrate the effectiveness of our method for transparent and interpretable
fine-tuning. We visualize and analyze the circuits before, during, and after
fine-tuning, providing new insights into the self-organization mechanism of a
neural network in the learning process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Mixture of <span class="highlight-title">Reasoning</span>s: Teach <span class="highlight-title">Large Language Model</span>s to Reason with
  Adaptive Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning. Our experiments show that MoR significantly
enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT
prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates
the need for task-specific prompts, offering a generalizable solution for
robust reasoning across diverse tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-02T00:00:00Z">2025-07-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">74</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text
  Recognition with Chain-of-Thought <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A Strong Baseline for Reasoning based Event Stream Scene Text
  Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>q<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>y<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>n<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Analyzing and Improving Speaker Similarity Assessment for Speech
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>-<span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>é<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>é<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>P<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>p<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>a<span class="highlight-author"></span>ï<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling voice identity is challenging due to its multifaceted nature. In
generative speech systems, identity is often assessed using automatic speaker
verification (ASV) embeddings, designed for discrimination rather than
characterizing identity. This paper investigates which aspects of a voice are
captured in such representations. We find that widely used ASV embeddings focus
mainly on static features like timbre and pitch range, while neglecting dynamic
elements such as rhythm. We also identify confounding factors that compromise
speaker similarity measurements and suggest mitigation strategies. To address
these gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm
patterns. This work contributes to the ongoing challenge of assessing speaker
identity consistency in the context of ever-better voice cloning systems. We
publicly release our code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SSW13 - Interspeech 2025 Speech Synthesis Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> <span class="highlight-title">Reasoning</span> or Not? A Comprehensive Evaluation of <span class="highlight-title">Reasoning</span> <span class="highlight-title">LLM</span>s for
  Dialogue Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>ç<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Dissecting the Impact of Mobile DVFS Governors on <span class="highlight-title">LLM</span> Inference
  Performance and Energy Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>p<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly being integrated into various
applications and services running on billions of mobile devices. However,
deploying LLMs on resource-limited mobile devices faces a significant challenge
due to their high demand for computation, memory, and ultimately energy. While
current LLM frameworks for mobile use three power-hungry components-CPU, GPU,
and Memory-even when running primarily-GPU LLM models, optimized DVFS governors
for CPU, GPU, and memory featured in modern mobile devices operate
independently and are oblivious of each other. Motivated by the above
observation, in this work, we first measure the energy-efficiency of a SOTA LLM
framework consisting of various LLM models on mobile phones which showed the
triplet mobile governors result in up to 40.4% longer prefilling and decoding
latency compared to optimal combinations of CPU, GPU, and memory frequencies
with the same energy consumption for sampled prefill and decode lengths.
Second, we conduct an in-depth measurement study to uncover how the intricate
interplay (or lack of) among the mobile governors cause the above inefficiency
in LLM inference. Finally, based on these insights, we design FUSE - a unified
energy-aware governor for optimizing the energy efficiency of LLM inference on
mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the
time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and
25.4%-36.8% on average with the same energy-per-token for various mobile LLM
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>equal contribution between Zhang and Dash</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Energy-Based Transformers are Scalable Learners and <span class="highlight-title">Think</span>ers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>s<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>q<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Evaluating the Promise and Pitfalls of <span class="highlight-title">LLM</span>s in Hiring Decisions <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>E<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Test-Time Scaling with Reflective Generative Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> The Thin Line Between Comprehension and Persuasion in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>y<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Adaptability of ASR Models on Low-Resource Language: A Comparative Study
  of Whisper and Wav2Vec-BERT on Bangla 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>s<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>k<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>d<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> NaturalThoughts: Selecting and Distilling <span class="highlight-title">Reasoning</span> Traces for General
  <span class="highlight-title">Reasoning</span> Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment
  of <span class="highlight-title">Large Language Model</span>s <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures. Accepted by ACL 2025 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> AI4Research: A Survey of Artificial Intelligence for Scientific Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> High-Layer Attention Pruning with Rescaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher
  Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DIY-MKG: An <span class="highlight-title">LLM</span>-Based Polyglot Language Learning System <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EMNLP 2025 System Demonstration</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Eka-Eval : A Comprehensive Evaluation Framework for Large Language
  Models in Indian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>b<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>U<span class="highlight-author"></span>p<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Low-Perplexity <span class="highlight-title">LLM</span>-Generated Sequences and Where To Find Them <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>y<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Evaluating Structured Output Robustness of Small Language Models for
  Open Attribute-Value Extraction from Clinical Notes <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the ACL Anthology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework
  for <span class="highlight-title">LLM</span>s <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>p<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>á<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>c<span class="highlight-author"></span>á<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5-page main paper (excluding references) + 11-page appendix, 3
  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for
  Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> The Anatomy of Evidence: An Investigation Into Explainable ICD Coding <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>ü<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> How Do Vision-Language Models Process Conflicting Information Across
  Modalities? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>All code and resources are available at:
  https://github.com/ethahtz/vlm_conflicting_info_processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Probing Evaluation Awareness of Language Models <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>f<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>ä<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical AI Governance Workshop, ICML (Poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MuRating: A High Quality Data Selecting Approach to Multilingual Large
  Language Model Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Data interference: emojis, homoglyphs, and issues of data fidelity in
  corpora and their results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Author submitted manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Tuning without Peeking: Provable Privacy and Generalization Bounds for
  <span class="highlight-title">LLM</span> Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>I<span class="highlight-author"></span>s<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>s<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>O<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and
  Comprehension of Corner Cases in Autonomous Driving <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>z<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> <span class="highlight-title">LLM</span>s for Legal Subsumption in German Employment Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>O<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PrePrint - ICAIL25, Chicago</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Stereotype Detection as a Catalyst for Enhanced Bias Detection: A
  Multi-Task Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> AdamMeme: Adaptively Probe the <span class="highlight-title">Reasoning</span> Capacity of Multimodal Large
  Language Models on Harmfulness <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Adapting Language Models to Indonesian Local Languages: An Empirical
  Study of Language Transferability on Zero-Shot Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AMLDS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Confidence and Stability of Global and Pairwise Scores in NLP Evaluation <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>U<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, accepted at ACL SRW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Chart Question Answering from Real-World Analytical Narratives <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>s<span class="highlight-author"></span>b<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the ACL Student Research Workshop
  (SRW) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> T3DM: Test-Time Training-Guided Distribution Shift Modelling for
  Temporal Knowledge Graph <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Emotionally Intelligent Task-oriented Dialogue Systems: Architecture,
  Representation, and Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>p<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>š<span class="highlight-author"></span>i<span class="highlight-author"></span>ć<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Is External Information Useful for Stance Detection with <span class="highlight-title">LLM</span>s? <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Q<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Efficient Out-of-Scope Detection in Dialogue Systems via
  Uncertainty-Driven <span class="highlight-title">LLM</span> Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Á<span class="highlight-author"></span>l<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Following the Clues: Experiments on Person Re-ID using Cross-Modal
  Intelligence <span class="chip">SC 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>u<span class="highlight-author"></span>f<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>l<span class="highlight-author"></span>ä<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication at the 2025 IEEE 28th International
  Conference on Intelligent Transportation Systems (ITSC 2025), taking place
  during November 18-21, 2025 in Gold Coast, Australia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Evaluating the Effectiveness of Direct Preference Optimization for
  Personalizing German Automatic Text Simplifications for Persons with
  Intellectual Disabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>b<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next
  Next Token Speculation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Clinical NLP with Attention-Based Deep Learning for Multi-Disease
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless
  Handwritten STEM Grading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figues, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> SHuBERT: Self-Supervised Sign Language Representation Learning via
  Multi-Stream Cluster Prediction <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16765v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16765v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>x<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language processing has traditionally relied on task-specific models,
limiting the potential for transfer learning across tasks. Pre-training methods
for sign language have typically focused on either supervised pre-training,
which cannot take advantage of unlabeled data, or context-independent (frame or
video segment) representations, which ignore the effects of relationships
across time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a
self-supervised contextual representation model learned from approximately
1,000 hours of American Sign Language video. SHuBERT adapts masked token
prediction objectives to multi-stream visual sign language input, learning to
predict multiple targets corresponding to clustered hand, face, and body pose
streams. SHuBERT achieves state-of-the-art performance across multiple tasks
including sign language translation, isolated sign language recognition, and
fingerspelling detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed Figure 1. ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Beyond Scale: The Diversity Coefficient as a Data Quality Metric for
  Variability in Natural Language Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13840v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13840v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>y<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>y<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>b<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>j<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current trends in pre-training Large Language Models (LLMs) primarily focus
on the scaling of model and dataset size. While the quality of pre-training
data is considered an important factor for training powerful LLMs, it remains a
nebulous concept that has not been rigorously characterized. To this end, we
propose a formalization of one key aspect of data quality -- measuring the
variability of natural language data -- specifically via a measure we call the
diversity coefficient. Our empirical analysis shows that the proposed diversity
coefficient aligns with the intuitive properties of diversity and variability,
e.g., it increases as the number of latent concepts increases. Then, we measure
the diversity coefficient of publicly available pre-training datasets and
demonstrate that their formal diversity is high compared to theoretical lower
and upper bounds. Finally, we conduct a comprehensive set of controlled
interventional experiments with GPT-2 and LLaMAv2 that demonstrate the
diversity coefficient of pre-training data characterizes useful aspects of
downstream model evaluation performance -- totaling 44 models of various sizes
(51M to 7B parameters). We conclude that our formal notion of diversity is an
important aspect of data quality that captures variability and causally leads
to improved evaluation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Re<span class="highlight-title">think</span>ing <span class="highlight-title">LLM</span> Training through Information Geometry and Quantum Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15830v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15830v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization in large language models (LLMs) unfolds over high-dimensional
parameter spaces with non-Euclidean structure. Information geometry frames this
landscape using the Fisher information metric, enabling more principled
learning via natural gradient descent. Though often impractical, this geometric
lens clarifies phenomena such as sharp minima, generalization, and observed
scaling laws. We argue that curvature-aware approaches deepen our understanding
of LLM training. Finally, we speculate on quantum analogies based on the
Fubini-Study metric and Quantum Fisher Information, hinting at efficient
optimization in quantum-enhanced systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure(s)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Quantifying the Importance of Data Alignment in Downstream Model
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08496v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08496v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>b<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>j<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrary to the conventional emphasis on dataset size, we explore the role of
data alignment -- an often overlooked aspect of data quality -- in training
capable Large Language Models (LLMs). To do so, we use the Task2Vec-based
alignment coefficient, a quantitative measure of the similarity between two
datasets, to quantify the impact of alignment between training data and
evaluation data on downstream performance. In particular, we conduct controlled
\textit{interventional} experiments for two settings: 1. the impact of
increased alignment coefficients between various pre-training (pt) against
evaluation datasets, and 2. the impact of increased alignment coefficients
between domain specific fine-tuning (ft) against domain specific evaluation.
The domain specific task we explore is Autoformalization -- the machine
translation task between natural language and code for formal verification. In
both settings, we find a strong, predictable negative correlation between the
alignment coefficient of a model's training and evaluation data and the model's
loss/perplexity on the respective downstream task. These findings suggest a
re-evaluation of LLM training approaches, demonstrating the relevance of data
alignment compared to data quantity, especially in specialized downstream tasks
such as Autoformalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> De-mark: Watermark Removal in <span class="highlight-title">Large Language Model</span>s <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking techniques offer a promising way to identify machine-generated
content via embedding covert information into the contents generated from
language models (LMs). However, the robustness of the watermarking schemes has
not been well explored. In this paper, we present De-mark, an advanced
framework designed to remove n-gram-based watermarks effectively. Our method
utilizes a novel querying strategy, termed random selection probing, which aids
in assessing the strength of the watermark and identifying the red-green list
within the n-gram watermark. Experiments on popular LMs, such as Llama3 and
ChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark
removal and exploitation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Sequential Diagnosis with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>z<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence holds great promise for expanding access to expert
medical knowledge and reasoning. However, most evaluations of language models
rely on static vignettes and multiple-choice questions that fail to reflect the
complexity and nuance of evidence-based medicine in real-world settings. In
clinical practice, physicians iteratively formulate and revise diagnostic
hypotheses, adapting each subsequent question and test to what they've just
learned, and weigh the evolving evidence before committing to a final
diagnosis. To emulate this iterative process, we introduce the Sequential
Diagnosis Benchmark, which transforms 304 diagnostically challenging New
England Journal of Medicine clinicopathological conference (NEJM-CPC) cases
into stepwise diagnostic encounters. A physician or AI begins with a short case
abstract and must iteratively request additional details from a gatekeeper
model that reveals findings only when explicitly queried. Performance is
assessed not just by diagnostic accuracy but also by the cost of physician
visits and tests performed. We also present the MAI Diagnostic Orchestrator
(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,
proposes likely differential diagnoses and strategically selects high-value,
cost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%
diagnostic accuracy--four times higher than the 20% average of generalist
physicians. MAI-DxO also reduces diagnostic costs by 20% compared to
physicians, and 70% compared to off-the-shelf o3. When configured for maximum
accuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO
generalize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and
Llama families. We highlight how AI systems, when guided to think iteratively
and act judiciously, can advance diagnostic precision and cost-effectiveness in
clinical care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Recursive Training Loops in <span class="highlight-title">LLM</span>s: How training data properties modulate
  distribution shift in generated data? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03814v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03814v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>č<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>é<span class="highlight-author"></span>r<span class="highlight-author"></span>é<span class="highlight-author"></span>m<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>é<span class="highlight-author"></span>m<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly used in the creation of online
content, creating feedback loops as subsequent generations of models will be
trained on this synthetic data. Such loops were shown to lead to distribution
shifts - models misrepresenting the true underlying distributions of human data
(also called model collapse). However, how human data properties affect such
shifts remains poorly understood. In this paper, we provide the first empirical
examination of the effect of such properties on the outcome of recursive
training. We first confirm that using different human datasets leads to
distribution shifts of different magnitudes. Through exhaustive manipulation of
dataset properties combined with regression analyses, we then identify a set of
properties predicting distribution shift magnitudes. Lexical diversity is found
to amplify these shifts, while semantic diversity and data quality mitigate
them. Furthermore, we find that these influences are highly modular: data
scrapped from a given internet domain has little influence on the content
generated for another domain. Finally, experiments on political bias reveal
that human data properties affect whether the initial bias will be amplified or
reduced. Overall, our results portray a novel view, where different parts of
internet may undergo different types of distribution shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Towards Universal Semantics With <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a
universal set of semantic primes: simple, primitive word-meanings that have
been shown to exist in most, if not all, languages of the world. According to
this framework, any word, regardless of complexity, can be paraphrased using
these primes, revealing a clear and universally translatable meaning. These
paraphrases, known as explications, can offer valuable applications for many
natural language processing (NLP) tasks, but producing them has traditionally
been a slow, manual process. In this work, we present the first study of using
large language models (LLMs) to generate NSM explications. We introduce
automatic evaluation methods, a tailored dataset for training and evaluation,
and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in
producing accurate, cross-translatable explications, marking a significant step
toward universal semantic representation with LLMs and opening up new
possibilities for applications in semantic analysis, translation, and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> LinguaSynth: Heterogeneous Linguistic Signals for News Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has significantly advanced NLP, but its reliance on large
black-box models introduces critical interpretability and computational
efficiency concerns. This paper proposes LinguaSynth, a novel text
classification framework that strategically integrates five complementary
linguistic feature types: lexical, syntactic, entity-level, word-level
semantics, and document-level semantics within a transparent logistic
regression model. Unlike transformer-based architectures, LinguaSynth maintains
interpretability and computational efficiency, achieving an accuracy of 84.89
percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by
3.32 percent. Through rigorous feature interaction analysis, we show that
syntactic and entity-level signals provide essential disambiguation and
effectively complement distributional semantics. LinguaSynth sets a new
benchmark for interpretable, resource-efficient NLP models and challenges the
prevailing assumption that deep neural networks are necessary for
high-performing text classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Guaranteed Generation from <span class="highlight-title">Large Language Model</span>s <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06716v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06716v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>y<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly used across various
applications, there is a growing need to control text generation to satisfy
specific constraints or requirements. This raises a crucial question: Is it
possible to guarantee strict constraint satisfaction in generated outputs while
preserving the distribution of the original model as much as possible? We first
define the ideal distribution - the one closest to the original model, which
also always satisfies the expressed constraint - as the ultimate goal of
guaranteed generation. We then state a fundamental limitation, namely that it
is impossible to reach that goal through autoregressive training alone. This
motivates the necessity of combining training-time and inference-time methods
to enforce such guarantees. Based on this insight, we propose GUARD, a simple
yet effective approach that combines an autoregressive proposal distribution
with rejection sampling. Through GUARD's theoretical properties, we show how
controlling the KL divergence between a specific proposal and the target ideal
distribution simultaneously optimizes inference speed and distributional
closeness. To validate these theoretical concepts, we conduct extensive
experiments on two text generation settings with hard-to-satisfy constraints: a
lexical constraint scenario and a sentiment reversal scenario. These
experiments show that GUARD achieves perfect constraint satisfaction while
almost preserving the ideal distribution with highly improved inference
efficiency. GUARD provides a principled approach to enforcing strict guarantees
for LLMs without compromising their generative capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> QAEncoder: Towards Aligned Representation Learning in Question Answering
  Systems <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20434v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20434v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern QA systems entail retrieval-augmented generation (RAG) for accurate
and trustworthy responses. However, the inherent gap between user queries and
relevant documents hinders precise matching. We introduce QAEncoder, a
training-free approach to bridge this gap. Specifically, QAEncoder estimates
the expectation of potential queries in the embedding space as a robust
surrogate for the document embedding, and attaches document fingerprints to
effectively distinguish these embeddings. Extensive experiments across diverse
datasets, languages, and embedding models confirmed QAEncoder's alignment
capability, which offers a simple-yet-effective solution with zero additional
index storage, retrieval latency, training costs, or catastrophic forgetting
and hallucination issues. The repository is publicly available at
https://github.com/IAAR-Shanghai/QAEncoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Unified Triplet-Level Hallucination Evaluation for Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23114v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23114v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>s<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the outstanding performance in vision-language reasoning, Large
Vision-Language Models (LVLMs) might generate hallucinated contents that do not
exist in the given image. Most existing LVLM hallucination benchmarks are
constrained to evaluate the object-related hallucinations. However, the
potential hallucination on the relations between two objects, i.e., relation
hallucination, still lacks investigation. To remedy that, we design a unified
framework to measure the object and relation hallucination in LVLMs
simultaneously. The core idea of our framework is to evaluate hallucinations
via (object, relation, object) triplets extracted from LVLMs' responses, making
it easily generalizable to different vision-language tasks. Based on our
framework, we further introduce Tri-HE, a novel Triplet-level Hallucination
Evaluation benchmark which can be used to study both object and relation
hallucination at the same time. With comprehensive evaluations on Tri-HE, we
observe that the relation hallucination issue is even more serious than object
hallucination among existing LVLMs, highlighting a previously neglected problem
towards reliable LVLMs. Moreover, based on our findings, we design a simple
training-free approach that effectively mitigates hallucinations for LVLMs. Our
dataset and code for the reproduction of our experiments are available publicly
at https://github.com/wujunjie1998/Tri-HE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TMLR 2025. Project Page:
  https://kaichen1998.github.io/projects/tri-he/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> On the Fundamental Impossibility of Hallucination Control in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>ł<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>p<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>z<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove that perfect hallucination control in large language models is
mathematically impossible. No LLM inference mechanism can simultaneously
achieve truthful response generation, semantic information conservation,
relevant knowledge revelation, and knowledge-constrained optimality. This
impossibility is fundamental, arising from the mathematical structure of
information aggregation itself rather than engineering limitations. The proof
spans three mathematical frameworks: auction theory, proper scoring theory for
probabilistic predictions, and log-sum-exp analysis for transformer
architectures. In each setting, we demonstrate that information aggregation
creates unavoidable violations of conservation principles. The Jensen gap in
transformer probability aggregation provides a direct measure of this
impossibility. These results reframe hallucination from an engineering bug to
an inevitable mathematical feature of distributed intelligence. There are
fundamental trade-offs between truthfulness, knowledge utilization, and
response completeness, providing principled foundations for managing rather
than eliminating hallucination. This work reveals deep connections between
neural network inference, philosophy of knowledge and reasoning, and classical
results in game theory and information theory, opening new research directions
for developing beneficial AI systems within mathematical constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>major review, transformer inference application, examples added,
  corrections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Caution for the Environment: Multimodal Agents are Susceptible to
  Environmental Distractions <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the faithfulness of multimodal large language model
(MLLM) agents in a graphical user interface (GUI) environment, aiming to
address the research question of whether multimodal GUI agents can be
distracted by environmental context. A general scenario is proposed where both
the user and the agent are benign, and the environment, while not malicious,
contains unrelated content. A wide range of MLLMs are evaluated as GUI agents
using a simulated dataset, following three working patterns with different
levels of perception. Experimental results reveal that even the most powerful
models, whether generalist agents or specialist GUI agents, are susceptible to
distractions. While recent studies predominantly focus on the helpfulness of
agents, our findings first indicate that these agents are prone to
environmental distractions. Furthermore, we implement an adversarial
environment injection and analyze the approach to improve faithfulness, calling
for a collective focus on this important topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Developing ChemDFM as a large language foundation model for chemistry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14818v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14818v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has played an increasingly important role in
chemical research. However, most models currently used in chemistry are
specialist models that require training and tuning for specific tasks. A more
generic and efficient solution would be an AI model that could address many
tasks and support free-form dialogue in the broad field of chemistry. In its
utmost form, such a generalist AI chemist could be referred to as Chemical
General Intelligence. Large language models (LLMs) have recently logged
tremendous success in the general domain of natural language processing,
showing emerging task generalization and free-form dialogue capabilities.
However, domain knowledge of chemistry is largely missing when training
general-domain LLMs. The lack of such knowledge greatly hinders the performance
of generalist LLMs in the field of chemistry. To this end, we develop ChemDFM,
a pioneering LLM for chemistry trained on 34B tokens from chemical literature
and textbooks, and fine-tuned using 2.7M instructions. As a result, it can
understand and reason with chemical knowledge in free-form dialogue.
Quantitative evaluations show that ChemDFM significantly surpasses most
representative open-source LLMs. It outperforms GPT-4 on a great portion of
chemical tasks, despite the substantial size difference. We have open-sourced
the inference codes, evaluation datasets, and model weights of ChemDFM on
Huggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 12 figures, 12 tables. Published on Cell Report Physical
  Science, DOI: https://doi.org/10.1016/j.xcrp.2025.102523</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Unifying Global and Near-Context Biasing in a Single Trie Pass 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>I<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>ú<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>-<span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>p<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>é<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>ğ<span class="highlight-author"></span>l<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of end-to-end automatic speech recognition (ASR) models,
challenges persist in recognizing rare, out-of-vocabulary words - including
named entities (NE) - and in adapting to new domains using only text data. This
work presents a practical approach to address these challenges through an
unexplored combination of an NE bias list and a word-level n-gram language
model (LM). This solution balances simplicity and effectiveness, improving
entities' recognition while maintaining or even enhancing overall ASR
performance. We efficiently integrate this enriched biasing method into a
transducer-based ASR system, enabling context adaptation with almost no
computational overhead. We present our results on three datasets spanning four
languages and compare them to state-of-the-art biasing strategies. We
demonstrate that the proposed combination of keyword biasing and n-gram LM
improves entity recognition by up to 32% relative and reduces overall WER by up
to a 12% relative.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TSD2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> BIS <span class="highlight-title">Reasoning</span> 1.0: The First Large-Scale Japanese Benchmark for
  Belief-Inconsistent Syllogistic <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06955v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06955v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present BIS Reasoning 1.0, the first large-scale Japanese dataset of
syllogistic reasoning problems explicitly designed to evaluate
belief-inconsistent reasoning in large language models (LLMs). Unlike prior
datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned
reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent
syllogisms to uncover reasoning biases in LLMs trained on human-aligned
corpora. We benchmark state-of-the-art models - including GPT models, Claude
models, and leading Japanese LLMs - revealing significant variance in
performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies
critical weaknesses in current LLMs when handling logically valid but
belief-conflicting inputs. These findings have important implications for
deploying LLMs in high-stakes domains such as law, healthcare, and scientific
literature, where truth must override intuitive belief to ensure integrity and
safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version includes typo corrections, added logit lens analysis for
  open models, and an updated author list</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language
  Models in Multi-Round, Multi-Party Dialogues <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>j<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing function-calling benchmarks focus on single-turn interactions.
However, they overlook the complexity of real-world scenarios. To quantify how
existing benchmarks address practical applications, we introduce DICE-SCORE, a
metric that evaluates the dispersion of tool-related information such as
function name and parameter values throughout the dialogue. Analyzing existing
benchmarks through DICE-SCORE reveals notably low scores, highlighting the need
for more realistic scenarios. To address this gap, we present DICE-BENCH, a
framework that constructs practical function-calling datasets by synthesizing
conversations through a tool graph that maintains dependencies across rounds
and a multi-agent system with distinct personas to enhance dialogue
naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our
experiments on 19 LLMs with DICE-BENCH show that significant advances are still
required before such models can be deployed effectively in real-world settings.
Our code and data are all publicly available:
https://snuhcc.github.io/DICE-Bench/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, ACL 2025 Vienna</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit
  Matching Visual Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12084v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12084v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visually linking matching cues is a crucial ability in daily life, such as
identifying the same person in multiple photos based on their cues, even
without knowing who they are. Despite the extensive knowledge that
vision-language models (VLMs) possess, it remains largely unexplored whether
they are capable of performing this fundamental task. To address this, we
introduce \textbf{VLM2-Bench}, a benchmark designed to assess whether VLMs can
Visually Link Matching cues, with 9 subtasks and over 3,000 test cases.
Comprehensive evaluation across twelve VLMs, along with further analysis of
various language-side and vision-side prompting methods, leads to a total of
eight key findings. We identify critical challenges in models' ability to link
visual cues, highlighting a significant performance gap. Based on these
insights, we advocate for (i) enhancing core visual capabilities to improve
adaptability and reduce reliance on prior knowledge, (ii) establishing clearer
principles for integrating language-based reasoning in vision-centric tasks to
prevent unnecessary biases, and (iii) shifting vision-text training paradigms
toward fostering models' ability to independently structure and infer
relationships among visual cues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://vlm2-bench.github.io/ Camera Ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Don't Say No: Jailbreaking <span class="highlight-title">LLM</span> by Suppressing Refusal <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16369v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16369v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the safety alignment of Large Language Models (LLMs) is critical for
generating responses consistent with human values. However, LLMs remain
vulnerable to jailbreaking attacks, where carefully crafted prompts manipulate
them into producing toxic content. One category of such attacks reformulates
the task as an optimization problem, aiming to elicit affirmative responses
from the LLM. However, these methods heavily rely on predefined objectionable
behaviors, limiting their effectiveness and adaptability to diverse harmful
queries. In this study, we first identify why the vanilla target loss is
suboptimal and then propose enhancements to the loss objective. We introduce
DSN (Don't Say No) attack, which combines a cosine decay schedule method with
refusal suppression to achieve higher success rates. Extensive experiments
demonstrate that DSN outperforms baseline attacks and achieves state-of-the-art
attack success rates (ASR). DSN also shows strong universality and
transferability to unseen datasets and black-box models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Transferable Modeling Strategies for Low-Resource <span class="highlight-title">LLM</span> Tasks: A Prompt
  and Alignment-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>q<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Text to Band Gap: Pre-trained Language Models as Encoders for
  Semiconductor Band Gap Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the use of transformer-based language models, RoBERTa, T5, and
LLaMA, for predicting the band gaps of semiconductor materials directly from
textual representations that encode key material features such as chemical
composition, crystal system, space group, number of atoms per unit cell,
valence electron count, and other relevant electronic and structural
properties. Quantum chemistry simulations such as DFT provide accurate
predictions but are computationally intensive, limiting their feasibility for
large-scale materials screening. Shallow ML models offer faster alternatives
but typically require extensive data preprocessing to convert non-numerical
material features into structured numerical inputs, often at the cost of losing
critical descriptive information. In contrast, our approach leverages
pretrained language models to process textual data directly, eliminating the
need for manual feature engineering. We construct material descriptions in two
formats: structured strings that combine key features in a consistent template,
and natural language narratives generated using the ChatGPT API. For each
model, we append a custom regression head and perform task-specific finetuning
on a curated dataset of inorganic compounds. Our results show that finetuned
language models, particularly the decoder-only LLaMA-3 architecture, can
outperform conventional approaches in prediction accuracy and flexibility,
achieving an MAE of 0.25 eV and R2 of 0.89, compared to the best shallow ML
baseline, which achieved an MAE of 0.32 eV and R2 of 0.84. Notably, LLaMA-3
achieves competitive accuracy with minimal finetuning, suggesting its
architecture enables more transferable representations for scientific tasks.
This work demonstrates the effectiveness of finetuned language models for
scientific property prediction and provides a scalable, language-native
framework for materials informatics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Delving into Multilingual Ethical Bias: The MSQAD with Statistical
  Hypothesis Tests for <span class="highlight-title">Large Language Model</span>s <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent strides in large language models, studies have underscored
the existence of social biases within these systems. In this paper, we delve
into the validation and comparison of the ethical biases of LLMs concerning
globally discussed and potentially sensitive topics, hypothesizing that these
biases may arise from language-specific distinctions. Introducing the
Multilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news
articles from Human Rights Watch covering 17 topics, and generated socially
sensitive questions along with corresponding responses in multiple languages.
We scrutinized the biases of these responses across languages and topics,
employing two statistical hypothesis tests. The results showed that the null
hypotheses were rejected in most cases, indicating biases arising from
cross-language differences. It demonstrates that ethical biases in responses
are widespread across various languages, and notably, these biases were
prevalent even among different LLMs. By making the proposed MSQAD openly
available, we aim to facilitate future research endeavors focused on examining
cross-language biases in LLMs and their variant models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Multi-interaction TTS toward professional recording reproduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice directors often iteratively refine voice actors' performances by
providing feedback to achieve the desired outcome. While this iterative
feedback-based refinement process is important in actual recordings, it has
been overlooked in text-to-speech synthesis (TTS). As a result, fine-grained
style refinement after the initial synthesis is not possible, even though the
synthesized speech often deviates from the user's intended style. To address
this issue, we propose a TTS method with multi-step interaction that allows
users to intuitively and rapidly refine synthesized speech. Our approach models
the interaction between the TTS model and its user to emulate the relationship
between voice actors and voice directors. Experiments show that the proposed
model with its corresponding dataset enables iterative style refinements in
accordance with users' directions, thus demonstrating its multi-interaction
capability. Sample audios are available:
https://ntt-hilab-gensp.github.io/ssw13multiinteractiontts/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18443v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18443v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>z<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>f<span class="highlight-author"></span>f<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>y<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PDF documents have the potential to provide trillions of novel, high-quality
tokens for training language models. However, these documents come in a
diversity of types with differing formats and visual layouts that pose a
challenge when attempting to extract and faithfully represent the underlying
content for language model use. Traditional open source tools often produce
lower quality extractions compared to vision language models (VLMs), but
reliance on the best VLMs can be prohibitively costly (e.g., over 6,240 USD per
million PDF pages for GPT-4o) or infeasible if the PDFs cannot be sent to
proprietary APIs. We present olmOCR, an open-source toolkit for processing PDFs
into clean, linearized plain text in natural reading order while preserving
structured content like sections, tables, lists, equations, and more. Our
toolkit runs a fine-tuned 7B vision language model (VLM) trained on
olmOCR-mix-0225, a sample of 260,000 pages from over 100,000 crawled PDFs with
diverse properties, including graphics, handwritten text and poor quality
scans. olmOCR is optimized for large-scale batch processing, able to scale
flexibly to different hardware setups and can convert a million PDF pages for
only 176 USD. To aid comparison with existing systems, we also introduce
olmOCR-Bench, a curated set of 1,400 PDFs capturing many content types that
remain challenging even for the best tools and VLMs, including formulas,
tables, tiny fonts, old scans, and more. We find olmOCR outperforms even top
VLMs including GPT-4o, Gemini Flash 2 and Qwen-2.5-VL. We openly release all
components of olmOCR: our fine-tuned VLM model, training code and data, an
efficient inference pipeline that supports vLLM and SGLang backends, and
benchmark olmOCR-Bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Direct Quantized Training of Language Models with Stochastic Rounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although recent quantized Large Language Models (LLMs), such as BitNet, have
paved the way for significant reduction in memory usage during deployment with
binary or ternary weights, training these models still demands substantial
memory footprints. This is partly because high-precision (i.e., unquantized)
weights required for straight-through estimation must be maintained throughout
the whole training process. To address this, we explore directly updating the
quantized low-precision weights without relying on straight-through estimation
during backpropagation, aiming to save memory usage during training.
Specifically, we employ a stochastic rounding technique to minimize the
information loss caused by the use of low-bit weights throughout training.
Experimental results on our LLaMA-structured models of various sizes indicate
that (1) training with only low-precision weights is feasible even when they
are constrained to ternary values; (2) extending the bit width to 8 bits
achieves performance on par with BitNet b1.58; (3) our models remain robust to
precision scaling and memory reduction, showing minimal performance degradation
when moving from FP32 to lower-memory environments (BF16/FP8); and (4) our
models also support inference using ternary weights, showcasing their
flexibility in deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress, extended experiments to 1B size models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Pre-training Large Memory Language Models with Internal and External
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>z<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural language models are black-boxes -- both linguistic patterns and
factual knowledge are distributed across billions of opaque parameters. This
entangled encoding makes it difficult to reliably inspect, verify, or update
specific facts. We propose a new class of language models, Large Memory
Language Models (LMLM) with a pre-training recipe that stores factual knowledge
in both internal weights and an external database. Our approach strategically
masks externally retrieved factual values from the training loss, thereby
teaching the model to perform targeted lookups rather than relying on
memorization in model weights. Our experiments demonstrate that LMLMs achieve
competitive performance compared to significantly larger, knowledge-dense LLMs
on standard benchmarks, while offering the advantages of explicit, editable,
and verifiable knowledge bases. This work represents a fundamental shift in how
language models interact with and manage factual knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, models, and data available at
  https://github.com/kilian-group/LMLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> KatFishNet: Detecting <span class="highlight-title">LLM</span>-Generated Korean Text through Linguistic
  Feature Analysis <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00032v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00032v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>-<span class="highlight-author"></span>K<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) increases the
difficulty of distinguishing between human-written and LLM-generated text.
Detecting LLM-generated text is crucial for upholding academic integrity,
preventing plagiarism, protecting copyrights, and ensuring ethical research
practices. Most prior studies on detecting LLM-generated text focus primarily
on English text. However, languages with distinct morphological and syntactic
characteristics require specialized detection approaches. Their unique
structures and usage patterns can hinder the direct application of methods
primarily designed for English. Among such languages, we focus on Korean, which
has relatively flexible spacing rules, a rich morphological system, and less
frequent comma usage compared to English. We introduce KatFish, the first
benchmark dataset for detecting LLM-generated Korean text. The dataset consists
of text written by humans and generated by four LLMs across three genres.
  By examining spacing patterns, part-of-speech diversity, and comma usage, we
illuminate the linguistic differences between human-written and LLM-generated
Korean text. Building on these observations, we propose KatFishNet, a detection
method specifically designed for the Korean language. KatFishNet achieves an
average of 19.78% higher AUROC compared to the best-performing existing
detection method. Our code and data are available at
https://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-07-05T05:32:43.746487900Z">
            2025-07-05 05:32:43 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
